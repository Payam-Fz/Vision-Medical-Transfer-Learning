{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetuning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:23:24.659168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "GPU: []\n",
      "Dataset: MIMIC-CXR\n",
      "Model: ./base-models/simclr/r152_2x_sk1/hub/\n",
      "Epochs: 11\n",
      "Batch size: 64\n",
      "2024-01-30 15:23:41.385229: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 54 of 128\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python finetuning2.py --dataset=MIMIC-CXR \\\n",
    "  --base_model_path=./base-models/simclr/r152_2x_sk1/hub/ \\\n",
    "  --epochs=10 --batch_size=64 --learning_rate=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 21:23:37.724870: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list: []\n",
      "WARNING:tensorflow:From /tmp/ipykernel_462910/2675560416.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "available: False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('list:', tf.config.list_physical_devices('GPU'))\n",
    "print('available:', tf.test.is_gpu_available())\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "#     c = tf.matmul(a, b)\n",
    "#     tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# Copyright 2023 The medical_research_foundations Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Original code from medical_research_foundations repository in:\n",
    "#   /colab/REMEDIS_finetuning_example.ipynb\n",
    "# Modified to add support for MIMIC-CXR-JPG dataset\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# import tensorflow.compat.v2 as tf\n",
    "# tf.compat.v1.enable_v2_behavior()\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_eager_execution()\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LARS optimizer from lars_optimizer.py in SimCLR repository\n",
    "from lars_optimizer import LARSOptimizer\n",
    "# macro_soft_f1 optimizer from multi-label-soft-f1 repository\n",
    "from objective_func import macro_soft_f1, macro_f1\n",
    "# Preprocessing functions from data_util.py in SimCLR repository\n",
    "from utils.augmentation import preprocess_image\n",
    "# utilities to plot, time, and score\n",
    "from utils.analysis import *\n",
    "\n",
    "from loader.mimic_cxr_jpg_loader import MIMIC_CXR_JPG_Loader\n",
    "\n",
    "\n",
    "#------------------- PARAMS -------------------#\n",
    "DATASET = \"MIMIC-CXR\" #@param [\"Chexpert\", \"Camelyon\", \"MIMIC-CXR\", \"Noise\"]\n",
    "BASE_MODEL = \"SimCLR\" #@param [\"SimCLR\", \"DINO\", \"REMEDIS\"]\n",
    "BATCH_SIZE = 5 #64\n",
    "IMAGE_SIZE = (448,448)\n",
    "CHANNELS = 3\n",
    "NUM_SAMPLES = 64\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 2 #10\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.\n",
    "\n",
    "\n",
    "#------------------- VARS -------------------#\n",
    "project_folder = os.getcwd()\n",
    "# model_required_shapes = {\n",
    "#   SimCLR: {image_size: (448,448), },\n",
    "#   DINO: {},\n",
    "#   REMEDIS: {}\n",
    "#   }\n",
    "\n",
    "#------------------- LOAD DATA -------------------#\n",
    "\n",
    "# Chexpert: TFDS.has Supervised - produces binary labels the way we were using them\n",
    "#           Chexpert data loader fails unless you have it downloaded - download & put in this directory\n",
    "#           Have self-selectors\n",
    "# Noise:    fake tfds for testing\n",
    "def _preprocess_train(x, y, info=None):\n",
    "  x = preprocess_image(\n",
    "      x, *IMAGE_SIZE,\n",
    "      is_training=True, color_distort=False, crop='Center')\n",
    "  return x, y\n",
    "\n",
    "def _preprocess_val(x, y, info=None):\n",
    "  x = preprocess_image(\n",
    "      x, *IMAGE_SIZE,\n",
    "      is_training=False, color_distort=False, crop='Center')\n",
    "  return x, y\n",
    "\n",
    "if DATASET == 'Noise':\n",
    "  def generate_fake_tfds_dataset(width, height, channels, num_classes, N=1000, data_type=tf.float32):\n",
    "    train_examples = np.random.normal(size=[N, width, height, channels])\n",
    "    classes = np.arange(0, num_classes)\n",
    "    random_labels = np.random.choice(a=classes, size=N)\n",
    "    one_hot_encoded = np.zeros((N, num_classes))\n",
    "    one_hot_encoded[np.arange(N), random_labels] = 1\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, one_hot_encoded))\n",
    "    return train_dataset\n",
    "  \n",
    "  num_classes = 14\n",
    "  train_tfds = generate_fake_tfds_dataset(*IMAGE_SIZE, CHANNELS, num_classes, NUM_SAMPLES)\n",
    "  \n",
    "elif DATASET == 'Camelyon':\n",
    "  # NOTE: This is too large to run with the public runtime. Run locally.\n",
    "  # To see more information about the patch_camelyon dataset, see\n",
    "  # (https://github.com/basveeling/pcam).\n",
    "  train_tfds, tfds_info = tfds.load('patch_camelyon',\n",
    "                                      split='train[:1%]',\n",
    "                                      with_info = True,\n",
    "                                      as_supervised = True)\n",
    "  num_images = tfds_info.splits['train'].num_examples\n",
    "  num_classes = tfds_info.features['label'].num_classes\n",
    "  train_tfds = train_tfds.map(lambda x, y: (x, tf.one_hot(y, num_classes)))\n",
    "\n",
    "elif DATASET == 'Chexpert':\n",
    "  # TODO: Load chexpert data here.\n",
    "  num_classes = 14\n",
    "  raise Exception(\"not implemented. Please download the chexpert data manually and add code to read here.\")\n",
    "\n",
    "elif DATASET == 'MIMIC-CXR':\n",
    "  customLoader = MIMIC_CXR_JPG_Loader({'train': NUM_SAMPLES, 'validate': NUM_SAMPLES, 'test': 0}, project_folder)\n",
    "  train_tfds, val_tfds, test_tfds = customLoader.load()\n",
    "  num_classes = customLoader.metadata['num_classes']\n",
    "\n",
    "else:\n",
    "  raise Exception('The Data Type specified does not have data loading defined.')\n",
    "\n",
    "train_tfds = train_tfds.shuffle(buffer_size=2*BATCH_SIZE)\n",
    "batched_train_tfds = train_tfds.map(_preprocess_train).batch(BATCH_SIZE)\n",
    "# val_tfds = val_tfds.shuffle(buffer_size=2*BATCH_SIZE)\n",
    "batched_val_tfds = val_tfds.map(_preprocess_val).batch(BATCH_SIZE)\n",
    "\n",
    "# TODO: in case improves performance\n",
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# batched_train_tfds = batched_train_tfds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# next_batch = tf.data.make_one_shot_iterator(batched_train_tfds).get_next()\n",
    "\n",
    "for f, l in batched_train_tfds.take(1):\n",
    "  print(\"Shape of features array:\", f.numpy().shape)\n",
    "  print(\"Shape of labels array:\", l.numpy().shape)\n",
    "\n",
    "#------------------- LOAD MODLES -------------------#\n",
    "# Load module and construct the computation graph\n",
    "# Load the base network and set it to non-trainable (for speedup fine-tuning)\n",
    "\n",
    "if BASE_MODEL == 'REMEDIS':\n",
    "  hub_path = os.path.join(project_folder, './base-models/remedis/cxr-152x2-remedis-m/')\n",
    "elif BASE_MODEL == 'SimCLR':\n",
    "  hub_path = os.path.join(project_folder, './base-models/simclr/r152_2x_sk1/hub/')\n",
    "elif BASE_MODEL == 'DINO':\n",
    "  raise Exception('TODO: Not implemented yet.')\n",
    "  hub_path = os.path.join(project_folder, './base-models/dino/???')\n",
    "\n",
    "try:\n",
    "  feature_extractor_layer = hub.KerasLayer(hub_path, input_shape=(*IMAGE_SIZE, CHANNELS), trainable=False)\n",
    "except:\n",
    "  print(f\"\"\"The model {hub_path} did not load. Please verify the model path. It is also worth considering that the model might still be in the process of being uploaded to the designated location. If you have recently uploaded it to a notebook, there could be delays associated with the upload.\"\"\")\n",
    "  raise\n",
    "\n",
    "#------------------- SETUP TRAINING HEAD -------------------#\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    layers.Dense(1024, activation='relu', name='hidden_layer'), # TODO: check with Leon, why 1024\n",
    "    layers.Dense(num_classes, activation='sigmoid', name='multi-label_classifier')\n",
    "])\n",
    "\n",
    "# TEMP for debugging\n",
    "print(model.summary())\n",
    "for batch in batched_val_tfds:\n",
    "    print('probability of an image for classes:', model.predict(batch[0])[:1])\n",
    "    break\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  weight_decay=WEIGHT_DECAY)\n",
    "optimizer.exclude_from_weight_decay(\n",
    "    var_names=['batch_normalization', 'bias', 'head_supervised'])\n",
    "\n",
    "model.compile(\n",
    "  optimizer=optimizer,\n",
    "  loss=macro_soft_f1,\n",
    "  metrics=[macro_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "start = time()\n",
    "history = model.fit(batched_train_tfds,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=batched_val_tfds)\n",
    "print('\\nTraining took {}'.format(print_time(time()-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSE\n",
    "\n",
    "from utils.analysis import *\n",
    "\n",
    "losses, val_losses, macro_f1s, val_macro_f1s = learning_curves(history)\n",
    "# model_bce_losses, model_bce_val_losses, model_bce_macro_f1s, model_bce_val_macro_f1s = learning_curves(history_bce)\n",
    "\n",
    "for batch in batched_val_tfds:\n",
    "  print('probability of an image for classes (after finetuning):', model.predict(batch[0])[:1])\n",
    "  break\n",
    "\n",
    "print(\"Macro soft-F1 loss: %.2f\" %val_losses[-1])\n",
    "print(\"Macro F1-score: %.2f\" %val_macro_f1s[-1])\n",
    "\n",
    "for batch in batched_val_tfds:\n",
    "  show_prediction(*batch, model)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
