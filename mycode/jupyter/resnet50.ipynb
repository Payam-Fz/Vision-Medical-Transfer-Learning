{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cwd to '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation'\n",
      "['/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode/neural_nets', '/home/payam/miniconda3/envs/tf2-gpu/lib/python39.zip', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/lib-dynload', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages', '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode']\n",
      "/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation\n",
      "2024-04-05 11:41:38.565200: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 11:41:39.080417: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-05 11:41:39.080495: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-05 11:41:39.080503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-05 11:41:40.106476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.141766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.141983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.142348: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 11:41:40.143379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.143569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.143696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.575720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.575909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.576041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 11:41:40.576143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "@_@\t\n",
      "@_@\t------------------ Configuration ------------------\n",
      "@_@\tStart: 2024-04-05_1141\n",
      "@_@\t\n",
      "@_@\tMode: train_then_eval\n",
      "@_@\tUnfreeze blocks: start 0, end 0\n",
      "@_@\tContinue from checkpoint: False\n",
      "@_@\t\n",
      "@_@\tDataset: MIMIC-CXR\n",
      "@_@\tTraining Dataset Size: 8\n",
      "@_@\tBatch size: 8\n",
      "@_@\tImage size: (448, 448)\n",
      "@_@\tMax Epochs per training round: 300\n",
      "@_@\tDefault Learning Rate: 0.1\n",
      "@_@\t\n",
      "@_@\tGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "@_@\tCPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "@_@\t\n",
      "@_@\t------------------ Data ------------------\n",
      "@_@\tnum_classes: 9\n",
      "@_@\tclass_names: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Pleural Effusion'\n",
      "@_@\t 'Pneumonia' 'Pneumothorax' 'Fracture' 'Support Devices']\n",
      "@_@\tall_data_size: 377110\n",
      "@_@\tall_data_filtered_size: 112134\n",
      "@_@\tsplit_size: {'train': 8, 'validate': 889, 'test': 512}\n",
      "@_@\tmax value before preprocess tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "@_@\tmin value before preprocess tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "@_@\tShape of image batch: [2402, 2697, 3]\n",
      "@_@\tShape of labels batch: [9]\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0405 11:42:29.317487 125002897123136 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "@_@\tmax value after preprocess tf.Tensor(255, shape=(), dtype=uint8)\n",
      "@_@\tmin value after preprocess tf.Tensor(0, shape=(), dtype=uint8)\n",
      "@_@\tShape of image batch: [8, 448, 448, 3]\n",
      "@_@\tShape of labels batch: [8, 9]\n",
      "Model: \"my_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 448, 448, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 454, 454, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 224, 224, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 224, 224, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 224, 224, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 226, 226, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 112, 112, 64  0           ['pool1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 112, 112, 64  4160        ['pool1_pool[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block1_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 112, 112, 25  16640       ['pool1_pool[0][0]']             \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 112, 112, 25  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                6)                                'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 112, 112, 25  0           ['conv2_block1_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 112, 112, 64  16448       ['conv2_block1_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block2_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 112, 112, 25  0           ['conv2_block1_out[0][0]',       \n",
      "                                6)                                'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 112, 112, 25  0           ['conv2_block2_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 112, 112, 64  16448       ['conv2_block2_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block3_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 112, 112, 25  0           ['conv2_block2_out[0][0]',       \n",
      "                                6)                                'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 112, 112, 25  0           ['conv2_block3_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 56, 56, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 56, 56, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 56, 56, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 56, 56, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 56, 56, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 56, 56, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 28, 28, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 28, 28, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 28, 28, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 28, 28, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 28, 28, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 28, 28, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 28, 28, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 28, 28, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 14, 14, 512)  524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 14, 14, 2048  2099200     ['conv4_block6_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 14, 14, 2048  0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 14, 14, 512)  1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 14, 14, 2048  0           ['conv5_block1_out[0][0]',       \n",
      "                                )                                 'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 14, 14, 512)  1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 14, 14, 2048  0           ['conv5_block2_out[0][0]',       \n",
      "                                )                                 'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " my_avg_pool (GlobalMaxPooling2  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " my_fc_1 (Dense)                (None, 256)          524544      ['my_avg_pool[0][0]']            \n",
      "                                                                                                  \n",
      " my_fc_2 (Dense)                (None, 128)          32896       ['my_fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " my_output (Dense)              (None, 9)            1161        ['my_fc_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,146,313\n",
      "Trainable params: 558,601\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "@_@\tUnfreezing 0 blocks...\n",
      "@_@\tTotal trainable weights: 6\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 0.001\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0405 11:42:32.012680 125002897123136 deprecation.py:554] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2024-04-05 11:42:34.899697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2024-04-05 11:42:35.551229: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x71aeb80135d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-05 11:42:35.551255: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-04-05 11:42:35.555520: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-05 11:42:35.658146: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.1674 - macro_f1_score: 0.3333 - soft_f1_loss: 0.6813 - AUC: 0.4462 - global_accuracy: 0.5694 - global_precision: 0.3750 - global_recall: 0.7143 - val_loss: 3.9230 - val_macro_f1_score: 0.1100 - val_soft_f1_loss: 0.8867 - val_AUC: 0.5093 - val_global_accuracy: 0.6389 - val_global_precision: 0.3333 - val_global_recall: 0.2381\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9230 - macro_f1_score: 0.1100 - soft_f1_loss: 0.8867 - AUC: 0.5093 - global_accuracy: 0.6389 - global_precision: 0.3333 - global_recall: 0.2381 - val_loss: 3.0056 - val_macro_f1_score: 0.2593 - val_soft_f1_loss: 0.7773 - val_AUC: 0.5380 - val_global_accuracy: 0.6944 - val_global_precision: 0.4737 - val_global_recall: 0.4286\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0056 - macro_f1_score: 0.2593 - soft_f1_loss: 0.7773 - AUC: 0.5380 - global_accuracy: 0.6944 - global_precision: 0.4737 - global_recall: 0.4286 - val_loss: 2.0961 - val_macro_f1_score: 0.1594 - val_soft_f1_loss: 0.8209 - val_AUC: 0.6266 - val_global_accuracy: 0.7222 - val_global_precision: 0.5385 - val_global_recall: 0.3333\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0961 - macro_f1_score: 0.1594 - soft_f1_loss: 0.8209 - AUC: 0.6266 - global_accuracy: 0.7222 - global_precision: 0.5385 - global_recall: 0.3333 - val_loss: 1.9309 - val_macro_f1_score: 0.1481 - val_soft_f1_loss: 0.8208 - val_AUC: 0.5843 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.3333\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9309 - macro_f1_score: 0.1481 - soft_f1_loss: 0.8208 - AUC: 0.5843 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.3333 - val_loss: 1.2300 - val_macro_f1_score: 0.5333 - val_soft_f1_loss: 0.5295 - val_AUC: 0.7500 - val_global_accuracy: 0.8333 - val_global_precision: 0.7647 - val_global_recall: 0.6190\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2300 - macro_f1_score: 0.5333 - soft_f1_loss: 0.5295 - AUC: 0.7500 - global_accuracy: 0.8333 - global_precision: 0.7647 - global_recall: 0.6190 - val_loss: 1.1605 - val_macro_f1_score: 0.5162 - val_soft_f1_loss: 0.5306 - val_AUC: 0.7583 - val_global_accuracy: 0.7500 - val_global_precision: 0.5517 - val_global_recall: 0.7619\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1605 - macro_f1_score: 0.5162 - soft_f1_loss: 0.5306 - AUC: 0.7583 - global_accuracy: 0.7500 - global_precision: 0.5517 - global_recall: 0.7619 - val_loss: 0.2904 - val_macro_f1_score: 0.7926 - val_soft_f1_loss: 0.3215 - val_AUC: 0.8889 - val_global_accuracy: 0.9167 - val_global_precision: 0.7778 - val_global_recall: 1.0000\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2904 - macro_f1_score: 0.7926 - soft_f1_loss: 0.3215 - AUC: 0.8889 - global_accuracy: 0.9167 - global_precision: 0.7778 - global_recall: 1.0000 - val_loss: 0.4980 - val_macro_f1_score: 0.6641 - val_soft_f1_loss: 0.3667 - val_AUC: 0.8889 - val_global_accuracy: 0.8056 - val_global_precision: 0.6296 - val_global_recall: 0.8095\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4980 - macro_f1_score: 0.6641 - soft_f1_loss: 0.3667 - AUC: 0.8889 - global_accuracy: 0.8056 - global_precision: 0.6296 - global_recall: 0.8095 - val_loss: 0.4515 - val_macro_f1_score: 0.4956 - val_soft_f1_loss: 0.4290 - val_AUC: 0.8704 - val_global_accuracy: 0.8194 - val_global_precision: 0.7500 - val_global_recall: 0.5714\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4515 - macro_f1_score: 0.4956 - soft_f1_loss: 0.4290 - AUC: 0.8704 - global_accuracy: 0.8194 - global_precision: 0.7500 - global_recall: 0.5714^C\n"
     ]
    }
   ],
   "source": [
    "!python ../neural_nets/resnet50_test.py \\\n",
    "  --ouput_name=resnet50-transfer-miniBatchTest \\\n",
    "  --image_size=448 --epochs=300 --batch_size=8 --train_size=8 \\\n",
    "  --mode=train_then_eval --min_unfreeze_blocks=0 --max_unfreeze_blocks=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cwd to '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation'\n",
      "['/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode/neural_nets', '/home/payam/miniconda3/envs/tf2-gpu/lib/python39.zip', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/lib-dynload', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages', '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode']\n",
      "/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation\n",
      "2024-04-02 13:00:16.081390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 13:00:16.597096: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-02 13:00:16.597203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-02 13:00:16.597211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-02 13:00:17.569550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:17.603970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:17.604185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:17.604551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 13:00:17.605783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:17.605944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:17.606073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:18.032508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:18.032725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:18.032883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 13:00:18.033011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "@_@\t\n",
      "@_@\t------------------ Configuration ------------------\n",
      "@_@\tStart: 2024-04-02_1300\n",
      "@_@\t\n",
      "@_@\tMode: train_then_eval\n",
      "@_@\tUnfreeze blocks: start 0, end 0\n",
      "@_@\tContinue from checkpoint: False\n",
      "@_@\t\n",
      "@_@\tDataset: MIMIC-CXR\n",
      "@_@\tTraining Dataset Size: 8\n",
      "@_@\tBatch size: 8\n",
      "@_@\tImage size: (448, 448)\n",
      "@_@\tMax Epochs per training round: 300\n",
      "@_@\tDefault Learning Rate: 0.1\n",
      "@_@\t\n",
      "@_@\tGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "@_@\tCPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "@_@\t\n",
      "@_@\t------------------ Data ------------------\n",
      "@_@\tnum_classes: 9\n",
      "@_@\tclass_names: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Pleural Effusion'\n",
      "@_@\t 'Pneumonia' 'Pneumothorax' 'Fracture' 'Support Devices']\n",
      "@_@\tall_data_size: 377110\n",
      "@_@\tall_data_filtered_size: 112134\n",
      "@_@\tsplit_size: {'train': 8, 'validate': 889, 'test': 512}\n",
      "@_@\tmax value before preprocess tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "@_@\tmin value before preprocess tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "@_@\tShape of image batch: [2402, 2697, 3]\n",
      "@_@\tShape of labels batch: [9]\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0402 13:01:04.181169 130360925050688 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "@_@\tmax value after preprocess tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "@_@\tmin value after preprocess tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "@_@\tShape of image batch: [8, 448, 448, 3]\n",
      "@_@\tShape of labels batch: [8, 9]\n",
      "Model: \"my_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 448, 448, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 454, 454, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 224, 224, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 224, 224, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 224, 224, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 226, 226, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 112, 112, 64  0           ['pool1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 112, 112, 64  4160        ['pool1_pool[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block1_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 112, 112, 25  16640       ['pool1_pool[0][0]']             \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 112, 112, 25  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                6)                                'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 112, 112, 25  0           ['conv2_block1_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 112, 112, 64  16448       ['conv2_block1_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block2_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 112, 112, 25  0           ['conv2_block1_out[0][0]',       \n",
      "                                6)                                'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 112, 112, 25  0           ['conv2_block2_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 112, 112, 64  16448       ['conv2_block2_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block3_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 112, 112, 25  0           ['conv2_block2_out[0][0]',       \n",
      "                                6)                                'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 112, 112, 25  0           ['conv2_block3_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 56, 56, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 56, 56, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 56, 56, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 56, 56, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 56, 56, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 56, 56, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 28, 28, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 28, 28, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 28, 28, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 28, 28, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 28, 28, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 28, 28, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 28, 28, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 28, 28, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 14, 14, 512)  524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 14, 14, 2048  2099200     ['conv4_block6_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 14, 14, 2048  0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 14, 14, 512)  1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 14, 14, 2048  0           ['conv5_block1_out[0][0]',       \n",
      "                                )                                 'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 14, 14, 512)  1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 14, 14, 2048  0           ['conv5_block2_out[0][0]',       \n",
      "                                )                                 'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " my_flatten (Flatten)           (None, 401408)       0           ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " my_fc_1 (Dense)                (None, 256)          102760704   ['my_flatten[0][0]']             \n",
      "                                                                                                  \n",
      " my_fc_2 (Dense)                (None, 128)          32896       ['my_fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " my_output (Dense)              (None, 9)            1161        ['my_fc_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 126,382,473\n",
      "Trainable params: 102,794,761\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "@_@\tUnfreezing 0 blocks...\n",
      "@_@\tTotal trainable weights: 6\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 0.001\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0402 13:01:06.808305 130360925050688 deprecation.py:554] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2024-04-02 13:01:09.763618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2024-04-02 13:01:10.433883: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x768e34011bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-02 13:01:10.433974: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-04-02 13:01:10.439839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-02 13:01:10.555949: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.2401 - macro_f1_score: 0.3582 - soft_f1_loss: 0.6327 - AUC: 0.4965 - global_accuracy: 0.4306 - global_precision: 0.3214 - global_recall: 0.8571 - val_loss: 21.7968 - val_macro_f1_score: 0.0741 - val_soft_f1_loss: 0.9259 - val_AUC: 0.4444 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.1905\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 21.7968 - macro_f1_score: 0.0741 - soft_f1_loss: 0.9259 - AUC: 0.4444 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.1905 - val_loss: 26.8662 - val_macro_f1_score: 0.1953 - val_soft_f1_loss: 0.8047 - val_AUC: 0.4444 - val_global_accuracy: 0.6528 - val_global_precision: 0.4167 - val_global_recall: 0.4762\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 26.8662 - macro_f1_score: 0.1953 - soft_f1_loss: 0.8047 - AUC: 0.4444 - global_accuracy: 0.6528 - global_precision: 0.4167 - global_recall: 0.4762 - val_loss: 16.3950 - val_macro_f1_score: 0.1273 - val_soft_f1_loss: 0.8758 - val_AUC: 0.5000 - val_global_accuracy: 0.6667 - val_global_precision: 0.4000 - val_global_recall: 0.2857\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 16.3950 - macro_f1_score: 0.1273 - soft_f1_loss: 0.8758 - AUC: 0.5000 - global_accuracy: 0.6667 - global_precision: 0.4000 - global_recall: 0.2857 - val_loss: 12.8203 - val_macro_f1_score: 0.0741 - val_soft_f1_loss: 0.9159 - val_AUC: 0.4704 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.1905\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 12.8203 - macro_f1_score: 0.0741 - soft_f1_loss: 0.9159 - AUC: 0.4704 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.1905 - val_loss: 14.0522 - val_macro_f1_score: 0.2088 - val_soft_f1_loss: 0.7912 - val_AUC: 0.4444 - val_global_accuracy: 0.6806 - val_global_precision: 0.4583 - val_global_recall: 0.5238\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14.0522 - macro_f1_score: 0.2088 - soft_f1_loss: 0.7912 - AUC: 0.4444 - global_accuracy: 0.6806 - global_precision: 0.4583 - global_recall: 0.5238 - val_loss: 9.5024 - val_macro_f1_score: 0.2370 - val_soft_f1_loss: 0.7472 - val_AUC: 0.5537 - val_global_accuracy: 0.5972 - val_global_precision: 0.3750 - val_global_recall: 0.5714\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.5024 - macro_f1_score: 0.2370 - soft_f1_loss: 0.7472 - AUC: 0.5537 - global_accuracy: 0.5972 - global_precision: 0.3750 - global_recall: 0.5714 - val_loss: 17.8452 - val_macro_f1_score: 0.1495 - val_soft_f1_loss: 0.8505 - val_AUC: 0.4444 - val_global_accuracy: 0.5694 - val_global_precision: 0.2917 - val_global_recall: 0.3333\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 17.8452 - macro_f1_score: 0.1495 - soft_f1_loss: 0.8505 - AUC: 0.4444 - global_accuracy: 0.5694 - global_precision: 0.2917 - global_recall: 0.3333 - val_loss: 10.5851 - val_macro_f1_score: 0.1630 - val_soft_f1_loss: 0.8370 - val_AUC: 0.4444 - val_global_accuracy: 0.5972 - val_global_precision: 0.3333 - val_global_recall: 0.3810\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.5851 - macro_f1_score: 0.1630 - soft_f1_loss: 0.8370 - AUC: 0.4444 - global_accuracy: 0.5972 - global_precision: 0.3333 - global_recall: 0.3810 - val_loss: 11.9466 - val_macro_f1_score: 0.0741 - val_soft_f1_loss: 0.9259 - val_AUC: 0.4444 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.1905\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.9466 - macro_f1_score: 0.0741 - soft_f1_loss: 0.9259 - AUC: 0.4444 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.1905 - val_loss: 8.1902 - val_macro_f1_score: 0.1926 - val_soft_f1_loss: 0.8046 - val_AUC: 0.4931 - val_global_accuracy: 0.6528 - val_global_precision: 0.4167 - val_global_recall: 0.4762\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.1902 - macro_f1_score: 0.1926 - soft_f1_loss: 0.8046 - AUC: 0.4931 - global_accuracy: 0.6528 - global_precision: 0.4167 - global_recall: 0.4762 - val_loss: 12.0604 - val_macro_f1_score: 0.2644 - val_soft_f1_loss: 0.7354 - val_AUC: 0.4556 - val_global_accuracy: 0.5139 - val_global_precision: 0.3250 - val_global_recall: 0.6190\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 12.0604 - macro_f1_score: 0.2644 - soft_f1_loss: 0.7354 - AUC: 0.4556 - global_accuracy: 0.5139 - global_precision: 0.3250 - global_recall: 0.6190 - val_loss: 11.3811 - val_macro_f1_score: 0.2260 - val_soft_f1_loss: 0.7730 - val_AUC: 0.4926 - val_global_accuracy: 0.5833 - val_global_precision: 0.3548 - val_global_recall: 0.5238\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.3811 - macro_f1_score: 0.2260 - soft_f1_loss: 0.7730 - AUC: 0.4926 - global_accuracy: 0.5833 - global_precision: 0.3548 - global_recall: 0.5238 - val_loss: 12.1444 - val_macro_f1_score: 0.1051 - val_soft_f1_loss: 0.8949 - val_AUC: 0.4444 - val_global_accuracy: 0.6250 - val_global_precision: 0.3125 - val_global_recall: 0.2381\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 12.1444 - macro_f1_score: 0.1051 - soft_f1_loss: 0.8949 - AUC: 0.4444 - global_accuracy: 0.6250 - global_precision: 0.3125 - global_recall: 0.2381 - val_loss: 11.7737 - val_macro_f1_score: 0.0000e+00 - val_soft_f1_loss: 0.9879 - val_AUC: 0.4815 - val_global_accuracy: 0.7083 - val_global_precision: 0.0000e+00 - val_global_recall: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.7737 - macro_f1_score: 0.0000e+00 - soft_f1_loss: 0.9879 - AUC: 0.4815 - global_accuracy: 0.7083 - global_precision: 0.0000e+00 - global_recall: 0.0000e+00 - val_loss: 11.1576 - val_macro_f1_score: 0.1051 - val_soft_f1_loss: 0.8948 - val_AUC: 0.4815 - val_global_accuracy: 0.6250 - val_global_precision: 0.3125 - val_global_recall: 0.2381\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.1576 - macro_f1_score: 0.1051 - soft_f1_loss: 0.8948 - AUC: 0.4815 - global_accuracy: 0.6250 - global_precision: 0.3125 - global_recall: 0.2381 - val_loss: 10.3872 - val_macro_f1_score: 0.1347 - val_soft_f1_loss: 0.8651 - val_AUC: 0.4444 - val_global_accuracy: 0.6806 - val_global_precision: 0.4375 - val_global_recall: 0.3333\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3872 - macro_f1_score: 0.1347 - soft_f1_loss: 0.8651 - AUC: 0.4444 - global_accuracy: 0.6806 - global_precision: 0.4375 - global_recall: 0.3333 - val_loss: 8.1394 - val_macro_f1_score: 0.2296 - val_soft_f1_loss: 0.7858 - val_AUC: 0.5519 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.4762\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.1394 - macro_f1_score: 0.2296 - soft_f1_loss: 0.7858 - AUC: 0.5519 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.4762 - val_loss: 6.9263 - val_macro_f1_score: 0.2599 - val_soft_f1_loss: 0.7403 - val_AUC: 0.5093 - val_global_accuracy: 0.6389 - val_global_precision: 0.4194 - val_global_recall: 0.6190\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.9263 - macro_f1_score: 0.2599 - soft_f1_loss: 0.7403 - AUC: 0.5093 - global_accuracy: 0.6389 - global_precision: 0.4194 - global_recall: 0.6190 - val_loss: 10.5206 - val_macro_f1_score: 0.1791 - val_soft_f1_loss: 0.8200 - val_AUC: 0.4365 - val_global_accuracy: 0.6250 - val_global_precision: 0.3750 - val_global_recall: 0.4286\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.5206 - macro_f1_score: 0.1791 - soft_f1_loss: 0.8200 - AUC: 0.4365 - global_accuracy: 0.6250 - global_precision: 0.3750 - global_recall: 0.4286 - val_loss: 11.6642 - val_macro_f1_score: 0.2821 - val_soft_f1_loss: 0.7259 - val_AUC: 0.5333 - val_global_accuracy: 0.5972 - val_global_precision: 0.3667 - val_global_recall: 0.5238\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.6642 - macro_f1_score: 0.2821 - soft_f1_loss: 0.7259 - AUC: 0.5333 - global_accuracy: 0.5972 - global_precision: 0.3667 - global_recall: 0.5238 - val_loss: 4.8460 - val_macro_f1_score: 0.1185 - val_soft_f1_loss: 0.8396 - val_AUC: 0.5148 - val_global_accuracy: 0.6528 - val_global_precision: 0.3750 - val_global_recall: 0.2857\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8460 - macro_f1_score: 0.1185 - soft_f1_loss: 0.8396 - AUC: 0.5148 - global_accuracy: 0.6528 - global_precision: 0.3750 - global_recall: 0.2857 - val_loss: 7.4005 - val_macro_f1_score: 0.2074 - val_soft_f1_loss: 0.7814 - val_AUC: 0.5519 - val_global_accuracy: 0.6806 - val_global_precision: 0.4444 - val_global_recall: 0.3810\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.4005 - macro_f1_score: 0.2074 - soft_f1_loss: 0.7814 - AUC: 0.5519 - global_accuracy: 0.6806 - global_precision: 0.4444 - global_recall: 0.3810 - val_loss: 10.2385 - val_macro_f1_score: 0.2667 - val_soft_f1_loss: 0.7348 - val_AUC: 0.5556 - val_global_accuracy: 0.6528 - val_global_precision: 0.4286 - val_global_recall: 0.5714\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.2385 - macro_f1_score: 0.2667 - soft_f1_loss: 0.7348 - AUC: 0.5556 - global_accuracy: 0.6528 - global_precision: 0.4286 - global_recall: 0.5714 - val_loss: 10.4011 - val_macro_f1_score: 0.1296 - val_soft_f1_loss: 0.8565 - val_AUC: 0.5000 - val_global_accuracy: 0.7222 - val_global_precision: 0.5556 - val_global_recall: 0.2381\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.4011 - macro_f1_score: 0.1296 - soft_f1_loss: 0.8565 - AUC: 0.5000 - global_accuracy: 0.7222 - global_precision: 0.5556 - global_recall: 0.2381 - val_loss: 8.2894 - val_macro_f1_score: 0.2791 - val_soft_f1_loss: 0.7477 - val_AUC: 0.5704 - val_global_accuracy: 0.7222 - val_global_precision: 0.5217 - val_global_recall: 0.5714\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.2894 - macro_f1_score: 0.2791 - soft_f1_loss: 0.7477 - AUC: 0.5704 - global_accuracy: 0.7222 - global_precision: 0.5217 - global_recall: 0.5714 - val_loss: 7.7051 - val_macro_f1_score: 0.3138 - val_soft_f1_loss: 0.6843 - val_AUC: 0.5093 - val_global_accuracy: 0.5972 - val_global_precision: 0.4000 - val_global_recall: 0.7619\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.7051 - macro_f1_score: 0.3138 - soft_f1_loss: 0.6843 - AUC: 0.5093 - global_accuracy: 0.5972 - global_precision: 0.4000 - global_recall: 0.7619 - val_loss: 5.5077 - val_macro_f1_score: 0.1347 - val_soft_f1_loss: 0.8641 - val_AUC: 0.4463 - val_global_accuracy: 0.6806 - val_global_precision: 0.4375 - val_global_recall: 0.3333\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5077 - macro_f1_score: 0.1347 - soft_f1_loss: 0.8641 - AUC: 0.4463 - global_accuracy: 0.6806 - global_precision: 0.4375 - global_recall: 0.3333 - val_loss: 5.5118 - val_macro_f1_score: 0.0889 - val_soft_f1_loss: 0.9119 - val_AUC: 0.5185 - val_global_accuracy: 0.7361 - val_global_precision: 0.6667 - val_global_recall: 0.1905\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5118 - macro_f1_score: 0.0889 - soft_f1_loss: 0.9119 - AUC: 0.5185 - global_accuracy: 0.7361 - global_precision: 0.6667 - global_recall: 0.1905 - val_loss: 4.8329 - val_macro_f1_score: 0.1630 - val_soft_f1_loss: 0.8407 - val_AUC: 0.6019 - val_global_accuracy: 0.7500 - val_global_precision: 1.0000 - val_global_recall: 0.1429\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8329 - macro_f1_score: 0.1630 - soft_f1_loss: 0.8407 - AUC: 0.6019 - global_accuracy: 0.7500 - global_precision: 1.0000 - global_recall: 0.1429 - val_loss: 6.2738 - val_macro_f1_score: 0.2532 - val_soft_f1_loss: 0.7223 - val_AUC: 0.5926 - val_global_accuracy: 0.6111 - val_global_precision: 0.3600 - val_global_recall: 0.4286\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.2738 - macro_f1_score: 0.2532 - soft_f1_loss: 0.7223 - AUC: 0.5926 - global_accuracy: 0.6111 - global_precision: 0.3600 - global_recall: 0.4286 - val_loss: 6.1078 - val_macro_f1_score: 0.2545 - val_soft_f1_loss: 0.7187 - val_AUC: 0.5299 - val_global_accuracy: 0.4861 - val_global_precision: 0.3000 - val_global_recall: 0.5714\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.1078 - macro_f1_score: 0.2545 - soft_f1_loss: 0.7187 - AUC: 0.5299 - global_accuracy: 0.4861 - global_precision: 0.3000 - global_recall: 0.5714 - val_loss: 3.3790 - val_macro_f1_score: 0.3185 - val_soft_f1_loss: 0.6681 - val_AUC: 0.6065 - val_global_accuracy: 0.7222 - val_global_precision: 0.5200 - val_global_recall: 0.6190\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3790 - macro_f1_score: 0.3185 - soft_f1_loss: 0.6681 - AUC: 0.6065 - global_accuracy: 0.7222 - global_precision: 0.5200 - global_recall: 0.6190 - val_loss: 5.7528 - val_macro_f1_score: 0.2000 - val_soft_f1_loss: 0.7969 - val_AUC: 0.5556 - val_global_accuracy: 0.6944 - val_global_precision: 0.4737 - val_global_recall: 0.4286\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7528 - macro_f1_score: 0.2000 - soft_f1_loss: 0.7969 - AUC: 0.5556 - global_accuracy: 0.6944 - global_precision: 0.4737 - global_recall: 0.4286 - val_loss: 6.0389 - val_macro_f1_score: 0.2138 - val_soft_f1_loss: 0.7723 - val_AUC: 0.6204 - val_global_accuracy: 0.6944 - val_global_precision: 0.4737 - val_global_recall: 0.4286\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0389 - macro_f1_score: 0.2138 - soft_f1_loss: 0.7723 - AUC: 0.6204 - global_accuracy: 0.6944 - global_precision: 0.4737 - global_recall: 0.4286 - val_loss: 4.3135 - val_macro_f1_score: 0.3327 - val_soft_f1_loss: 0.6263 - val_AUC: 0.6667 - val_global_accuracy: 0.7778 - val_global_precision: 0.6316 - val_global_recall: 0.5714\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3135 - macro_f1_score: 0.3327 - soft_f1_loss: 0.6263 - AUC: 0.6667 - global_accuracy: 0.7778 - global_precision: 0.6316 - global_recall: 0.5714 - val_loss: 3.7015 - val_macro_f1_score: 0.1926 - val_soft_f1_loss: 0.8049 - val_AUC: 0.6111 - val_global_accuracy: 0.6806 - val_global_precision: 0.4444 - val_global_recall: 0.3810\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7015 - macro_f1_score: 0.1926 - soft_f1_loss: 0.8049 - AUC: 0.6111 - global_accuracy: 0.6806 - global_precision: 0.4444 - global_recall: 0.3810 - val_loss: 2.3687 - val_macro_f1_score: 0.1852 - val_soft_f1_loss: 0.7979 - val_AUC: 0.6759 - val_global_accuracy: 0.7222 - val_global_precision: 0.5556 - val_global_recall: 0.2381\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3687 - macro_f1_score: 0.1852 - soft_f1_loss: 0.7979 - AUC: 0.6759 - global_accuracy: 0.7222 - global_precision: 0.5556 - global_recall: 0.2381 - val_loss: 5.3251 - val_macro_f1_score: 0.2939 - val_soft_f1_loss: 0.6987 - val_AUC: 0.5556 - val_global_accuracy: 0.6111 - val_global_precision: 0.3871 - val_global_recall: 0.5714\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3251 - macro_f1_score: 0.2939 - soft_f1_loss: 0.6987 - AUC: 0.5556 - global_accuracy: 0.6111 - global_precision: 0.3871 - global_recall: 0.5714 - val_loss: 4.1383 - val_macro_f1_score: 0.2384 - val_soft_f1_loss: 0.7524 - val_AUC: 0.6667 - val_global_accuracy: 0.5972 - val_global_precision: 0.3462 - val_global_recall: 0.4286\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1383 - macro_f1_score: 0.2384 - soft_f1_loss: 0.7524 - AUC: 0.6667 - global_accuracy: 0.5972 - global_precision: 0.3462 - global_recall: 0.4286 - val_loss: 4.2328 - val_macro_f1_score: 0.1791 - val_soft_f1_loss: 0.7957 - val_AUC: 0.5185 - val_global_accuracy: 0.6250 - val_global_precision: 0.3750 - val_global_recall: 0.4286\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2328 - macro_f1_score: 0.1791 - soft_f1_loss: 0.7957 - AUC: 0.5185 - global_accuracy: 0.6250 - global_precision: 0.3750 - global_recall: 0.4286 - val_loss: 4.1542 - val_macro_f1_score: 0.2519 - val_soft_f1_loss: 0.7525 - val_AUC: 0.6019 - val_global_accuracy: 0.7639 - val_global_precision: 0.6250 - val_global_recall: 0.4762\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1542 - macro_f1_score: 0.2519 - soft_f1_loss: 0.7525 - AUC: 0.6019 - global_accuracy: 0.7639 - global_precision: 0.6250 - global_recall: 0.4762 - val_loss: 5.3216 - val_macro_f1_score: 0.2088 - val_soft_f1_loss: 0.7857 - val_AUC: 0.5065 - val_global_accuracy: 0.6806 - val_global_precision: 0.4583 - val_global_recall: 0.5238\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3216 - macro_f1_score: 0.2088 - soft_f1_loss: 0.7857 - AUC: 0.5065 - global_accuracy: 0.6806 - global_precision: 0.4583 - global_recall: 0.5238 - val_loss: 4.0204 - val_macro_f1_score: 0.2155 - val_soft_f1_loss: 0.6984 - val_AUC: 0.5713 - val_global_accuracy: 0.6944 - val_global_precision: 0.4783 - val_global_recall: 0.5238\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0204 - macro_f1_score: 0.2155 - soft_f1_loss: 0.6984 - AUC: 0.5713 - global_accuracy: 0.6944 - global_precision: 0.4783 - global_recall: 0.5238 - val_loss: 2.9554 - val_macro_f1_score: 0.2914 - val_soft_f1_loss: 0.6989 - val_AUC: 0.6696 - val_global_accuracy: 0.6944 - val_global_precision: 0.4737 - val_global_recall: 0.4286\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9554 - macro_f1_score: 0.2914 - soft_f1_loss: 0.6989 - AUC: 0.6696 - global_accuracy: 0.6944 - global_precision: 0.4737 - global_recall: 0.4286 - val_loss: 4.0131 - val_macro_f1_score: 0.2680 - val_soft_f1_loss: 0.7079 - val_AUC: 0.5648 - val_global_accuracy: 0.6667 - val_global_precision: 0.4211 - val_global_recall: 0.3810\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0131 - macro_f1_score: 0.2680 - soft_f1_loss: 0.7079 - AUC: 0.5648 - global_accuracy: 0.6667 - global_precision: 0.4211 - global_recall: 0.3810 - val_loss: 3.4113 - val_macro_f1_score: 0.1902 - val_soft_f1_loss: 0.8315 - val_AUC: 0.5556 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.2381\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4113 - macro_f1_score: 0.1902 - soft_f1_loss: 0.8315 - AUC: 0.5556 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.2381 - val_loss: 2.4637 - val_macro_f1_score: 0.3939 - val_soft_f1_loss: 0.6244 - val_AUC: 0.6481 - val_global_accuracy: 0.7222 - val_global_precision: 0.5200 - val_global_recall: 0.6190\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4637 - macro_f1_score: 0.3939 - soft_f1_loss: 0.6244 - AUC: 0.6481 - global_accuracy: 0.7222 - global_precision: 0.5200 - global_recall: 0.6190 - val_loss: 1.8715 - val_macro_f1_score: 0.3845 - val_soft_f1_loss: 0.5759 - val_AUC: 0.6991 - val_global_accuracy: 0.6806 - val_global_precision: 0.4688 - val_global_recall: 0.7143\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8715 - macro_f1_score: 0.3845 - soft_f1_loss: 0.5759 - AUC: 0.6991 - global_accuracy: 0.6806 - global_precision: 0.4688 - global_recall: 0.7143 - val_loss: 1.6207 - val_macro_f1_score: 0.3098 - val_soft_f1_loss: 0.6932 - val_AUC: 0.6713 - val_global_accuracy: 0.7361 - val_global_precision: 0.5417 - val_global_recall: 0.6190\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6207 - macro_f1_score: 0.3098 - soft_f1_loss: 0.6932 - AUC: 0.6713 - global_accuracy: 0.7361 - global_precision: 0.5417 - global_recall: 0.6190 - val_loss: 1.8220 - val_macro_f1_score: 0.4582 - val_soft_f1_loss: 0.5416 - val_AUC: 0.7685 - val_global_accuracy: 0.8611 - val_global_precision: 0.9231 - val_global_recall: 0.5714\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8220 - macro_f1_score: 0.4582 - soft_f1_loss: 0.5416 - AUC: 0.7685 - global_accuracy: 0.8611 - global_precision: 0.9231 - global_recall: 0.5714 - val_loss: 2.5425 - val_macro_f1_score: 0.5249 - val_soft_f1_loss: 0.5256 - val_AUC: 0.7130 - val_global_accuracy: 0.8056 - val_global_precision: 0.6842 - val_global_recall: 0.6190\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5425 - macro_f1_score: 0.5249 - soft_f1_loss: 0.5256 - AUC: 0.7130 - global_accuracy: 0.8056 - global_precision: 0.6842 - global_recall: 0.6190 - val_loss: 1.9553 - val_macro_f1_score: 0.4453 - val_soft_f1_loss: 0.5588 - val_AUC: 0.7189 - val_global_accuracy: 0.7778 - val_global_precision: 0.6190 - val_global_recall: 0.6190\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9553 - macro_f1_score: 0.4453 - soft_f1_loss: 0.5588 - AUC: 0.7189 - global_accuracy: 0.7778 - global_precision: 0.6190 - global_recall: 0.6190 - val_loss: 0.7071 - val_macro_f1_score: 0.4290 - val_soft_f1_loss: 0.5738 - val_AUC: 0.7546 - val_global_accuracy: 0.8333 - val_global_precision: 0.8000 - val_global_recall: 0.5714\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7071 - macro_f1_score: 0.4290 - soft_f1_loss: 0.5738 - AUC: 0.7546 - global_accuracy: 0.8333 - global_precision: 0.8000 - global_recall: 0.5714 - val_loss: 2.1951 - val_macro_f1_score: 0.4952 - val_soft_f1_loss: 0.5098 - val_AUC: 0.7083 - val_global_accuracy: 0.6528 - val_global_precision: 0.4545 - val_global_recall: 0.9524\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1951 - macro_f1_score: 0.4952 - soft_f1_loss: 0.5098 - AUC: 0.7083 - global_accuracy: 0.6528 - global_precision: 0.4545 - global_recall: 0.9524 - val_loss: 1.8699 - val_macro_f1_score: 0.3791 - val_soft_f1_loss: 0.6178 - val_AUC: 0.6500 - val_global_accuracy: 0.7222 - val_global_precision: 0.5217 - val_global_recall: 0.5714\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8699 - macro_f1_score: 0.3791 - soft_f1_loss: 0.6178 - AUC: 0.6500 - global_accuracy: 0.7222 - global_precision: 0.5217 - global_recall: 0.5714 - val_loss: 2.0521 - val_macro_f1_score: 0.2593 - val_soft_f1_loss: 0.6774 - val_AUC: 0.6759 - val_global_accuracy: 0.7778 - val_global_precision: 1.0000 - val_global_recall: 0.2381\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0521 - macro_f1_score: 0.2593 - soft_f1_loss: 0.6774 - AUC: 0.6759 - global_accuracy: 0.7778 - global_precision: 1.0000 - global_recall: 0.2381 - val_loss: 1.7469 - val_macro_f1_score: 0.3556 - val_soft_f1_loss: 0.6470 - val_AUC: 0.7315 - val_global_accuracy: 0.7917 - val_global_precision: 0.8750 - val_global_recall: 0.3333\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7469 - macro_f1_score: 0.3556 - soft_f1_loss: 0.6470 - AUC: 0.7315 - global_accuracy: 0.7917 - global_precision: 0.8750 - global_recall: 0.3333 - val_loss: 1.5252 - val_macro_f1_score: 0.6000 - val_soft_f1_loss: 0.4151 - val_AUC: 0.7500 - val_global_accuracy: 0.8194 - val_global_precision: 0.6667 - val_global_recall: 0.7619\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5252 - macro_f1_score: 0.6000 - soft_f1_loss: 0.4151 - AUC: 0.7500 - global_accuracy: 0.8194 - global_precision: 0.6667 - global_recall: 0.7619 - val_loss: 2.3246 - val_macro_f1_score: 0.5192 - val_soft_f1_loss: 0.4907 - val_AUC: 0.6954 - val_global_accuracy: 0.6667 - val_global_precision: 0.4615 - val_global_recall: 0.8571\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3246 - macro_f1_score: 0.5192 - soft_f1_loss: 0.4907 - AUC: 0.6954 - global_accuracy: 0.6667 - global_precision: 0.4615 - global_recall: 0.8571 - val_loss: 2.2007 - val_macro_f1_score: 0.3440 - val_soft_f1_loss: 0.6404 - val_AUC: 0.6217 - val_global_accuracy: 0.6944 - val_global_precision: 0.4815 - val_global_recall: 0.6190\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2007 - macro_f1_score: 0.3440 - soft_f1_loss: 0.6404 - AUC: 0.6217 - global_accuracy: 0.6944 - global_precision: 0.4815 - global_recall: 0.6190 - val_loss: 1.4667 - val_macro_f1_score: 0.2660 - val_soft_f1_loss: 0.7172 - val_AUC: 0.6398 - val_global_accuracy: 0.7639 - val_global_precision: 0.7000 - val_global_recall: 0.3333\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4667 - macro_f1_score: 0.2660 - soft_f1_loss: 0.7172 - AUC: 0.6398 - global_accuracy: 0.7639 - global_precision: 0.7000 - global_recall: 0.3333 - val_loss: 2.1166 - val_macro_f1_score: 0.2741 - val_soft_f1_loss: 0.7240 - val_AUC: 0.6528 - val_global_accuracy: 0.6667 - val_global_precision: 0.4286 - val_global_recall: 0.4286\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1166 - macro_f1_score: 0.2741 - soft_f1_loss: 0.7240 - AUC: 0.6528 - global_accuracy: 0.6667 - global_precision: 0.4286 - global_recall: 0.4286 - val_loss: 1.6914 - val_macro_f1_score: 0.4508 - val_soft_f1_loss: 0.5514 - val_AUC: 0.7407 - val_global_accuracy: 0.7361 - val_global_precision: 0.5417 - val_global_recall: 0.6190\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6914 - macro_f1_score: 0.4508 - soft_f1_loss: 0.5514 - AUC: 0.7407 - global_accuracy: 0.7361 - global_precision: 0.5417 - global_recall: 0.6190 - val_loss: 1.2429 - val_macro_f1_score: 0.4938 - val_soft_f1_loss: 0.5226 - val_AUC: 0.7546 - val_global_accuracy: 0.8194 - val_global_precision: 0.7000 - val_global_recall: 0.6667\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2429 - macro_f1_score: 0.4938 - soft_f1_loss: 0.5226 - AUC: 0.7546 - global_accuracy: 0.8194 - global_precision: 0.7000 - global_recall: 0.6667 - val_loss: 1.7464 - val_macro_f1_score: 0.3981 - val_soft_f1_loss: 0.6241 - val_AUC: 0.6944 - val_global_accuracy: 0.8056 - val_global_precision: 0.7059 - val_global_recall: 0.5714\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7464 - macro_f1_score: 0.3981 - soft_f1_loss: 0.6241 - AUC: 0.6944 - global_accuracy: 0.8056 - global_precision: 0.7059 - global_recall: 0.5714 - val_loss: 1.4966 - val_macro_f1_score: 0.4533 - val_soft_f1_loss: 0.5775 - val_AUC: 0.6852 - val_global_accuracy: 0.8472 - val_global_precision: 0.7778 - val_global_recall: 0.6667\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4966 - macro_f1_score: 0.4533 - soft_f1_loss: 0.5775 - AUC: 0.6852 - global_accuracy: 0.8472 - global_precision: 0.7778 - global_recall: 0.6667 - val_loss: 1.1007 - val_macro_f1_score: 0.4833 - val_soft_f1_loss: 0.5465 - val_AUC: 0.7917 - val_global_accuracy: 0.8472 - val_global_precision: 0.8125 - val_global_recall: 0.6190\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1007 - macro_f1_score: 0.4833 - soft_f1_loss: 0.5465 - AUC: 0.7917 - global_accuracy: 0.8472 - global_precision: 0.8125 - global_recall: 0.6190 - val_loss: 0.7268 - val_macro_f1_score: 0.5360 - val_soft_f1_loss: 0.4495 - val_AUC: 0.7963 - val_global_accuracy: 0.8472 - val_global_precision: 0.7778 - val_global_recall: 0.6667\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7268 - macro_f1_score: 0.5360 - soft_f1_loss: 0.4495 - AUC: 0.7963 - global_accuracy: 0.8472 - global_precision: 0.7778 - global_recall: 0.6667 - val_loss: 1.2108 - val_macro_f1_score: 0.4734 - val_soft_f1_loss: 0.5254 - val_AUC: 0.7241 - val_global_accuracy: 0.7778 - val_global_precision: 0.6000 - val_global_recall: 0.7143\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2108 - macro_f1_score: 0.4734 - soft_f1_loss: 0.5254 - AUC: 0.7241 - global_accuracy: 0.7778 - global_precision: 0.6000 - global_recall: 0.7143 - val_loss: 1.0232 - val_macro_f1_score: 0.5197 - val_soft_f1_loss: 0.4835 - val_AUC: 0.7602 - val_global_accuracy: 0.7361 - val_global_precision: 0.5294 - val_global_recall: 0.8571\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0232 - macro_f1_score: 0.5197 - soft_f1_loss: 0.4835 - AUC: 0.7602 - global_accuracy: 0.7361 - global_precision: 0.5294 - global_recall: 0.8571 - val_loss: 0.3765 - val_macro_f1_score: 0.6747 - val_soft_f1_loss: 0.3314 - val_AUC: 0.8611 - val_global_accuracy: 0.8611 - val_global_precision: 0.7619 - val_global_recall: 0.7619\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3765 - macro_f1_score: 0.6747 - soft_f1_loss: 0.3314 - AUC: 0.8611 - global_accuracy: 0.8611 - global_precision: 0.7619 - global_recall: 0.7619 - val_loss: 0.8176 - val_macro_f1_score: 0.5494 - val_soft_f1_loss: 0.4695 - val_AUC: 0.8287 - val_global_accuracy: 0.7778 - val_global_precision: 0.6316 - val_global_recall: 0.5714\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8176 - macro_f1_score: 0.5494 - soft_f1_loss: 0.4695 - AUC: 0.8287 - global_accuracy: 0.7778 - global_precision: 0.6316 - global_recall: 0.5714 - val_loss: 0.8869 - val_macro_f1_score: 0.4878 - val_soft_f1_loss: 0.5107 - val_AUC: 0.7870 - val_global_accuracy: 0.8194 - val_global_precision: 0.9000 - val_global_recall: 0.4286\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8869 - macro_f1_score: 0.4878 - soft_f1_loss: 0.5107 - AUC: 0.7870 - global_accuracy: 0.8194 - global_precision: 0.9000 - global_recall: 0.4286 - val_loss: 0.9426 - val_macro_f1_score: 0.4593 - val_soft_f1_loss: 0.5247 - val_AUC: 0.7731 - val_global_accuracy: 0.8056 - val_global_precision: 0.6842 - val_global_recall: 0.6190\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9426 - macro_f1_score: 0.4593 - soft_f1_loss: 0.5247 - AUC: 0.7731 - global_accuracy: 0.8056 - global_precision: 0.6842 - global_recall: 0.6190 - val_loss: 0.4358 - val_macro_f1_score: 0.5938 - val_soft_f1_loss: 0.4035 - val_AUC: 0.8611 - val_global_accuracy: 0.8611 - val_global_precision: 0.7391 - val_global_recall: 0.8095\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4358 - macro_f1_score: 0.5938 - soft_f1_loss: 0.4035 - AUC: 0.8611 - global_accuracy: 0.8611 - global_precision: 0.7391 - global_recall: 0.8095 - val_loss: 0.6219 - val_macro_f1_score: 0.6212 - val_soft_f1_loss: 0.3978 - val_AUC: 0.8611 - val_global_accuracy: 0.8194 - val_global_precision: 0.6538 - val_global_recall: 0.8095\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6219 - macro_f1_score: 0.6212 - soft_f1_loss: 0.3978 - AUC: 0.8611 - global_accuracy: 0.8194 - global_precision: 0.6538 - global_recall: 0.8095 - val_loss: 0.4677 - val_macro_f1_score: 0.5771 - val_soft_f1_loss: 0.4193 - val_AUC: 0.8148 - val_global_accuracy: 0.8750 - val_global_precision: 0.8333 - val_global_recall: 0.7143\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4677 - macro_f1_score: 0.5771 - soft_f1_loss: 0.4193 - AUC: 0.8148 - global_accuracy: 0.8750 - global_precision: 0.8333 - global_recall: 0.7143 - val_loss: 0.8485 - val_macro_f1_score: 0.5878 - val_soft_f1_loss: 0.4347 - val_AUC: 0.7926 - val_global_accuracy: 0.8056 - val_global_precision: 0.6296 - val_global_recall: 0.8095\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8485 - macro_f1_score: 0.5878 - soft_f1_loss: 0.4347 - AUC: 0.7926 - global_accuracy: 0.8056 - global_precision: 0.6296 - global_recall: 0.8095 - val_loss: 0.6708 - val_macro_f1_score: 0.4414 - val_soft_f1_loss: 0.5480 - val_AUC: 0.8148 - val_global_accuracy: 0.8333 - val_global_precision: 0.7647 - val_global_recall: 0.6190\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6708 - macro_f1_score: 0.4414 - soft_f1_loss: 0.5480 - AUC: 0.8148 - global_accuracy: 0.8333 - global_precision: 0.7647 - global_recall: 0.6190 - val_loss: 0.6303 - val_macro_f1_score: 0.5341 - val_soft_f1_loss: 0.4431 - val_AUC: 0.8389 - val_global_accuracy: 0.8333 - val_global_precision: 0.7647 - val_global_recall: 0.6190\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6303 - macro_f1_score: 0.5341 - soft_f1_loss: 0.4431 - AUC: 0.8389 - global_accuracy: 0.8333 - global_precision: 0.7647 - global_recall: 0.6190 - val_loss: 0.4468 - val_macro_f1_score: 0.6286 - val_soft_f1_loss: 0.3475 - val_AUC: 0.8426 - val_global_accuracy: 0.8889 - val_global_precision: 0.8095 - val_global_recall: 0.8095\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4468 - macro_f1_score: 0.6286 - soft_f1_loss: 0.3475 - AUC: 0.8426 - global_accuracy: 0.8889 - global_precision: 0.8095 - global_recall: 0.8095 - val_loss: 0.3666 - val_macro_f1_score: 0.6926 - val_soft_f1_loss: 0.3298 - val_AUC: 0.8426 - val_global_accuracy: 0.8889 - val_global_precision: 0.8421 - val_global_recall: 0.7619\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3666 - macro_f1_score: 0.6926 - soft_f1_loss: 0.3298 - AUC: 0.8426 - global_accuracy: 0.8889 - global_precision: 0.8421 - global_recall: 0.7619 - val_loss: 0.3357 - val_macro_f1_score: 0.7444 - val_soft_f1_loss: 0.3199 - val_AUC: 0.8704 - val_global_accuracy: 0.9028 - val_global_precision: 0.8182 - val_global_recall: 0.8571\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3357 - macro_f1_score: 0.7444 - soft_f1_loss: 0.3199 - AUC: 0.8704 - global_accuracy: 0.9028 - global_precision: 0.8182 - global_recall: 0.8571 - val_loss: 0.4619 - val_macro_f1_score: 0.6138 - val_soft_f1_loss: 0.3602 - val_AUC: 0.8611 - val_global_accuracy: 0.8333 - val_global_precision: 0.6667 - val_global_recall: 0.8571\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4619 - macro_f1_score: 0.6138 - soft_f1_loss: 0.3602 - AUC: 0.8611 - global_accuracy: 0.8333 - global_precision: 0.6667 - global_recall: 0.8571 - val_loss: 0.6563 - val_macro_f1_score: 0.5660 - val_soft_f1_loss: 0.4367 - val_AUC: 0.8009 - val_global_accuracy: 0.8472 - val_global_precision: 0.7778 - val_global_recall: 0.6667\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6563 - macro_f1_score: 0.5660 - soft_f1_loss: 0.4367 - AUC: 0.8009 - global_accuracy: 0.8472 - global_precision: 0.7778 - global_recall: 0.6667 - val_loss: 0.3804 - val_macro_f1_score: 0.7771 - val_soft_f1_loss: 0.3019 - val_AUC: 0.8426 - val_global_accuracy: 0.9028 - val_global_precision: 0.7692 - val_global_recall: 0.9524\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3804 - macro_f1_score: 0.7771 - soft_f1_loss: 0.3019 - AUC: 0.8426 - global_accuracy: 0.9028 - global_precision: 0.7692 - global_recall: 0.9524 - val_loss: 0.5477 - val_macro_f1_score: 0.5715 - val_soft_f1_loss: 0.4283 - val_AUC: 0.8426 - val_global_accuracy: 0.8333 - val_global_precision: 0.7368 - val_global_recall: 0.6667\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5477 - macro_f1_score: 0.5715 - soft_f1_loss: 0.4283 - AUC: 0.8426 - global_accuracy: 0.8333 - global_precision: 0.7368 - global_recall: 0.6667 - val_loss: 0.5003 - val_macro_f1_score: 0.5648 - val_soft_f1_loss: 0.4414 - val_AUC: 0.8750 - val_global_accuracy: 0.8472 - val_global_precision: 0.8571 - val_global_recall: 0.5714\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5003 - macro_f1_score: 0.5648 - soft_f1_loss: 0.4414 - AUC: 0.8750 - global_accuracy: 0.8472 - global_precision: 0.8571 - global_recall: 0.5714 - val_loss: 0.3634 - val_macro_f1_score: 0.6644 - val_soft_f1_loss: 0.3623 - val_AUC: 0.8519 - val_global_accuracy: 0.8750 - val_global_precision: 0.8333 - val_global_recall: 0.7143\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3634 - macro_f1_score: 0.6644 - soft_f1_loss: 0.3623 - AUC: 0.8519 - global_accuracy: 0.8750 - global_precision: 0.8333 - global_recall: 0.7143 - val_loss: 0.3364 - val_macro_f1_score: 0.7711 - val_soft_f1_loss: 0.2566 - val_AUC: 0.8750 - val_global_accuracy: 0.9028 - val_global_precision: 0.7692 - val_global_recall: 0.9524\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3364 - macro_f1_score: 0.7711 - soft_f1_loss: 0.2566 - AUC: 0.8750 - global_accuracy: 0.9028 - global_precision: 0.7692 - global_recall: 0.9524 - val_loss: 0.3113 - val_macro_f1_score: 0.7919 - val_soft_f1_loss: 0.2799 - val_AUC: 0.8750 - val_global_accuracy: 0.9167 - val_global_precision: 0.8000 - val_global_recall: 0.9524\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3113 - macro_f1_score: 0.7919 - soft_f1_loss: 0.2799 - AUC: 0.8750 - global_accuracy: 0.9167 - global_precision: 0.8000 - global_recall: 0.9524 - val_loss: 0.3365 - val_macro_f1_score: 0.6222 - val_soft_f1_loss: 0.3838 - val_AUC: 0.8519 - val_global_accuracy: 0.9028 - val_global_precision: 0.8889 - val_global_recall: 0.7619\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3365 - macro_f1_score: 0.6222 - soft_f1_loss: 0.3838 - AUC: 0.8519 - global_accuracy: 0.9028 - global_precision: 0.8889 - global_recall: 0.7619 - val_loss: 0.2469 - val_macro_f1_score: 0.6852 - val_soft_f1_loss: 0.3223 - val_AUC: 0.8796 - val_global_accuracy: 0.8889 - val_global_precision: 0.9333 - val_global_recall: 0.6667\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2469 - macro_f1_score: 0.6852 - soft_f1_loss: 0.3223 - AUC: 0.8796 - global_accuracy: 0.8889 - global_precision: 0.9333 - global_recall: 0.6667 - val_loss: 0.3054 - val_macro_f1_score: 0.6533 - val_soft_f1_loss: 0.3086 - val_AUC: 0.8796 - val_global_accuracy: 0.9028 - val_global_precision: 0.8182 - val_global_recall: 0.8571\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3054 - macro_f1_score: 0.6533 - soft_f1_loss: 0.3086 - AUC: 0.8796 - global_accuracy: 0.9028 - global_precision: 0.8182 - global_recall: 0.8571 - val_loss: 0.2090 - val_macro_f1_score: 0.7673 - val_soft_f1_loss: 0.2434 - val_AUC: 0.8796 - val_global_accuracy: 0.9167 - val_global_precision: 0.8261 - val_global_recall: 0.9048\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2090 - macro_f1_score: 0.7673 - soft_f1_loss: 0.2434 - AUC: 0.8796 - global_accuracy: 0.9167 - global_precision: 0.8261 - global_recall: 0.9048 - val_loss: 0.2728 - val_macro_f1_score: 0.6582 - val_soft_f1_loss: 0.3519 - val_AUC: 0.8565 - val_global_accuracy: 0.9028 - val_global_precision: 1.0000 - val_global_recall: 0.6667\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2728 - macro_f1_score: 0.6582 - soft_f1_loss: 0.3519 - AUC: 0.8565 - global_accuracy: 0.9028 - global_precision: 1.0000 - global_recall: 0.6667 - val_loss: 0.1549 - val_macro_f1_score: 0.7556 - val_soft_f1_loss: 0.2840 - val_AUC: 0.8796 - val_global_accuracy: 0.9583 - val_global_precision: 1.0000 - val_global_recall: 0.8571\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1549 - macro_f1_score: 0.7556 - soft_f1_loss: 0.2840 - AUC: 0.8796 - global_accuracy: 0.9583 - global_precision: 1.0000 - global_recall: 0.8571 - val_loss: 0.3631 - val_macro_f1_score: 0.6427 - val_soft_f1_loss: 0.3000 - val_AUC: 0.8796 - val_global_accuracy: 0.8333 - val_global_precision: 0.6452 - val_global_recall: 0.9524\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3631 - macro_f1_score: 0.6427 - soft_f1_loss: 0.3000 - AUC: 0.8796 - global_accuracy: 0.8333 - global_precision: 0.6452 - global_recall: 0.9524 - val_loss: 0.2953 - val_macro_f1_score: 0.6607 - val_soft_f1_loss: 0.3716 - val_AUC: 0.8796 - val_global_accuracy: 0.8750 - val_global_precision: 0.7500 - val_global_recall: 0.8571\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2953 - macro_f1_score: 0.6607 - soft_f1_loss: 0.3716 - AUC: 0.8796 - global_accuracy: 0.8750 - global_precision: 0.7500 - global_recall: 0.8571 - val_loss: 0.2179 - val_macro_f1_score: 0.7175 - val_soft_f1_loss: 0.3148 - val_AUC: 0.8843 - val_global_accuracy: 0.9306 - val_global_precision: 0.9444 - val_global_recall: 0.8095\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2179 - macro_f1_score: 0.7175 - soft_f1_loss: 0.3148 - AUC: 0.8843 - global_accuracy: 0.9306 - global_precision: 0.9444 - global_recall: 0.8095 - val_loss: 0.3523 - val_macro_f1_score: 0.6049 - val_soft_f1_loss: 0.3396 - val_AUC: 0.8796 - val_global_accuracy: 0.8472 - val_global_precision: 0.7273 - val_global_recall: 0.7619\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3523 - macro_f1_score: 0.6049 - soft_f1_loss: 0.3396 - AUC: 0.8796 - global_accuracy: 0.8472 - global_precision: 0.7273 - global_recall: 0.7619 - val_loss: 0.3457 - val_macro_f1_score: 0.6407 - val_soft_f1_loss: 0.3652 - val_AUC: 0.8889 - val_global_accuracy: 0.9028 - val_global_precision: 0.9375 - val_global_recall: 0.7143\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3457 - macro_f1_score: 0.6407 - soft_f1_loss: 0.3652 - AUC: 0.8889 - global_accuracy: 0.9028 - global_precision: 0.9375 - global_recall: 0.7143 - val_loss: 0.2636 - val_macro_f1_score: 0.6815 - val_soft_f1_loss: 0.3228 - val_AUC: 0.8796 - val_global_accuracy: 0.9167 - val_global_precision: 0.8947 - val_global_recall: 0.8095\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2636 - macro_f1_score: 0.6815 - soft_f1_loss: 0.3228 - AUC: 0.8796 - global_accuracy: 0.9167 - global_precision: 0.8947 - global_recall: 0.8095 - val_loss: 0.1406 - val_macro_f1_score: 0.7796 - val_soft_f1_loss: 0.2337 - val_AUC: 0.8889 - val_global_accuracy: 0.9167 - val_global_precision: 0.7778 - val_global_recall: 1.0000\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1406 - macro_f1_score: 0.7796 - soft_f1_loss: 0.2337 - AUC: 0.8889 - global_accuracy: 0.9167 - global_precision: 0.7778 - global_recall: 1.0000 - val_loss: 0.2728 - val_macro_f1_score: 0.6709 - val_soft_f1_loss: 0.3265 - val_AUC: 0.8333 - val_global_accuracy: 0.8889 - val_global_precision: 0.7826 - val_global_recall: 0.8571\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2728 - macro_f1_score: 0.6709 - soft_f1_loss: 0.3265 - AUC: 0.8333 - global_accuracy: 0.8889 - global_precision: 0.7826 - global_recall: 0.8571 - val_loss: 0.3418 - val_macro_f1_score: 0.5704 - val_soft_f1_loss: 0.4197 - val_AUC: 0.8333 - val_global_accuracy: 0.9028 - val_global_precision: 0.9375 - val_global_recall: 0.7143\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3418 - macro_f1_score: 0.5704 - soft_f1_loss: 0.4197 - AUC: 0.8333 - global_accuracy: 0.9028 - global_precision: 0.9375 - global_recall: 0.7143 - val_loss: 0.1948 - val_macro_f1_score: 0.7835 - val_soft_f1_loss: 0.2483 - val_AUC: 0.8889 - val_global_accuracy: 0.9028 - val_global_precision: 0.7692 - val_global_recall: 0.9524\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1948 - macro_f1_score: 0.7835 - soft_f1_loss: 0.2483 - AUC: 0.8889 - global_accuracy: 0.9028 - global_precision: 0.7692 - global_recall: 0.9524 - val_loss: 0.1957 - val_macro_f1_score: 0.6932 - val_soft_f1_loss: 0.2988 - val_AUC: 0.8889 - val_global_accuracy: 0.9167 - val_global_precision: 0.8000 - val_global_recall: 0.9524\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1957 - macro_f1_score: 0.6932 - soft_f1_loss: 0.2988 - AUC: 0.8889 - global_accuracy: 0.9167 - global_precision: 0.8000 - global_recall: 0.9524 - val_loss: 0.1316 - val_macro_f1_score: 0.7841 - val_soft_f1_loss: 0.2608 - val_AUC: 0.8889 - val_global_accuracy: 0.9306 - val_global_precision: 0.9444 - val_global_recall: 0.8095\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1316 - macro_f1_score: 0.7841 - soft_f1_loss: 0.2608 - AUC: 0.8889 - global_accuracy: 0.9306 - global_precision: 0.9444 - global_recall: 0.8095 - val_loss: 0.1736 - val_macro_f1_score: 0.7582 - val_soft_f1_loss: 0.2956 - val_AUC: 0.8889 - val_global_accuracy: 0.9306 - val_global_precision: 1.0000 - val_global_recall: 0.7619\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1736 - macro_f1_score: 0.7582 - soft_f1_loss: 0.2956 - AUC: 0.8889 - global_accuracy: 0.9306 - global_precision: 1.0000 - global_recall: 0.7619 - val_loss: 0.1271 - val_macro_f1_score: 0.8444 - val_soft_f1_loss: 0.1948 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 0.8750 - val_global_recall: 1.0000\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1271 - macro_f1_score: 0.8444 - soft_f1_loss: 0.1948 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 0.8750 - global_recall: 1.0000 - val_loss: 0.1737 - val_macro_f1_score: 0.8014 - val_soft_f1_loss: 0.2223 - val_AUC: 0.8889 - val_global_accuracy: 0.9306 - val_global_precision: 0.8077 - val_global_recall: 1.0000\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1737 - macro_f1_score: 0.8014 - soft_f1_loss: 0.2223 - AUC: 0.8889 - global_accuracy: 0.9306 - global_precision: 0.8077 - global_recall: 1.0000 - val_loss: 0.1803 - val_macro_f1_score: 0.6667 - val_soft_f1_loss: 0.3092 - val_AUC: 0.8889 - val_global_accuracy: 0.9028 - val_global_precision: 1.0000 - val_global_recall: 0.6667\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1803 - macro_f1_score: 0.6667 - soft_f1_loss: 0.3092 - AUC: 0.8889 - global_accuracy: 0.9028 - global_precision: 1.0000 - global_recall: 0.6667 - val_loss: 0.0593 - val_macro_f1_score: 0.8519 - val_soft_f1_loss: 0.2059 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 1.0000 - val_global_recall: 0.9524\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0593 - macro_f1_score: 0.8519 - soft_f1_loss: 0.2059 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 1.0000 - global_recall: 0.9524 - val_loss: 0.1461 - val_macro_f1_score: 0.8004 - val_soft_f1_loss: 0.2169 - val_AUC: 0.8889 - val_global_accuracy: 0.9306 - val_global_precision: 0.8077 - val_global_recall: 1.0000\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1461 - macro_f1_score: 0.8004 - soft_f1_loss: 0.2169 - AUC: 0.8889 - global_accuracy: 0.9306 - global_precision: 0.8077 - global_recall: 1.0000 - val_loss: 0.0343 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1632 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0343 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1632 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.1036 - val_macro_f1_score: 0.7026 - val_soft_f1_loss: 0.2560 - val_AUC: 0.8889 - val_global_accuracy: 0.9306 - val_global_precision: 1.0000 - val_global_recall: 0.7619\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1036 - macro_f1_score: 0.7026 - soft_f1_loss: 0.2560 - AUC: 0.8889 - global_accuracy: 0.9306 - global_precision: 1.0000 - global_recall: 0.7619 - val_loss: 0.0497 - val_macro_f1_score: 0.8730 - val_soft_f1_loss: 0.1652 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 0.9545 - val_global_recall: 1.0000\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0497 - macro_f1_score: 0.8730 - soft_f1_loss: 0.1652 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 0.9545 - global_recall: 1.0000 - val_loss: 0.0817 - val_macro_f1_score: 0.8384 - val_soft_f1_loss: 0.1804 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 0.8750 - val_global_recall: 1.0000\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0817 - macro_f1_score: 0.8384 - soft_f1_loss: 0.1804 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 0.8750 - global_recall: 1.0000 - val_loss: 0.0361 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1624 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0361 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1624 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0815 - val_macro_f1_score: 0.8333 - val_soft_f1_loss: 0.2058 - val_AUC: 0.8889 - val_global_accuracy: 0.9722 - val_global_precision: 1.0000 - val_global_recall: 0.9048\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0815 - macro_f1_score: 0.8333 - soft_f1_loss: 0.2058 - AUC: 0.8889 - global_accuracy: 0.9722 - global_precision: 1.0000 - global_recall: 0.9048 - val_loss: 0.0435 - val_macro_f1_score: 0.8667 - val_soft_f1_loss: 0.1656 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 0.9545 - val_global_recall: 1.0000\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0435 - macro_f1_score: 0.8667 - soft_f1_loss: 0.1656 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 0.9545 - global_recall: 1.0000 - val_loss: 0.0619 - val_macro_f1_score: 0.8508 - val_soft_f1_loss: 0.1704 - val_AUC: 0.8889 - val_global_accuracy: 0.9722 - val_global_precision: 0.9130 - val_global_recall: 1.0000\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0619 - macro_f1_score: 0.8508 - soft_f1_loss: 0.1704 - AUC: 0.8889 - global_accuracy: 0.9722 - global_precision: 0.9130 - global_recall: 1.0000 - val_loss: 0.0264 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1509 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0264 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1509 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0640 - val_macro_f1_score: 0.7556 - val_soft_f1_loss: 0.2061 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 1.0000 - val_global_recall: 0.8571\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0640 - macro_f1_score: 0.7556 - soft_f1_loss: 0.2061 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 1.0000 - global_recall: 0.8571 - val_loss: 0.0313 - val_macro_f1_score: 0.8730 - val_soft_f1_loss: 0.1488 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 0.9545 - val_global_recall: 1.0000\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0313 - macro_f1_score: 0.8730 - soft_f1_loss: 0.1488 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 0.9545 - global_recall: 1.0000 - val_loss: 0.0557 - val_macro_f1_score: 0.8384 - val_soft_f1_loss: 0.1614 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 0.8750 - val_global_recall: 1.0000\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0557 - macro_f1_score: 0.8384 - soft_f1_loss: 0.1614 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 0.8750 - global_recall: 1.0000 - val_loss: 0.0232 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1453 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0232 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1453 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0598 - val_macro_f1_score: 0.8508 - val_soft_f1_loss: 0.1851 - val_AUC: 0.8889 - val_global_accuracy: 0.9722 - val_global_precision: 1.0000 - val_global_recall: 0.9048\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0598 - macro_f1_score: 0.8508 - soft_f1_loss: 0.1851 - AUC: 0.8889 - global_accuracy: 0.9722 - global_precision: 1.0000 - global_recall: 0.9048 - val_loss: 0.0259 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1482 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0259 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1482 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0529 - val_macro_f1_score: 0.8384 - val_soft_f1_loss: 0.1637 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 0.8750 - val_global_recall: 1.0000\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0529 - macro_f1_score: 0.8384 - soft_f1_loss: 0.1637 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 0.8750 - global_recall: 1.0000 - val_loss: 0.0191 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1409 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0191 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1409 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0494 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1791 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0494 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1791 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0194 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1400 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0194 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1400 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0407 - val_macro_f1_score: 0.8607 - val_soft_f1_loss: 0.1546 - val_AUC: 0.8889 - val_global_accuracy: 0.9722 - val_global_precision: 0.9130 - val_global_recall: 1.0000\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - macro_f1_score: 0.8607 - soft_f1_loss: 0.1546 - AUC: 0.8889 - global_accuracy: 0.9722 - global_precision: 0.9130 - global_recall: 1.0000 - val_loss: 0.0188 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1383 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0188 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1383 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0305 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1533 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0305 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1533 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0227 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1443 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0227 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1443 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0221 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1429 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0221 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1429 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0251 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1446 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0251 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1446 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0164 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1367 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0164 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1367 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0245 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1482 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0245 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1482 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0166 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1371 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0166 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1371 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0193 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1373 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0193 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1373 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0190 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1373 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0190 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1373 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0149 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1342 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0149 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1342 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0197 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1403 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0197 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1403 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0153 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1347 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0153 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1347 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0162 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1354 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0162 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1354 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0168 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1356 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0168 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1356 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0137 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1326 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1326 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0160 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1363 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0160 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1363 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0146 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1343 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0146 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1343 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0137 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1321 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1321 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0149 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1334 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0149 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1334 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0133 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1321 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1321 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0138 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1328 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1328 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0140 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1330 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0140 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1330 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0128 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1313 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0128 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1313 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0135 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1317 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1317 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0130 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1312 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0130 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1312 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0124 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1308 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0124 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1308 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0131 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1317 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1317 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0125 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1309 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0125 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1309 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0122 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1303 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1303 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0126 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1306 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0126 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1306 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0121 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1301 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1301 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0120 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1302 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0120 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1302 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0122 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1304 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1304 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0117 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1298 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1298 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0118 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1297 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1297 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0118 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1296 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1296 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0115 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1294 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1294 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0116 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1296 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0116 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1296 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0115 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1295 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1295 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0113 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1291 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1291 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0113 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1291 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1291 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0113 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1290 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1290 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0111 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1288 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1288 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0111 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1289 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1289 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0110 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1288 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1288 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0109 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1285 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1285 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0109 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1285 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1285 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0108 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1284 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1284 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0107 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1283 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1283 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0107 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1284 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1284 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0107 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1282 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1282 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0106 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1281 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0106 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1281 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0106 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1280 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0106 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1280 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0105 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1279 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1279 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0104 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1279 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0104 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1279 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0104 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1278 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0104 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1278 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0103 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1277 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1277 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0103 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1276 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1276 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0103 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1275 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1275 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0102 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1275 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1275 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0101 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1274 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0101 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1274 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0101 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1274 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0101 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1274 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0100 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1273 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1273 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0100 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1272 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1272 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0100 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1271 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1271 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0099 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1270 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0099 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1270 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0099 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1270 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0099 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1270 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0098 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1269 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1269 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0098 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1269 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1269 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0097 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1268 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0097 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1268 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0097 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1267 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0097 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1267 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0097 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1267 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0097 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1267 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0096 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1266 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0096 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1266 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0096 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1265 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0096 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1265 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0095 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1265 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0095 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1265 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0095 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1264 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0095 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1264 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0094 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1263 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0094 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1263 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0094 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1263 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0094 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1263 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0094 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1262 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0094 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1262 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0093 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1262 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0093 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1262 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0093 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1261 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0093 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1261 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0092 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1260 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0092 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1260 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0092 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1260 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0092 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1260 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0092 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1259 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0092 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1259 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0091 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1259 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0091 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1259 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0091 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1258 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0091 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1258 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0091 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1258 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0091 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1258 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0090 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1257 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0090 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1257 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0090 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1256 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0090 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1256 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0090 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1256 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0090 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1256 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0089 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1255 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0089 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1255 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0089 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1255 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0089 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1255 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0088 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1254 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0088 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1254 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0088 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1254 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0088 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1254 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0088 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1253 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0088 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1253 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0087 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1253 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0087 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1253 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0087 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1252 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0087 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1252 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0087 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1252 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0087 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1252 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0086 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1251 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0086 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1251 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0086 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1250 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0086 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1250 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0086 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1250 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0086 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1250 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0085 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1249 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0085 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1249 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0085 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1249 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0085 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1249 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0085 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1248 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0085 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1248 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0084 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1248 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0084 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1248 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0084 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1247 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0084 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1247 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0084 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1247 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0084 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1247 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0084 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1246 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0084 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1246 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0083 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1246 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0083 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1246 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0083 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1245 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0083 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1245 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0083 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1245 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0083 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1245 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0082 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1244 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0082 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1244 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0082 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1244 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0082 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1244 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0082 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1244 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0082 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1244 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0081 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1243 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0081 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1243 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0081 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1243 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0081 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1243 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0081 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1242 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0081 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1242 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0080 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1242 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0080 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1242 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0080 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1241 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0080 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1241 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0080 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1241 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0080 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1241 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0080 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1240 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0080 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1240 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0079 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1240 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0079 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1240 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0079 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1239 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0079 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1239 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0079 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1239 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0079 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1239 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0078 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1239 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0078 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1239 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0078 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1238 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0078 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1238 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0078 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1238 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0078 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1238 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0078 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1237 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0078 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1237 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0077 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1237 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0077 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1237 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0077 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1236 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0077 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1236 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0077 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1236 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0077 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1236 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0077 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1236 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0077 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1236 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0076 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1235 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0076 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1235 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0076 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1235 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0076 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1235 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0076 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1234 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0076 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1234 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0076 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1234 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0076 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1234 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0075 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1233 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0075 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1233 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0075 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1233 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0075 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1233 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0075 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1233 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0075 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1233 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0075 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1232 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0075 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1232 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0074 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1232 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0074 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1232 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0074 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1231 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0074 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1231 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0074 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1231 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0074 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1231 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0074 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1231 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0074 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1231 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0073 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1230 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1230 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0073 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1230 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1230 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0073 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1229 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1229 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0073 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1229 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1229 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0072 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1229 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0072 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1229 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0072 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1228 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0072 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1228 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0072 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1228 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0072 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1228 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0072 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1228 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0072 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1228 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0071 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1227 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0071 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1227 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0071 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1227 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0071 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1227 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0071 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1226 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0071 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1226 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0071 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1226 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0071 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1226 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0070 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1226 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0070 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1226 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0070 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1225 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0070 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1225 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0070 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1225 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0070 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1225 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0070 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1225 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0070 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1225 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0070 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1224 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0070 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1224 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0069 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1224 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0069 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1224 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0069 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1224 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0069 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1224 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0069 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1223 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:5m:48s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [1.240058183670044, 21.7967529296875, 26.86615562438965, 16.39504623413086, 12.820306777954102, 14.052240371704102, 9.502406120300293, 17.845190048217773, 10.585073471069336, 11.946610450744629, 8.190206527709961, 12.060384750366211, 11.381070137023926, 12.144434928894043, 11.773661613464355, 11.157583236694336, 10.387247085571289, 8.139424324035645, 6.926342010498047, 10.520552635192871, 11.66424560546875, 4.846014976501465, 7.40049409866333, 10.238457679748535, 10.40114974975586, 8.289382934570312, 7.705056667327881, 5.507745742797852, 5.5117998123168945, 4.832943916320801, 6.273785591125488, 6.10775899887085, 3.3790297508239746, 5.7527852058410645, 6.038909435272217, 4.313506126403809, 3.70150089263916, 2.3686931133270264, 5.325137138366699, 4.138288497924805, 4.232817649841309, 4.154174327850342, 5.321627140045166, 4.020432472229004, 2.955415725708008, 4.013071060180664, 3.411348342895508, 2.4636852741241455, 1.8715415000915527, 1.62066650390625, 1.8219906091690063, 2.542482376098633, 1.955312967300415, 0.7070920467376709, 2.1951069831848145, 1.8698562383651733, 2.052096128463745, 1.7469172477722168, 1.5252134799957275, 2.3246283531188965, 2.2006542682647705, 1.466734528541565, 2.1166038513183594, 1.6914267539978027, 1.2429119348526, 1.7464354038238525, 1.4966483116149902, 1.1006892919540405, 0.7268027067184448, 1.210842490196228, 1.0231854915618896, 0.3765488266944885, 0.8175650238990784, 0.8869050145149231, 0.9425632357597351, 0.43581634759902954, 0.6219356060028076, 0.467683881521225, 0.8484959602355957, 0.6708095669746399, 0.6303185224533081, 0.44677892327308655, 0.36657875776290894, 0.3357250690460205, 0.46188801527023315, 0.6562833189964294, 0.3804381489753723, 0.5476763248443604, 0.5003162622451782, 0.3633890151977539, 0.3363790810108185, 0.31129923462867737, 0.33650660514831543, 0.24691911041736603, 0.30539411306381226, 0.20903903245925903, 0.27282631397247314, 0.15490157902240753, 0.3630865216255188, 0.29527994990348816, 0.21788844466209412, 0.35227710008621216, 0.34565022587776184, 0.26357120275497437, 0.14062830805778503, 0.2728390097618103, 0.34175416827201843, 0.19477787613868713, 0.19567576050758362, 0.13159218430519104, 0.17356270551681519, 0.12708574533462524, 0.17370006442070007, 0.18026015162467957, 0.059265997260808945, 0.14607921242713928, 0.034301403909921646, 0.10357712209224701, 0.049655668437480927, 0.0817098319530487, 0.03613269329071045, 0.08146008849143982, 0.043499790132045746, 0.0618949756026268, 0.02639523148536682, 0.06400434672832489, 0.03128666430711746, 0.05574538931250572, 0.023190336301922798, 0.05979137867689133, 0.02585800178349018, 0.052876126021146774, 0.01906568743288517, 0.049367137253284454, 0.019393928349018097, 0.04065260291099548, 0.018811950460076332, 0.0305293258279562, 0.022702712565660477, 0.022084828466176987, 0.025134675204753876, 0.0163643229752779, 0.02454088255763054, 0.016557032242417336, 0.019331758841872215, 0.0189613476395607, 0.014856154099106789, 0.019740931689739227, 0.015323692932724953, 0.0161946602165699, 0.01677282527089119, 0.013738127425312996, 0.015963977202773094, 0.014621056616306305, 0.013729947619140148, 0.014921562746167183, 0.013334473595023155, 0.013808665797114372, 0.014040058478713036, 0.012833626940846443, 0.013476166874170303, 0.013012949377298355, 0.012432492338120937, 0.013083506375551224, 0.0124608613550663, 0.01220262236893177, 0.012579482048749924, 0.012056665495038033, 0.012038525193929672, 0.012209765613079071, 0.011747634038329124, 0.011776205152273178, 0.011807078495621681, 0.011501852422952652, 0.011570462957024574, 0.011512590572237968, 0.011279397644102573, 0.011342424899339676, 0.011264175176620483, 0.011105639860033989, 0.01114658359438181, 0.011039474979043007, 0.010908122174441814, 0.010934186168015003, 0.010846441611647606, 0.010746028274297714, 0.010746865533292294, 0.010664497502148151, 0.010586556047201157, 0.01057598926126957, 0.010502459481358528, 0.010433959774672985, 0.010407272726297379, 0.010338029824197292, 0.010282473638653755, 0.010255085304379463, 0.010191782377660275, 0.010138902813196182, 0.01010687742382288, 0.010049255564808846, 0.010000376962125301, 0.009966064244508743, 0.009911946952342987, 0.009863749146461487, 0.009828463196754456, 0.00978005863726139, 0.009734591469168663, 0.009697962552309036, 0.009651953354477882, 0.009607858955860138, 0.009570606052875519, 0.009527537971735, 0.009485185146331787, 0.009447550401091576, 0.009406115859746933, 0.009365467354655266, 0.009328633546829224, 0.009288756176829338, 0.009248970076441765, 0.00921260379254818, 0.009174076840281487, 0.009135644882917404, 0.009099527262151241, 0.009062578901648521, 0.009024912491440773, 0.00898933969438076, 0.008953355252742767, 0.008917154744267464, 0.008882120251655579, 0.008847086690366268, 0.0088117104023695, 0.008777402341365814, 0.008743218146264553, 0.008708738721907139, 0.008675082586705685, 0.008641848340630531, 0.008608080446720123, 0.008575137704610825, 0.008542603813111782, 0.008509939536452293, 0.008477428928017616, 0.00844559632241726, 0.008413631469011307, 0.008382003754377365, 0.008350571617484093, 0.008319399319589138, 0.008288534358143806, 0.008257809095084667, 0.00822732038795948, 0.008196831680834293, 0.008166786283254623, 0.008137174881994724, 0.008107373490929604, 0.008077909238636494, 0.008048826828598976, 0.008019608445465565, 0.007990690879523754, 0.00796221662312746, 0.007933653891086578, 0.007905433885753155, 0.007877257652580738, 0.007849476300179958, 0.007821742445230484, 0.007794242352247238, 0.007766825146973133, 0.007739683147519827, 0.007712744642049074, 0.007686039432883263, 0.0076593137346208096, 0.007632777094841003, 0.007606525905430317, 0.007580368779599667, 0.007554496172815561, 0.0075286682695150375, 0.007502920925617218, 0.007477576844394207, 0.007452201098203659, 0.007427050266414881, 0.007401986047625542, 0.007377238012850285, 0.007352450862526894, 0.007327978033572435, 0.007303535472601652, 0.007279279641807079, 0.007255218457430601, 0.007231360767036676, 0.007207391783595085, 0.007183760870248079, 0.007160336710512638, 0.007136979140341282, 0.007113737054169178, 0.007090571336448193, 0.007067548576742411, 0.007044746540486813, 0.0070219505578279495, 0.006999534089118242, 0.006976989563554525, 0.006954806856811047, 0.006932551972568035, 0.006910461001098156]\n",
      "@_@\t\tmacro_f1_score: [0.3582491874694824, 0.07407407462596893, 0.19528619945049286, 0.12727274000644684, 0.07407407462596893, 0.20875422656536102, 0.2370370477437973, 0.1494949460029602, 0.16296297311782837, 0.07407407462596893, 0.192592591047287, 0.2644219994544983, 0.2260381579399109, 0.10505051165819168, 0.0, 0.10505051165819168, 0.1346801370382309, 0.22962962090969086, 0.25993266701698303, 0.1791245937347412, 0.28209877014160156, 0.11851852387189865, 0.2074074149131775, 0.2666666805744171, 0.12962964177131653, 0.27912458777427673, 0.3138047456741333, 0.1346801370382309, 0.08888889104127884, 0.16296297311782837, 0.25319865345954895, 0.2545454800128937, 0.3185185194015503, 0.19999998807907104, 0.21375662088394165, 0.33265992999076843, 0.192592591047287, 0.18518517911434174, 0.29393938183784485, 0.23838384449481964, 0.1791245937347412, 0.2518518567085266, 0.20875422656536102, 0.2154882252216339, 0.29135802388191223, 0.26801347732543945, 0.1902356892824173, 0.39393937587738037, 0.3845118284225464, 0.3097642958164215, 0.4582010805606842, 0.5248677730560303, 0.4453262686729431, 0.4289562404155731, 0.49517399072647095, 0.37912458181381226, 0.25925928354263306, 0.35555559396743774, 0.6000000238418579, 0.5191919803619385, 0.3440355956554413, 0.26599326729774475, 0.27407407760620117, 0.45079368352890015, 0.4938271939754486, 0.39809203147888184, 0.4532628059387207, 0.4833333194255829, 0.5359788537025452, 0.47340065240859985, 0.5196969509124756, 0.674691379070282, 0.5493826866149902, 0.4878306984901428, 0.4592592418193817, 0.5938271880149841, 0.621164083480835, 0.5771043300628662, 0.5878307223320007, 0.4413580298423767, 0.5341269969940186, 0.6285713911056519, 0.6925926208496094, 0.7444444298744202, 0.6137565970420837, 0.5659933090209961, 0.777104377746582, 0.5715488195419312, 0.5648148059844971, 0.6643738746643066, 0.7711079120635986, 0.7919191718101501, 0.6222221851348877, 0.6851851940155029, 0.6532627940177917, 0.7672839760780334, 0.6582010388374329, 0.7555555701255798, 0.6427128911018372, 0.6606702208518982, 0.7174603343009949, 0.604938268661499, 0.640740692615509, 0.6814814805984497, 0.7796295881271362, 0.670899510383606, 0.5703703761100769, 0.783453643321991, 0.6932098865509033, 0.7841269969940186, 0.7582010626792908, 0.8444444537162781, 0.8014109134674072, 0.6666666865348816, 0.8518518805503845, 0.8003527522087097, 0.8888888955116272, 0.7026455402374268, 0.8730158805847168, 0.8384479880332947, 0.8888888955116272, 0.8333333134651184, 0.8666666746139526, 0.8507936000823975, 0.8888888955116272, 0.7555555701255798, 0.8730158805847168, 0.8384479880332947, 0.8888888955116272, 0.8507936000823975, 0.8888888955116272, 0.8384479880332947, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8606702089309692, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tsoft_f1_loss: [0.6327014565467834, 0.9259259104728699, 0.8047134280204773, 0.8757806420326233, 0.9159449338912964, 0.7911720871925354, 0.7471768856048584, 0.8505048155784607, 0.8370363116264343, 0.9259259104728699, 0.804576575756073, 0.7354417443275452, 0.7729617357254028, 0.8949260711669922, 0.9879322052001953, 0.894827127456665, 0.8650988340377808, 0.785810112953186, 0.7403154373168945, 0.8200278878211975, 0.7258585691452026, 0.8395621180534363, 0.7813830375671387, 0.7348071932792664, 0.8565220832824707, 0.7476968169212341, 0.6843443512916565, 0.864094078540802, 0.9118853807449341, 0.8406667709350586, 0.7222700119018555, 0.7187330722808838, 0.6680936217308044, 0.7968798279762268, 0.7723186612129211, 0.6263046860694885, 0.8049471378326416, 0.7979365587234497, 0.6986818313598633, 0.7524335384368896, 0.7956678867340088, 0.7525132298469543, 0.7856846451759338, 0.6984462738037109, 0.698905348777771, 0.7078828811645508, 0.8314768671989441, 0.6244005560874939, 0.575924277305603, 0.6931540966033936, 0.5415675640106201, 0.5255863666534424, 0.5587508082389832, 0.5738313794136047, 0.5097924470901489, 0.6177662014961243, 0.6773948669433594, 0.6469765901565552, 0.4151468575000763, 0.49067389965057373, 0.6403745412826538, 0.7172027230262756, 0.7240450382232666, 0.5513947606086731, 0.5226407647132874, 0.6240940690040588, 0.577511727809906, 0.5464943647384644, 0.4494945704936981, 0.5253667235374451, 0.4835215210914612, 0.3314431607723236, 0.469531387090683, 0.5106761455535889, 0.5246855616569519, 0.40353235602378845, 0.39777660369873047, 0.4193204641342163, 0.43466705083847046, 0.5480154156684875, 0.443050354719162, 0.3475089371204376, 0.3297744393348694, 0.319933146238327, 0.3602429926395416, 0.4366763234138489, 0.301921010017395, 0.42827004194259644, 0.4414340555667877, 0.36229100823402405, 0.256574809551239, 0.2799464166164398, 0.3838140070438385, 0.3222929835319519, 0.30863142013549805, 0.2434305101633072, 0.35187119245529175, 0.2839743196964264, 0.3000439405441284, 0.37164103984832764, 0.3148200809955597, 0.3396085500717163, 0.36517518758773804, 0.32282403111457825, 0.23369503021240234, 0.3264899253845215, 0.41970083117485046, 0.24825851619243622, 0.29875490069389343, 0.2608136236667633, 0.2955988645553589, 0.19479531049728394, 0.2223389446735382, 0.3091803193092346, 0.20594297349452972, 0.2169191539287567, 0.16323481500148773, 0.25599634647369385, 0.1651674211025238, 0.1804216206073761, 0.16243335604667664, 0.20576517283916473, 0.16558633744716644, 0.170375794172287, 0.15094566345214844, 0.20609542727470398, 0.14881709218025208, 0.16137933731079102, 0.14531633257865906, 0.1851462721824646, 0.14820405840873718, 0.16368815302848816, 0.14085224270820618, 0.179119274020195, 0.14001420140266418, 0.15462586283683777, 0.13833267986774445, 0.15333297848701477, 0.14428645372390747, 0.14290139079093933, 0.14463117718696594, 0.1367194503545761, 0.14821414649486542, 0.13709110021591187, 0.1372578740119934, 0.13726918399333954, 0.1342250257730484, 0.14025962352752686, 0.13468873500823975, 0.13540206849575043, 0.13562613725662231, 0.13261573016643524, 0.13626503944396973, 0.13426831364631653, 0.13211551308631897, 0.13341327011585236, 0.13205769658088684, 0.13275864720344543, 0.1330312043428421, 0.13131380081176758, 0.13170501589775085, 0.1311943531036377, 0.13076451420783997, 0.1316937357187271, 0.13085097074508667, 0.13030733168125153, 0.13064493238925934, 0.13014763593673706, 0.13021861016750336, 0.13040092587471008, 0.12976907193660736, 0.12968288362026215, 0.1296481490135193, 0.12939365208148956, 0.12961280345916748, 0.12950466573238373, 0.12909550964832306, 0.1291072815656662, 0.1290128380060196, 0.12884825468063354, 0.12891997396945953, 0.12876397371292114, 0.12853962182998657, 0.1285172551870346, 0.12840595841407776, 0.12832337617874146, 0.12835104763507843, 0.1282173991203308, 0.12806165218353271, 0.12801510095596313, 0.12792479991912842, 0.12785464525222778, 0.12782806158065796, 0.127719908952713, 0.1276070922613144, 0.1275431364774704, 0.1274641901254654, 0.1274077296257019, 0.12736593186855316, 0.12727504968643188, 0.12718574702739716, 0.12712089717388153, 0.12704792618751526, 0.12699060142040253, 0.12694287300109863, 0.12686556577682495, 0.12678319215774536, 0.12671691179275513, 0.1266525387763977, 0.12659712135791779, 0.12654413282871246, 0.12647481262683868, 0.12640231847763062, 0.12633872032165527, 0.12627744674682617, 0.12622258067131042, 0.1261703372001648, 0.12610721588134766, 0.12604019045829773, 0.12597960233688354, 0.1259215623140335, 0.12586697936058044, 0.1258144974708557, 0.12575668096542358, 0.12569448351860046, 0.12563544511795044, 0.12557947635650635, 0.1255263239145279, 0.12547460198402405, 0.1254204362630844, 0.12536297738552094, 0.1253069043159485, 0.1252530962228775, 0.1252012848854065, 0.12515081465244293, 0.12509959936141968, 0.12504561245441437, 0.12499208003282547, 0.12494055181741714, 0.12489053606987, 0.12484124302864075, 0.12479199469089508, 0.1247410923242569, 0.12469014525413513, 0.12464001774787903, 0.12459145486354828, 0.1245439350605011, 0.12449640780687332, 0.12444816529750824, 0.12439955770969391, 0.12435141950845718, 0.12430482357740402, 0.12425854802131653, 0.12421289831399918, 0.1241670474410057, 0.12412063032388687, 0.12407442927360535, 0.12402931600809097, 0.12398449331521988, 0.12394038587808609, 0.12389613687992096, 0.12385188043117523, 0.12380761653184891, 0.12376381456851959, 0.12372056394815445, 0.12367796897888184, 0.12363553047180176, 0.12359318137168884, 0.12355062365531921, 0.12350822985172272, 0.12346664816141129, 0.12342540174722672, 0.12338455766439438, 0.12334354966878891, 0.12330260872840881, 0.12326215207576752, 0.123221755027771, 0.12318183481693268, 0.12314213812351227, 0.1231028363108635, 0.12306339293718338, 0.12302439659833908, 0.12298539280891418, 0.12294682115316391, 0.1229085922241211, 0.12287069857120514, 0.1228325366973877, 0.12279486656188965, 0.12275730073451996, 0.12272009253501892, 0.12268301844596863, 0.12264619767665863, 0.12260957807302475, 0.12257315218448639, 0.12253668904304504, 0.12250084429979324, 0.12246484309434891, 0.12242952734231949, 0.122393898665905, 0.12235873192548752]\n",
      "@_@\t\tAUC: [0.4965277910232544, 0.4444444477558136, 0.4444444477558136, 0.5, 0.47037041187286377, 0.4444444477558136, 0.5537037253379822, 0.4444444477558136, 0.4444444477558136, 0.4444444477558136, 0.4930555522441864, 0.4555555582046509, 0.49259260296821594, 0.4444444477558136, 0.4814814329147339, 0.4814814329147339, 0.4444444477558136, 0.5518518686294556, 0.5092592835426331, 0.4365079402923584, 0.5333333611488342, 0.5148147940635681, 0.5518518686294556, 0.5555555820465088, 0.5, 0.5703703761100769, 0.5092592835426331, 0.4462962746620178, 0.5185185670852661, 0.6018518209457397, 0.5925925374031067, 0.5298611521720886, 0.6064814329147339, 0.5555555820465088, 0.6203703284263611, 0.6666666865348816, 0.6111111044883728, 0.6759259104728699, 0.5555555820465088, 0.6666666865348816, 0.5185185074806213, 0.6018518805503845, 0.5064814686775208, 0.5712962746620178, 0.6695767045021057, 0.5648148059844971, 0.5555555820465088, 0.6481481194496155, 0.6990740299224854, 0.6712963581085205, 0.7685185670852661, 0.7129629254341125, 0.7189153432846069, 0.7546296715736389, 0.7083333134651184, 0.6499999761581421, 0.6759259700775146, 0.7314814925193787, 0.75, 0.6953703761100769, 0.6216931343078613, 0.6398147940635681, 0.6527777910232544, 0.7407407164573669, 0.7546296715736389, 0.6944444179534912, 0.6851851940155029, 0.7916666865348816, 0.7962962985038757, 0.7240740656852722, 0.760185182094574, 0.8611111044883728, 0.8287037014961243, 0.7870370149612427, 0.7731481194496155, 0.8611111044883728, 0.8611111044883728, 0.8148148059844971, 0.7925925850868225, 0.8148148059844971, 0.8388888835906982, 0.8425925374031067, 0.8425925374031067, 0.8703703284263611, 0.8611111044883728, 0.8009259104728699, 0.8425925374031067, 0.8425925374031067, 0.875, 0.8518518209457397, 0.875, 0.875, 0.8518518805503845, 0.8796296119689941, 0.8796296715736389, 0.8796296715736389, 0.8564814329147339, 0.8796296119689941, 0.8796296715736389, 0.8796296119689941, 0.8842592239379883, 0.8796296119689941, 0.8888888955116272, 0.8796296715736389, 0.8888888955116272, 0.8333333134651184, 0.8333333134651184, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tglobal_accuracy: [0.4305555522441864, 0.7083333134651184, 0.6527777910232544, 0.6666666865348816, 0.7083333134651184, 0.6805555820465088, 0.5972222089767456, 0.5694444179534912, 0.5972222089767456, 0.7083333134651184, 0.6527777910232544, 0.5138888955116272, 0.5833333134651184, 0.625, 0.7083333134651184, 0.625, 0.6805555820465088, 0.7083333134651184, 0.6388888955116272, 0.625, 0.5972222089767456, 0.6527777910232544, 0.6805555820465088, 0.6527777910232544, 0.7222222089767456, 0.7222222089767456, 0.5972222089767456, 0.6805555820465088, 0.7361111044883728, 0.75, 0.6111111044883728, 0.4861111044883728, 0.7222222089767456, 0.6944444179534912, 0.6944444179534912, 0.7777777910232544, 0.6805555820465088, 0.7222222089767456, 0.6111111044883728, 0.5972222089767456, 0.625, 0.7638888955116272, 0.6805555820465088, 0.6944444179534912, 0.6944444179534912, 0.6666666865348816, 0.7083333134651184, 0.7222222089767456, 0.6805555820465088, 0.7361111044883728, 0.8611111044883728, 0.8055555820465088, 0.7777777910232544, 0.8333333134651184, 0.6527777910232544, 0.7222222089767456, 0.7777777910232544, 0.7916666865348816, 0.8194444179534912, 0.6666666865348816, 0.6944444179534912, 0.7638888955116272, 0.6666666865348816, 0.7361111044883728, 0.8194444179534912, 0.8055555820465088, 0.8472222089767456, 0.8472222089767456, 0.8472222089767456, 0.7777777910232544, 0.7361111044883728, 0.8611111044883728, 0.7777777910232544, 0.8194444179534912, 0.8055555820465088, 0.8611111044883728, 0.8194444179534912, 0.875, 0.8055555820465088, 0.8333333134651184, 0.8333333134651184, 0.8888888955116272, 0.8888888955116272, 0.9027777910232544, 0.8333333134651184, 0.8472222089767456, 0.9027777910232544, 0.8333333134651184, 0.8472222089767456, 0.875, 0.9027777910232544, 0.9166666865348816, 0.9027777910232544, 0.8888888955116272, 0.9027777910232544, 0.9166666865348816, 0.9027777910232544, 0.9583333134651184, 0.8333333134651184, 0.875, 0.9305555820465088, 0.8472222089767456, 0.9027777910232544, 0.9166666865348816, 0.9166666865348816, 0.8888888955116272, 0.9027777910232544, 0.9027777910232544, 0.9166666865348816, 0.9305555820465088, 0.9305555820465088, 0.9583333134651184, 0.9305555820465088, 0.9027777910232544, 0.9861111044883728, 0.9305555820465088, 1.0, 0.9305555820465088, 0.9861111044883728, 0.9583333134651184, 1.0, 0.9722222089767456, 0.9861111044883728, 0.9722222089767456, 1.0, 0.9583333134651184, 0.9861111044883728, 0.9583333134651184, 1.0, 0.9722222089767456, 1.0, 0.9583333134651184, 1.0, 1.0, 1.0, 0.9722222089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tglobal_precision: [0.3214285671710968, 0.5, 0.4166666567325592, 0.4000000059604645, 0.5, 0.4583333432674408, 0.375, 0.2916666567325592, 0.3333333432674408, 0.5, 0.4166666567325592, 0.32499998807907104, 0.35483869910240173, 0.3125, 0.0, 0.3125, 0.4375, 0.5, 0.4193548262119293, 0.375, 0.36666667461395264, 0.375, 0.4444444477558136, 0.4285714328289032, 0.5555555820465088, 0.52173912525177, 0.4000000059604645, 0.4375, 0.6666666865348816, 1.0, 0.36000001430511475, 0.30000001192092896, 0.5199999809265137, 0.4736842215061188, 0.4736842215061188, 0.6315789222717285, 0.4444444477558136, 0.5555555820465088, 0.3870967626571655, 0.3461538553237915, 0.375, 0.625, 0.4583333432674408, 0.47826087474823, 0.4736842215061188, 0.42105263471603394, 0.5, 0.5199999809265137, 0.46875, 0.5416666865348816, 0.9230769276618958, 0.6842105388641357, 0.6190476417541504, 0.800000011920929, 0.4545454680919647, 0.52173912525177, 1.0, 0.875, 0.6666666865348816, 0.4615384638309479, 0.48148149251937866, 0.699999988079071, 0.4285714328289032, 0.5416666865348816, 0.699999988079071, 0.7058823704719543, 0.7777777910232544, 0.8125, 0.7777777910232544, 0.6000000238418579, 0.529411792755127, 0.761904776096344, 0.6315789222717285, 0.8999999761581421, 0.6842105388641357, 0.739130437374115, 0.6538461446762085, 0.8333333134651184, 0.6296296119689941, 0.7647058963775635, 0.7647058963775635, 0.8095238208770752, 0.8421052694320679, 0.8181818127632141, 0.6666666865348816, 0.7777777910232544, 0.7692307829856873, 0.7368420958518982, 0.8571428656578064, 0.8333333134651184, 0.7692307829856873, 0.800000011920929, 0.8888888955116272, 0.9333333373069763, 0.8181818127632141, 0.8260869383811951, 1.0, 1.0, 0.6451612710952759, 0.75, 0.9444444179534912, 0.7272727489471436, 0.9375, 0.8947368264198303, 0.7777777910232544, 0.782608687877655, 0.9375, 0.7692307829856873, 0.800000011920929, 0.9444444179534912, 1.0, 0.875, 0.807692289352417, 1.0, 1.0, 0.807692289352417, 1.0, 1.0, 0.9545454382896423, 0.875, 1.0, 1.0, 0.9545454382896423, 0.9130434989929199, 1.0, 1.0, 0.9545454382896423, 0.875, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.9130434989929199, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tglobal_recall: [0.8571428656578064, 0.190476194024086, 0.4761904776096344, 0.2857142984867096, 0.190476194024086, 0.523809552192688, 0.5714285969734192, 0.3333333432674408, 0.380952388048172, 0.190476194024086, 0.4761904776096344, 0.6190476417541504, 0.523809552192688, 0.2380952388048172, 0.0, 0.2380952388048172, 0.3333333432674408, 0.4761904776096344, 0.6190476417541504, 0.4285714328289032, 0.523809552192688, 0.2857142984867096, 0.380952388048172, 0.5714285969734192, 0.2380952388048172, 0.5714285969734192, 0.761904776096344, 0.3333333432674408, 0.190476194024086, 0.1428571492433548, 0.4285714328289032, 0.5714285969734192, 0.6190476417541504, 0.4285714328289032, 0.4285714328289032, 0.5714285969734192, 0.380952388048172, 0.2380952388048172, 0.5714285969734192, 0.4285714328289032, 0.4285714328289032, 0.4761904776096344, 0.523809552192688, 0.523809552192688, 0.4285714328289032, 0.380952388048172, 0.2380952388048172, 0.6190476417541504, 0.7142857313156128, 0.6190476417541504, 0.5714285969734192, 0.6190476417541504, 0.6190476417541504, 0.5714285969734192, 0.9523809552192688, 0.5714285969734192, 0.2380952388048172, 0.3333333432674408, 0.761904776096344, 0.8571428656578064, 0.6190476417541504, 0.3333333432674408, 0.4285714328289032, 0.6190476417541504, 0.6666666865348816, 0.5714285969734192, 0.6666666865348816, 0.6190476417541504, 0.6666666865348816, 0.7142857313156128, 0.8571428656578064, 0.761904776096344, 0.5714285969734192, 0.4285714328289032, 0.6190476417541504, 0.8095238208770752, 0.8095238208770752, 0.7142857313156128, 0.8095238208770752, 0.6190476417541504, 0.6190476417541504, 0.8095238208770752, 0.761904776096344, 0.8571428656578064, 0.8571428656578064, 0.6666666865348816, 0.9523809552192688, 0.6666666865348816, 0.5714285969734192, 0.7142857313156128, 0.9523809552192688, 0.9523809552192688, 0.761904776096344, 0.6666666865348816, 0.8571428656578064, 0.9047619104385376, 0.6666666865348816, 0.8571428656578064, 0.9523809552192688, 0.8571428656578064, 0.8095238208770752, 0.761904776096344, 0.7142857313156128, 0.8095238208770752, 1.0, 0.8571428656578064, 0.7142857313156128, 0.9523809552192688, 0.9523809552192688, 0.8095238208770752, 0.761904776096344, 1.0, 1.0, 0.6666666865348816, 0.9523809552192688, 1.0, 1.0, 0.761904776096344, 1.0, 1.0, 1.0, 0.9047619104385376, 1.0, 1.0, 1.0, 0.8571428656578064, 1.0, 1.0, 1.0, 0.9047619104385376, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_loss: [21.796751022338867, 26.86615753173828, 16.39504623413086, 12.820306777954102, 14.052240371704102, 9.50240707397461, 17.84518814086914, 10.58507251739502, 11.946611404418945, 8.190207481384277, 12.060383796691895, 11.381071090698242, 12.144434928894043, 11.773662567138672, 11.157583236694336, 10.387247085571289, 8.139424324035645, 6.926342487335205, 10.520551681518555, 11.66424560546875, 4.846014976501465, 7.400494575500488, 10.238457679748535, 10.40114974975586, 8.289382934570312, 7.705056667327881, 5.507745742797852, 5.5117998123168945, 4.832943916320801, 6.273785591125488, 6.107758522033691, 3.3790297508239746, 5.7527852058410645, 6.038909435272217, 4.313506126403809, 3.70150089263916, 2.3686928749084473, 5.325137138366699, 4.138288497924805, 4.232817649841309, 4.154174327850342, 5.321627140045166, 4.020432472229004, 2.955415725708008, 4.013070583343506, 3.411348342895508, 2.4636852741241455, 1.8715416193008423, 1.62066650390625, 1.821990728378296, 2.542482614517212, 1.955312967300415, 0.7070919871330261, 2.1951069831848145, 1.8698562383651733, 2.052096366882324, 1.7469172477722168, 1.5252134799957275, 2.3246283531188965, 2.2006540298461914, 1.4667346477508545, 2.1166038513183594, 1.6914268732070923, 1.2429118156433105, 1.7464354038238525, 1.4966481924057007, 1.1006892919540405, 0.7268027067184448, 1.2108423709869385, 1.0231853723526, 0.3765488266944885, 0.8175650238990784, 0.8869050741195679, 0.9425632953643799, 0.43581637740135193, 0.6219356060028076, 0.46768391132354736, 0.8484959602355957, 0.6708095669746399, 0.6303185224533081, 0.44677889347076416, 0.36657875776290894, 0.3357250690460205, 0.46188801527023315, 0.6562833189964294, 0.3804381489753723, 0.5476763248443604, 0.500316321849823, 0.3633890450000763, 0.3363790512084961, 0.31129923462867737, 0.33650657534599304, 0.24691911041736603, 0.30539408326148987, 0.20903903245925903, 0.27282631397247314, 0.15490159392356873, 0.3630865216255188, 0.29527997970581055, 0.21788844466209412, 0.35227710008621216, 0.34565022587776184, 0.26357120275497437, 0.14062829315662384, 0.2728390097618103, 0.3417541980743408, 0.19477789103984833, 0.19567576050758362, 0.13159218430519104, 0.17356270551681519, 0.12708574533462524, 0.17370006442070007, 0.18026015162467957, 0.059265993535518646, 0.14607921242713928, 0.034301407635211945, 0.1035771295428276, 0.049655668437480927, 0.0817098319530487, 0.03613269329071045, 0.08146008849143982, 0.043499790132045746, 0.0618949756026268, 0.02639523148536682, 0.06400434672832489, 0.03128666430711746, 0.05574538931250572, 0.023190336301922798, 0.059791382402181625, 0.02585800178349018, 0.052876126021146774, 0.01906568929553032, 0.04936714097857475, 0.019393928349018097, 0.04065260291099548, 0.018811950460076332, 0.03052932769060135, 0.022702712565660477, 0.022084828466176987, 0.025134673342108727, 0.0163643229752779, 0.02454088255763054, 0.016557032242417336, 0.019331758841872215, 0.0189613476395607, 0.014856154099106789, 0.019740931689739227, 0.015323692932724953, 0.0161946602165699, 0.01677282527089119, 0.013738127425312996, 0.015963977202773094, 0.014621056616306305, 0.013729948550462723, 0.014921561814844608, 0.01333447266370058, 0.013808664865791798, 0.014040058478713036, 0.012833626940846443, 0.013476166874170303, 0.01301295030862093, 0.012432492338120937, 0.01308350544422865, 0.0124608613550663, 0.01220262236893177, 0.012579482048749924, 0.012056666426360607, 0.012038524262607098, 0.012209765613079071, 0.011747634038329124, 0.011776207014918327, 0.011807078495621681, 0.011501852422952652, 0.011570462957024574, 0.011512590572237968, 0.011279397644102573, 0.011342424899339676, 0.011264175176620483, 0.011105639860033989, 0.01114658359438181, 0.011039474979043007, 0.010908123105764389, 0.010934186168015003, 0.010846441611647606, 0.010746028274297714, 0.010746865533292294, 0.010664498433470726, 0.010586556047201157, 0.01057598926126957, 0.010502459481358528, 0.01043396070599556, 0.010407272726297379, 0.010338028892874718, 0.010282473638653755, 0.010255085304379463, 0.010191782377660275, 0.010138902813196182, 0.01010687742382288, 0.010049255564808846, 0.010000376030802727, 0.009966064244508743, 0.009911946021020412, 0.009863749146461487, 0.009828463196754456, 0.00978005863726139, 0.009734591469168663, 0.009697962552309036, 0.009651953354477882, 0.009607858955860138, 0.009570606052875519, 0.009527537971735, 0.009485185146331787, 0.009447550401091576, 0.009406115859746933, 0.009365467354655266, 0.009328634478151798, 0.009288757108151913, 0.009248970076441765, 0.00921260379254818, 0.009174076840281487, 0.009135644882917404, 0.009099526330828667, 0.009062578901648521, 0.009024912491440773, 0.00898933969438076, 0.008953354321420193, 0.008917155675590038, 0.008882119320333004, 0.008847086690366268, 0.008811711333692074, 0.00877740141004324, 0.008743218146264553, 0.008708737790584564, 0.00867508165538311, 0.008641848340630531, 0.008608080446720123, 0.008575137704610825, 0.008542603813111782, 0.008509940467774868, 0.008477429859340191, 0.008445595391094685, 0.008413631469011307, 0.008382003754377365, 0.008350571617484093, 0.008319399319589138, 0.008288534358143806, 0.008257808163762093, 0.00822732038795948, 0.008196832612156868, 0.008166786283254623, 0.008137175813317299, 0.008107373490929604, 0.008077909238636494, 0.00804882775992155, 0.008019608445465565, 0.007990691810846329, 0.00796221662312746, 0.007933653891086578, 0.007905433885753155, 0.007877257652580738, 0.007849475368857384, 0.007821742445230484, 0.007794242352247238, 0.00776682561263442, 0.007739683613181114, 0.007712744642049074, 0.007686039432883263, 0.007659313268959522, 0.007632777094841003, 0.007606525905430317, 0.007580368779599667, 0.007554495707154274, 0.007528668735176325, 0.007502920459955931, 0.007477575913071632, 0.007452200632542372, 0.0074270498007535934, 0.007401986047625542, 0.007377238012850285, 0.007352450862526894, 0.007327978033572435, 0.007303535472601652, 0.007279280107468367, 0.007255218457430601, 0.007231361232697964, 0.007207391783595085, 0.007183760870248079, 0.007160336710512638, 0.007136979140341282, 0.007113737054169178, 0.007090571336448193, 0.007067548576742411, 0.007044745609164238, 0.0070219505578279495, 0.006999533623456955, 0.006976990029215813, 0.006954806856811047, 0.006932551972568035, 0.006910460535436869, 0.00688851997256279]\n",
      "@_@\t\tval_macro_f1_score: [0.07407407462596893, 0.19528619945049286, 0.12727274000644684, 0.07407407462596893, 0.20875422656536102, 0.2370370477437973, 0.1494949460029602, 0.16296297311782837, 0.07407407462596893, 0.192592591047287, 0.2644219994544983, 0.2260381579399109, 0.10505051165819168, 0.0, 0.10505051165819168, 0.1346801370382309, 0.22962962090969086, 0.25993266701698303, 0.1791245937347412, 0.28209877014160156, 0.11851852387189865, 0.2074074149131775, 0.2666666805744171, 0.12962964177131653, 0.27912458777427673, 0.3138047456741333, 0.1346801370382309, 0.08888889104127884, 0.16296297311782837, 0.25319865345954895, 0.2545454800128937, 0.3185185194015503, 0.19999998807907104, 0.21375662088394165, 0.33265992999076843, 0.192592591047287, 0.18518517911434174, 0.29393938183784485, 0.23838384449481964, 0.1791245937347412, 0.2518518567085266, 0.20875422656536102, 0.2154882252216339, 0.29135802388191223, 0.26801347732543945, 0.1902356892824173, 0.39393937587738037, 0.3845118284225464, 0.3097642958164215, 0.4582010805606842, 0.5248677730560303, 0.4453262686729431, 0.4289562404155731, 0.49517399072647095, 0.37912458181381226, 0.25925928354263306, 0.35555559396743774, 0.6000000238418579, 0.5191919803619385, 0.3440355956554413, 0.26599326729774475, 0.27407407760620117, 0.45079368352890015, 0.4938271939754486, 0.39809203147888184, 0.4532628059387207, 0.4833333194255829, 0.5359788537025452, 0.47340065240859985, 0.5196969509124756, 0.674691379070282, 0.5493826866149902, 0.4878306984901428, 0.4592592418193817, 0.5938271880149841, 0.621164083480835, 0.5771043300628662, 0.5878307223320007, 0.4413580298423767, 0.5341269969940186, 0.6285713911056519, 0.6925926208496094, 0.7444444298744202, 0.6137565970420837, 0.5659933090209961, 0.777104377746582, 0.5715488195419312, 0.5648148059844971, 0.6643738746643066, 0.7711079120635986, 0.7919191718101501, 0.6222221851348877, 0.6851851940155029, 0.6532627940177917, 0.7672839760780334, 0.6582010388374329, 0.7555555701255798, 0.6427128911018372, 0.6606702208518982, 0.7174603343009949, 0.604938268661499, 0.640740692615509, 0.6814814805984497, 0.7796295881271362, 0.670899510383606, 0.5703703761100769, 0.783453643321991, 0.6932098865509033, 0.7841269969940186, 0.7582010626792908, 0.8444444537162781, 0.8014109134674072, 0.6666666865348816, 0.8518518805503845, 0.8003527522087097, 0.8888888955116272, 0.7026455402374268, 0.8730158805847168, 0.8384479880332947, 0.8888888955116272, 0.8333333134651184, 0.8666666746139526, 0.8507936000823975, 0.8888888955116272, 0.7555555701255798, 0.8730158805847168, 0.8384479880332947, 0.8888888955116272, 0.8507936000823975, 0.8888888955116272, 0.8384479880332947, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8606702089309692, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tval_soft_f1_loss: [0.9259259104728699, 0.8047134280204773, 0.8757806420326233, 0.9159449338912964, 0.7911720871925354, 0.7471768856048584, 0.8505048155784607, 0.8370363116264343, 0.9259259104728699, 0.804576575756073, 0.7354417443275452, 0.7729617357254028, 0.8949260711669922, 0.9879322052001953, 0.894827127456665, 0.8650988340377808, 0.785810112953186, 0.7403154373168945, 0.8200278878211975, 0.7258585691452026, 0.8395621180534363, 0.7813830375671387, 0.7348071932792664, 0.8565220832824707, 0.7476968169212341, 0.6843443512916565, 0.864094078540802, 0.9118853807449341, 0.8406667709350586, 0.7222700119018555, 0.7187330722808838, 0.6680936217308044, 0.7968798279762268, 0.7723186612129211, 0.6263047456741333, 0.8049471378326416, 0.7979365587234497, 0.6986817717552185, 0.7524335384368896, 0.7956678867340088, 0.7525132298469543, 0.7856846451759338, 0.6984462738037109, 0.698905348777771, 0.7078828811645508, 0.8314768671989441, 0.6244005560874939, 0.575924277305603, 0.6931540966033936, 0.5415675640106201, 0.5255863666534424, 0.5587508082389832, 0.5738313794136047, 0.5097924470901489, 0.6177662014961243, 0.6773948669433594, 0.6469765901565552, 0.4151468575000763, 0.49067389965057373, 0.6403745412826538, 0.7172027230262756, 0.7240450382232666, 0.5513947606086731, 0.5226407647132874, 0.6240940690040588, 0.577511727809906, 0.5464943647384644, 0.4494945704936981, 0.5253667235374451, 0.4835214614868164, 0.3314431607723236, 0.469531387090683, 0.5106762051582336, 0.5246855616569519, 0.40353235602378845, 0.39777660369873047, 0.4193204641342163, 0.43466711044311523, 0.5480154156684875, 0.443050354719162, 0.3475089371204376, 0.3297744393348694, 0.3199331760406494, 0.3602429926395416, 0.4366763234138489, 0.301921010017395, 0.42827004194259644, 0.4414340555667877, 0.36229100823402405, 0.256574809551239, 0.2799464166164398, 0.3838140070438385, 0.3222929835319519, 0.30863142013549805, 0.2434305101633072, 0.35187119245529175, 0.2839743196964264, 0.3000439405441284, 0.37164103984832764, 0.3148200809955597, 0.3396085500717163, 0.36517518758773804, 0.32282403111457825, 0.23369503021240234, 0.3264899253845215, 0.41970086097717285, 0.24825851619243622, 0.29875490069389343, 0.2608136236667633, 0.2955988645553589, 0.19479531049728394, 0.2223389744758606, 0.3091803193092346, 0.20594297349452972, 0.2169191539287567, 0.16323481500148773, 0.25599634647369385, 0.1651674211025238, 0.1804216206073761, 0.16243335604667664, 0.20576517283916473, 0.16558633744716644, 0.170375794172287, 0.15094566345214844, 0.20609542727470398, 0.14881709218025208, 0.16137933731079102, 0.14531633257865906, 0.1851462572813034, 0.14820405840873718, 0.16368815302848816, 0.14085224270820618, 0.179119274020195, 0.14001420140266418, 0.15462586283683777, 0.13833267986774445, 0.15333297848701477, 0.14428645372390747, 0.14290139079093933, 0.14463117718696594, 0.1367194503545761, 0.14821414649486542, 0.13709110021591187, 0.1372578740119934, 0.13726918399333954, 0.1342250257730484, 0.14025962352752686, 0.13468872010707855, 0.13540206849575043, 0.13562613725662231, 0.13261573016643524, 0.13626503944396973, 0.13426831364631653, 0.13211551308631897, 0.13341327011585236, 0.13205769658088684, 0.13275864720344543, 0.1330312341451645, 0.13131380081176758, 0.13170501589775085, 0.13119438290596008, 0.13076452910900116, 0.1316937506198883, 0.13085097074508667, 0.13030733168125153, 0.13064493238925934, 0.13014763593673706, 0.13021861016750336, 0.13040092587471008, 0.12976907193660736, 0.12968288362026215, 0.1296481341123581, 0.12939365208148956, 0.12961280345916748, 0.12950466573238373, 0.12909550964832306, 0.1291072815656662, 0.1290128380060196, 0.12884823977947235, 0.12891997396945953, 0.12876397371292114, 0.12853962182998657, 0.1285172551870346, 0.12840597331523895, 0.12832337617874146, 0.12835104763507843, 0.1282173991203308, 0.12806165218353271, 0.12801510095596313, 0.1279248148202896, 0.12785464525222778, 0.12782806158065796, 0.127719908952713, 0.1276070922613144, 0.1275431364774704, 0.1274641901254654, 0.1274077296257019, 0.12736593186855316, 0.12727504968643188, 0.12718574702739716, 0.12712089717388153, 0.12704794108867645, 0.12699060142040253, 0.12694287300109863, 0.12686556577682495, 0.12678319215774536, 0.12671691179275513, 0.1266525387763977, 0.12659712135791779, 0.12654413282871246, 0.12647481262683868, 0.12640231847763062, 0.12633872032165527, 0.12627744674682617, 0.12622258067131042, 0.1261703372001648, 0.12610721588134766, 0.12604019045829773, 0.12597960233688354, 0.1259215623140335, 0.12586697936058044, 0.1258144974708557, 0.12575668096542358, 0.12569448351860046, 0.12563544511795044, 0.12557949125766754, 0.1255263239145279, 0.12547461688518524, 0.1254204362630844, 0.12536297738552094, 0.1253069043159485, 0.1252530962228775, 0.1252012848854065, 0.12515081465244293, 0.12509959936141968, 0.12504561245441437, 0.12499208003282547, 0.12494055181741714, 0.12489053606987, 0.12484124302864075, 0.12479199469089508, 0.1247410923242569, 0.12469014525413513, 0.12464001774787903, 0.12459145486354828, 0.1245439350605011, 0.12449640780687332, 0.12444816529750824, 0.12439955770969391, 0.12435141950845718, 0.12430482357740402, 0.12425854802131653, 0.12421289831399918, 0.1241670474410057, 0.12412063032388687, 0.12407442927360535, 0.12402931600809097, 0.12398449331521988, 0.12394038587808609, 0.12389613687992096, 0.12385188043117523, 0.12380761653184891, 0.12376381456851959, 0.12372056394815445, 0.12367796897888184, 0.12363553047180176, 0.12359318137168884, 0.12355062365531921, 0.12350822985172272, 0.12346664816141129, 0.12342540174722672, 0.12338455766439438, 0.12334354966878891, 0.12330260872840881, 0.12326215207576752, 0.123221755027771, 0.12318183481693268, 0.12314213812351227, 0.1231028363108635, 0.12306339293718338, 0.12302439659833908, 0.12298539280891418, 0.12294682115316391, 0.1229085922241211, 0.12287069857120514, 0.1228325366973877, 0.12279486656188965, 0.12275730073451996, 0.12272009253501892, 0.12268301844596863, 0.12264619767665863, 0.12260957807302475, 0.12257315218448639, 0.12253668904304504, 0.12250084429979324, 0.12246484309434891, 0.12242952734231949, 0.122393898665905, 0.12235873192548752, 0.12232359498739243]\n",
      "@_@\t\tval_AUC: [0.4444444477558136, 0.4444444477558136, 0.5, 0.47037041187286377, 0.4444444477558136, 0.5537037253379822, 0.4444444477558136, 0.4444444477558136, 0.4444444477558136, 0.4930555522441864, 0.4555555582046509, 0.49259260296821594, 0.4444444477558136, 0.4814814329147339, 0.4814814329147339, 0.4444444477558136, 0.5518518686294556, 0.5092592835426331, 0.4365079402923584, 0.5333333611488342, 0.5148147940635681, 0.5518518686294556, 0.5555555820465088, 0.5, 0.5703703761100769, 0.5092592835426331, 0.4462962746620178, 0.5185185670852661, 0.6018518209457397, 0.5925925374031067, 0.5298611521720886, 0.6064814329147339, 0.5555555820465088, 0.6203703284263611, 0.6666666865348816, 0.6111111044883728, 0.6759259104728699, 0.5555555820465088, 0.6666666865348816, 0.5185185074806213, 0.6018518805503845, 0.5064814686775208, 0.5712962746620178, 0.6695767045021057, 0.5648148059844971, 0.5555555820465088, 0.6481481194496155, 0.6990740299224854, 0.6712963581085205, 0.7685185670852661, 0.7129629254341125, 0.7189153432846069, 0.7546296715736389, 0.7083333134651184, 0.6499999761581421, 0.6759259700775146, 0.7314814925193787, 0.75, 0.6953703761100769, 0.6216931343078613, 0.6398147940635681, 0.6527777910232544, 0.7407407164573669, 0.7546296715736389, 0.6944444179534912, 0.6851851940155029, 0.7916666865348816, 0.7962962985038757, 0.7240740656852722, 0.760185182094574, 0.8611111044883728, 0.8287037014961243, 0.7870370149612427, 0.7731481194496155, 0.8611111044883728, 0.8611111044883728, 0.8148148059844971, 0.7925925850868225, 0.8148148059844971, 0.8388888835906982, 0.8425925374031067, 0.8425925374031067, 0.8703703284263611, 0.8611111044883728, 0.8009259104728699, 0.8425925374031067, 0.8425925374031067, 0.875, 0.8518518209457397, 0.875, 0.875, 0.8518518805503845, 0.8796296119689941, 0.8796296715736389, 0.8796296715736389, 0.8564814329147339, 0.8796296119689941, 0.8796296715736389, 0.8796296119689941, 0.8842592239379883, 0.8796296119689941, 0.8888888955116272, 0.8796296715736389, 0.8888888955116272, 0.8333333134651184, 0.8333333134651184, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tval_global_accuracy: [0.7083333134651184, 0.6527777910232544, 0.6666666865348816, 0.7083333134651184, 0.6805555820465088, 0.5972222089767456, 0.5694444179534912, 0.5972222089767456, 0.7083333134651184, 0.6527777910232544, 0.5138888955116272, 0.5833333134651184, 0.625, 0.7083333134651184, 0.625, 0.6805555820465088, 0.7083333134651184, 0.6388888955116272, 0.625, 0.5972222089767456, 0.6527777910232544, 0.6805555820465088, 0.6527777910232544, 0.7222222089767456, 0.7222222089767456, 0.5972222089767456, 0.6805555820465088, 0.7361111044883728, 0.75, 0.6111111044883728, 0.4861111044883728, 0.7222222089767456, 0.6944444179534912, 0.6944444179534912, 0.7777777910232544, 0.6805555820465088, 0.7222222089767456, 0.6111111044883728, 0.5972222089767456, 0.625, 0.7638888955116272, 0.6805555820465088, 0.6944444179534912, 0.6944444179534912, 0.6666666865348816, 0.7083333134651184, 0.7222222089767456, 0.6805555820465088, 0.7361111044883728, 0.8611111044883728, 0.8055555820465088, 0.7777777910232544, 0.8333333134651184, 0.6527777910232544, 0.7222222089767456, 0.7777777910232544, 0.7916666865348816, 0.8194444179534912, 0.6666666865348816, 0.6944444179534912, 0.7638888955116272, 0.6666666865348816, 0.7361111044883728, 0.8194444179534912, 0.8055555820465088, 0.8472222089767456, 0.8472222089767456, 0.8472222089767456, 0.7777777910232544, 0.7361111044883728, 0.8611111044883728, 0.7777777910232544, 0.8194444179534912, 0.8055555820465088, 0.8611111044883728, 0.8194444179534912, 0.875, 0.8055555820465088, 0.8333333134651184, 0.8333333134651184, 0.8888888955116272, 0.8888888955116272, 0.9027777910232544, 0.8333333134651184, 0.8472222089767456, 0.9027777910232544, 0.8333333134651184, 0.8472222089767456, 0.875, 0.9027777910232544, 0.9166666865348816, 0.9027777910232544, 0.8888888955116272, 0.9027777910232544, 0.9166666865348816, 0.9027777910232544, 0.9583333134651184, 0.8333333134651184, 0.875, 0.9305555820465088, 0.8472222089767456, 0.9027777910232544, 0.9166666865348816, 0.9166666865348816, 0.8888888955116272, 0.9027777910232544, 0.9027777910232544, 0.9166666865348816, 0.9305555820465088, 0.9305555820465088, 0.9583333134651184, 0.9305555820465088, 0.9027777910232544, 0.9861111044883728, 0.9305555820465088, 1.0, 0.9305555820465088, 0.9861111044883728, 0.9583333134651184, 1.0, 0.9722222089767456, 0.9861111044883728, 0.9722222089767456, 1.0, 0.9583333134651184, 0.9861111044883728, 0.9583333134651184, 1.0, 0.9722222089767456, 1.0, 0.9583333134651184, 1.0, 1.0, 1.0, 0.9722222089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_global_precision: [0.5, 0.4166666567325592, 0.4000000059604645, 0.5, 0.4583333432674408, 0.375, 0.2916666567325592, 0.3333333432674408, 0.5, 0.4166666567325592, 0.32499998807907104, 0.35483869910240173, 0.3125, 0.0, 0.3125, 0.4375, 0.5, 0.4193548262119293, 0.375, 0.36666667461395264, 0.375, 0.4444444477558136, 0.4285714328289032, 0.5555555820465088, 0.52173912525177, 0.4000000059604645, 0.4375, 0.6666666865348816, 1.0, 0.36000001430511475, 0.30000001192092896, 0.5199999809265137, 0.4736842215061188, 0.4736842215061188, 0.6315789222717285, 0.4444444477558136, 0.5555555820465088, 0.3870967626571655, 0.3461538553237915, 0.375, 0.625, 0.4583333432674408, 0.47826087474823, 0.4736842215061188, 0.42105263471603394, 0.5, 0.5199999809265137, 0.46875, 0.5416666865348816, 0.9230769276618958, 0.6842105388641357, 0.6190476417541504, 0.800000011920929, 0.4545454680919647, 0.52173912525177, 1.0, 0.875, 0.6666666865348816, 0.4615384638309479, 0.48148149251937866, 0.699999988079071, 0.4285714328289032, 0.5416666865348816, 0.699999988079071, 0.7058823704719543, 0.7777777910232544, 0.8125, 0.7777777910232544, 0.6000000238418579, 0.529411792755127, 0.761904776096344, 0.6315789222717285, 0.8999999761581421, 0.6842105388641357, 0.739130437374115, 0.6538461446762085, 0.8333333134651184, 0.6296296119689941, 0.7647058963775635, 0.7647058963775635, 0.8095238208770752, 0.8421052694320679, 0.8181818127632141, 0.6666666865348816, 0.7777777910232544, 0.7692307829856873, 0.7368420958518982, 0.8571428656578064, 0.8333333134651184, 0.7692307829856873, 0.800000011920929, 0.8888888955116272, 0.9333333373069763, 0.8181818127632141, 0.8260869383811951, 1.0, 1.0, 0.6451612710952759, 0.75, 0.9444444179534912, 0.7272727489471436, 0.9375, 0.8947368264198303, 0.7777777910232544, 0.782608687877655, 0.9375, 0.7692307829856873, 0.800000011920929, 0.9444444179534912, 1.0, 0.875, 0.807692289352417, 1.0, 1.0, 0.807692289352417, 1.0, 1.0, 0.9545454382896423, 0.875, 1.0, 1.0, 0.9545454382896423, 0.9130434989929199, 1.0, 1.0, 0.9545454382896423, 0.875, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.9130434989929199, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_global_recall: [0.190476194024086, 0.4761904776096344, 0.2857142984867096, 0.190476194024086, 0.523809552192688, 0.5714285969734192, 0.3333333432674408, 0.380952388048172, 0.190476194024086, 0.4761904776096344, 0.6190476417541504, 0.523809552192688, 0.2380952388048172, 0.0, 0.2380952388048172, 0.3333333432674408, 0.4761904776096344, 0.6190476417541504, 0.4285714328289032, 0.523809552192688, 0.2857142984867096, 0.380952388048172, 0.5714285969734192, 0.2380952388048172, 0.5714285969734192, 0.761904776096344, 0.3333333432674408, 0.190476194024086, 0.1428571492433548, 0.4285714328289032, 0.5714285969734192, 0.6190476417541504, 0.4285714328289032, 0.4285714328289032, 0.5714285969734192, 0.380952388048172, 0.2380952388048172, 0.5714285969734192, 0.4285714328289032, 0.4285714328289032, 0.4761904776096344, 0.523809552192688, 0.523809552192688, 0.4285714328289032, 0.380952388048172, 0.2380952388048172, 0.6190476417541504, 0.7142857313156128, 0.6190476417541504, 0.5714285969734192, 0.6190476417541504, 0.6190476417541504, 0.5714285969734192, 0.9523809552192688, 0.5714285969734192, 0.2380952388048172, 0.3333333432674408, 0.761904776096344, 0.8571428656578064, 0.6190476417541504, 0.3333333432674408, 0.4285714328289032, 0.6190476417541504, 0.6666666865348816, 0.5714285969734192, 0.6666666865348816, 0.6190476417541504, 0.6666666865348816, 0.7142857313156128, 0.8571428656578064, 0.761904776096344, 0.5714285969734192, 0.4285714328289032, 0.6190476417541504, 0.8095238208770752, 0.8095238208770752, 0.7142857313156128, 0.8095238208770752, 0.6190476417541504, 0.6190476417541504, 0.8095238208770752, 0.761904776096344, 0.8571428656578064, 0.8571428656578064, 0.6666666865348816, 0.9523809552192688, 0.6666666865348816, 0.5714285969734192, 0.7142857313156128, 0.9523809552192688, 0.9523809552192688, 0.761904776096344, 0.6666666865348816, 0.8571428656578064, 0.9047619104385376, 0.6666666865348816, 0.8571428656578064, 0.9523809552192688, 0.8571428656578064, 0.8095238208770752, 0.761904776096344, 0.7142857313156128, 0.8095238208770752, 1.0, 0.8571428656578064, 0.7142857313156128, 0.9523809552192688, 0.9523809552192688, 0.8095238208770752, 0.761904776096344, 1.0, 1.0, 0.6666666865348816, 0.9523809552192688, 1.0, 1.0, 0.761904776096344, 1.0, 1.0, 1.0, 0.9047619104385376, 1.0, 1.0, 1.0, 0.8571428656578064, 1.0, 1.0, 1.0, 0.9047619104385376, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1300/0_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.0069\n",
      "@_@\tValidation Macro F1-score: 0.8889\n",
      "W0402 13:07:02.759559 130360925050688 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1300/0_unfrozen_block/model/saved_model/assets\n",
      "I0402 13:07:22.361438 130360925050688 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1300/0_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1300/0_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Evaluate ------------------\n",
      "1/1 [==============================] - 1s 652ms/step\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1300/predict_sample.png\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "@_@\tResult of testing on 1 batches:\n",
      "@_@\t\tbinary_crossentropy: 0.0068884119391441345\n",
      "@_@\t\tAUC: 0.8888888955116272\n",
      "@_@\t\tmacro_f1_score: 0.8888888955116272\n",
      "@_@\t\tsoft_f1_loss: 0.12232359498739243\n",
      "@_@\t\tglobal_accuracy: 1.0\n",
      "@_@\t\tglobal_precision: 1.0\n",
      "@_@\t\tglobal_recall: 1.0\n",
      "@_@\t\n",
      "@_@\tDONE!\n"
     ]
    }
   ],
   "source": [
    "!python ../neural_nets/resnet50_test.py \\\n",
    "  --ouput_name=resnet50-transfer-miniBatchTest \\\n",
    "  --image_size=448 --epochs=300 --batch_size=8 --train_size=8 \\\n",
    "  --mode=train_then_eval --min_unfreeze_blocks=0 --max_unfreeze_blocks=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cwd to '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation'\n",
      "['/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode/neural_nets', '/home/payam/miniconda3/envs/tf2-gpu/lib/python39.zip', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/lib-dynload', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages', '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode']\n",
      "/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation\n",
      "2024-04-02 18:50:38.250096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 18:50:38.802731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-02 18:50:38.802842: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-02 18:50:38.802850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-02 18:50:39.817082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:39.853009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:39.853230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:39.853600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 18:50:39.854630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:39.855002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:39.855155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:40.313888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:40.314090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:40.314223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 18:50:40.314324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "@_@\t\n",
      "@_@\t------------------ Configuration ------------------\n",
      "@_@\tStart: 2024-04-02_1850\n",
      "@_@\t\n",
      "@_@\tMode: train_then_eval\n",
      "@_@\tUnfreeze blocks: start 0, end 0\n",
      "@_@\tContinue from checkpoint: False\n",
      "@_@\t\n",
      "@_@\tDataset: MIMIC-CXR\n",
      "@_@\tTraining Dataset Size: 8\n",
      "@_@\tBatch size: 8\n",
      "@_@\tImage size: (224, 224)\n",
      "@_@\tMax Epochs per training round: 300\n",
      "@_@\tDefault Learning Rate: 0.1\n",
      "@_@\t\n",
      "@_@\tGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "@_@\tCPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "@_@\t\n",
      "@_@\t------------------ Data ------------------\n",
      "@_@\tnum_classes: 9\n",
      "@_@\tclass_names: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Pleural Effusion'\n",
      "@_@\t 'Pneumonia' 'Pneumothorax' 'Fracture' 'Support Devices']\n",
      "@_@\tall_data_size: 377110\n",
      "@_@\tall_data_filtered_size: 112134\n",
      "@_@\tsplit_size: {'train': 8, 'validate': 889, 'test': 512}\n",
      "@_@\tmax value before preprocess tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "@_@\tmin value before preprocess tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "@_@\tShape of image batch: [2402, 2697, 3]\n",
      "@_@\tShape of labels batch: [9]\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0402 18:51:27.013268 125766439323456 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "@_@\tmax value after preprocess tf.Tensor(255, shape=(), dtype=uint8)\n",
      "@_@\tmin value after preprocess tf.Tensor(0, shape=(), dtype=uint8)\n",
      "@_@\tShape of image batch: [8, 224, 224, 3]\n",
      "@_@\tShape of labels batch: [8, 9]\n",
      "Model: \"my_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " my_avg_pool (GlobalAveragePool  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " my_fc_1 (Dense)                (None, 256)          524544      ['my_avg_pool[0][0]']            \n",
      "                                                                                                  \n",
      " my_fc_2 (Dense)                (None, 128)          32896       ['my_fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " my_output (Dense)              (None, 9)            1161        ['my_fc_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,146,313\n",
      "Trainable params: 558,601\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "@_@\tUnfreezing 0 blocks...\n",
      "@_@\tTotal trainable weights: 6\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 0.001\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0402 18:51:29.731150 125766439323456 deprecation.py:554] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2024-04-02 18:51:32.631342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2024-04-02 18:51:33.140374: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7260744ab700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-02 18:51:33.140428: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-04-02 18:51:33.145289: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-02 18:51:33.226642: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6739 - macro_f1_score: 0.2643 - soft_f1_loss: 0.6657 - AUC: 0.3915 - global_accuracy: 0.6250 - global_precision: 0.4062 - global_recall: 0.6190 - val_loss: 0.6157 - val_macro_f1_score: 0.1333 - val_soft_f1_loss: 0.7452 - val_AUC: 0.7571 - val_global_accuracy: 0.7361 - val_global_precision: 0.7500 - val_global_recall: 0.1429\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 1s 959ms/step - loss: 0.6157 - macro_f1_score: 0.1333 - soft_f1_loss: 0.7452 - AUC: 0.7571 - global_accuracy: 0.7361 - global_precision: 0.7500 - global_recall: 0.1429 - val_loss: 0.4444 - val_macro_f1_score: 0.2370 - val_soft_f1_loss: 0.6420 - val_AUC: 0.8630 - val_global_accuracy: 0.7639 - val_global_precision: 1.0000 - val_global_recall: 0.1905\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 1s 915ms/step - loss: 0.4444 - macro_f1_score: 0.2370 - soft_f1_loss: 0.6420 - AUC: 0.8630 - global_accuracy: 0.7639 - global_precision: 1.0000 - global_recall: 0.1905 - val_loss: 0.3324 - val_macro_f1_score: 0.4765 - val_soft_f1_loss: 0.5425 - val_AUC: 0.8741 - val_global_accuracy: 0.8611 - val_global_precision: 0.9231 - val_global_recall: 0.5714\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 1s 993ms/step - loss: 0.3324 - macro_f1_score: 0.4765 - soft_f1_loss: 0.5425 - AUC: 0.8741 - global_accuracy: 0.8611 - global_precision: 0.9231 - global_recall: 0.5714 - val_loss: 0.2910 - val_macro_f1_score: 0.6389 - val_soft_f1_loss: 0.4729 - val_AUC: 0.8815 - val_global_accuracy: 0.9306 - val_global_precision: 0.9000 - val_global_recall: 0.8571\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2910 - macro_f1_score: 0.6389 - soft_f1_loss: 0.4729 - AUC: 0.8815 - global_accuracy: 0.9306 - global_precision: 0.9000 - global_recall: 0.8571 - val_loss: 0.2327 - val_macro_f1_score: 0.6508 - val_soft_f1_loss: 0.4186 - val_AUC: 0.8889 - val_global_accuracy: 0.9444 - val_global_precision: 0.9474 - val_global_recall: 0.8571\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 1s 917ms/step - loss: 0.2327 - macro_f1_score: 0.6508 - soft_f1_loss: 0.4186 - AUC: 0.8889 - global_accuracy: 0.9444 - global_precision: 0.9474 - global_recall: 0.8571 - val_loss: 0.1881 - val_macro_f1_score: 0.8519 - val_soft_f1_loss: 0.3765 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 1.0000 - val_global_recall: 0.9524\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 1s 938ms/step - loss: 0.1881 - macro_f1_score: 0.8519 - soft_f1_loss: 0.3765 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 1.0000 - global_recall: 0.9524 - val_loss: 0.1504 - val_macro_f1_score: 0.8519 - val_soft_f1_loss: 0.3365 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 1.0000 - val_global_recall: 0.9524\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 0.1504 - macro_f1_score: 0.8519 - soft_f1_loss: 0.3365 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 1.0000 - global_recall: 0.9524 - val_loss: 0.1220 - val_macro_f1_score: 0.8519 - val_soft_f1_loss: 0.3003 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 1.0000 - val_global_recall: 0.9524\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1220 - macro_f1_score: 0.8519 - soft_f1_loss: 0.3003 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 1.0000 - global_recall: 0.9524 - val_loss: 0.1002 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.2674 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 1s 988ms/step - loss: 0.1002 - macro_f1_score: 0.8889 - soft_f1_loss: 0.2674 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0830 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.2400 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 1s 927ms/step - loss: 0.0830 - macro_f1_score: 0.8889 - soft_f1_loss: 0.2400 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0668 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.2150 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0668 - macro_f1_score: 0.8889 - soft_f1_loss: 0.2150 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0524 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1922 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 1s 931ms/step - loss: 0.0524 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1922 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0413 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1741 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0413 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1741 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0333 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1612 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0333 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1612 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0270 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1518 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 1s 951ms/step - loss: 0.0270 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1518 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0219 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1444 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0219 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1444 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0179 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1385 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 0.0179 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1385 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0147 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1336 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 0.0147 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1336 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0120 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1295 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 1s 918ms/step - loss: 0.0120 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1295 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0100 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1263 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 1s 960ms/step - loss: 0.0100 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1263 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0084 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1238 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0084 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1238 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0071 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1219 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 1s 963ms/step - loss: 0.0071 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1219 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0062 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1205 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0062 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1205 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0054 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1193 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0054 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1193 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0047 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1182 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0047 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1182 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0041 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1173 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 1s 962ms/step - loss: 0.0041 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1173 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0036 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1166 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 1s 980ms/step - loss: 0.0036 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1166 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0032 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1159 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 1s 936ms/step - loss: 0.0032 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1159 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0028 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1154 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0028 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1154 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0025 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1149 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0025 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1149 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0023 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1146 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 1s 987ms/step - loss: 0.0023 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1146 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0021 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1143 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 0.0021 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1143 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0019 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1140 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 1s 872ms/step - loss: 0.0019 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1140 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0018 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1138 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0018 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1138 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0016 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1136 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 1s 955ms/step - loss: 0.0016 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1136 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0015 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1134 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 1s 1000ms/step - loss: 0.0015 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1134 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0014 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1133 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 1s 872ms/step - loss: 0.0014 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1133 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0013 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1131 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0013 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1131 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0012 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1130 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0012 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1130 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0012 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1129 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0012 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1129 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0011 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1128 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0011 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1128 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0010 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1127 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 1s 994ms/step - loss: 0.0010 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1127 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.7378e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1126 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 9.7378e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1126 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.2163e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1125 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 1s 843ms/step - loss: 9.2163e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1125 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.7480e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1124 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.7480e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1124 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.3293e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1124 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.3293e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1124 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.9578e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1123 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.9578e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1123 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.6286e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1123 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.6286e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1123 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.3357e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1122 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.3357e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1122 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.0749e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1122 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 7.0749e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1122 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.8423e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1121 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 1s 960ms/step - loss: 6.8423e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1121 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.6332e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1121 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 1s 851ms/step - loss: 6.6332e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1121 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.4441e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1121 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.4441e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1121 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.2700e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1121 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 1s 996ms/step - loss: 6.2700e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1121 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.1098e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1120 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 1s 963ms/step - loss: 6.1098e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1120 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.9597e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1120 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 1s 981ms/step - loss: 5.9597e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1120 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.8182e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1120 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.8182e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1120 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.6848e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1120 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6848e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1120 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.5576e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1120 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 1s 944ms/step - loss: 5.5576e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1120 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.4361e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 1s 980ms/step - loss: 5.4361e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.3215e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3215e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.2136e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 1s 983ms/step - loss: 5.2136e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.1115e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 1s 1000ms/step - loss: 5.1115e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.0142e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 1s 938ms/step - loss: 5.0142e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.9213e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9213e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.8328e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 1s 982ms/step - loss: 4.8328e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.7489e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 1s 891ms/step - loss: 4.7489e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.6706e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6706e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.5954e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 1s 989ms/step - loss: 4.5954e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.5235e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5235e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.4539e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 1s 952ms/step - loss: 4.4539e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.3875e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 4.3875e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.3234e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3234e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2617e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2617e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2028e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 1s 865ms/step - loss: 4.2028e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.1468e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 1s 989ms/step - loss: 4.1468e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.0925e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 4.0925e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.0396e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 1s 953ms/step - loss: 4.0396e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9881e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9881e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9382e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 1s 994ms/step - loss: 3.9382e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8894e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8894e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8429e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8429e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7973e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 1s 908ms/step - loss: 3.7973e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7525e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 1s 994ms/step - loss: 3.7525e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7086e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 3.7086e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6662e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6662e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6243e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6243e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5831e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 1s 988ms/step - loss: 3.5831e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5429e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 1s 961ms/step - loss: 3.5429e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5038e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 1s 937ms/step - loss: 3.5038e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4659e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4659e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4286e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4286e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3913e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 1s 987ms/step - loss: 3.3913e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3545e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 1s 929ms/step - loss: 3.3545e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3184e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3184e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2827e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2827e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2474e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 1s 955ms/step - loss: 3.2474e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2127e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 1s 962ms/step - loss: 3.2127e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1789e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 1s 853ms/step - loss: 3.1789e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1458e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 1s 989ms/step - loss: 3.1458e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1132e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1132e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0810e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 1s 1000ms/step - loss: 3.0810e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0494e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0494e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0189e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0189e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9892e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9892e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9597e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 1s 906ms/step - loss: 2.9597e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9305e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 1s 984ms/step - loss: 2.9305e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9016e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 1s 973ms/step - loss: 2.9016e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8733e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8733e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8455e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8455e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8180e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8180e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7908e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7908e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7640e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7640e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7376e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7376e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7118e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 1s 893ms/step - loss: 2.7118e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6866e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6866e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6617e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 1s 963ms/step - loss: 2.6617e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6372e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6372e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6129e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 2.6129e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5891e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5891e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5656e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5656e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5423e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 1s 918ms/step - loss: 2.5423e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5192e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 1s 925ms/step - loss: 2.5192e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4964e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 2.4964e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4742e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 2.4742e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4521e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4521e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4304e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 1s 887ms/step - loss: 2.4304e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4089e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 1s 965ms/step - loss: 2.4089e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.3878e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 1s 922ms/step - loss: 2.3878e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.3668e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 1s 891ms/step - loss: 2.3668e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.3462e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 1s 973ms/step - loss: 2.3462e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.3260e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 1s 960ms/step - loss: 2.3260e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.3061e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3061e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.2863e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2863e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.2667e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2667e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.2475e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 2.2475e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.2284e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2284e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.2096e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 1s 915ms/step - loss: 2.2096e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.1910e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1910e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.1726e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1726e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.1545e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 1s 894ms/step - loss: 2.1545e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.1365e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 1s 993ms/step - loss: 2.1365e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.1190e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 1s 936ms/step - loss: 2.1190e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.1016e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 1s 890ms/step - loss: 2.1016e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0844e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 2.0844e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0672e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0672e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0503e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0503e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0334e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0334e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0168e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 1s 915ms/step - loss: 2.0168e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0004e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 1s 971ms/step - loss: 2.0004e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.9842e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 1s 955ms/step - loss: 1.9842e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.9681e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 1s 886ms/step - loss: 1.9681e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.9520e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9520e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.9364e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 1s 968ms/step - loss: 1.9364e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.9209e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 1.9208e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.9054e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 1s 889ms/step - loss: 1.9054e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8901e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 1.8901e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8751e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8751e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8603e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 1s 898ms/step - loss: 1.8603e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8456e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8456e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8311e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 1.8311e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8167e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 1s 964ms/step - loss: 1.8167e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8024e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 1s 939ms/step - loss: 1.8024e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7883e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7883e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7743e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 1s 995ms/step - loss: 1.7743e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7605e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 1s 937ms/step - loss: 1.7605e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7470e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 1s 892ms/step - loss: 1.7470e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7336e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7336e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7202e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7202e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7070e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 1s 995ms/step - loss: 1.7070e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6940e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6940e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6811e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 1s 999ms/step - loss: 1.6811e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6684e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 1s 900ms/step - loss: 1.6684e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6559e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6559e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6435e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6435e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6312e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6312e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6192e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6192e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6072e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 1.6072e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5952e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 1s 963ms/step - loss: 1.5952e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5835e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5835e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5718e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 1s 913ms/step - loss: 1.5718e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5603e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 1s 960ms/step - loss: 1.5603e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5489e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5489e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5376e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 1s 856ms/step - loss: 1.5376e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5265e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 1.5265e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5154e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5154e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5045e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5045e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4937e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4937e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4831e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 1s 871ms/step - loss: 1.4831e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4725e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 1.4725e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4620e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 1s 932ms/step - loss: 1.4620e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4516e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 1.4516e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4414e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4414e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4312e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 1s 963ms/step - loss: 1.4312e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4212e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 1.4212e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4113e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4113e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4014e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 1s 919ms/step - loss: 1.4014e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3917e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 1s 988ms/step - loss: 1.3917e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3820e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 1s 977ms/step - loss: 1.3820e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3724e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3724e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3630e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3630e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3536e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 1.3536e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3444e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3444e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3352e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 1s 949ms/step - loss: 1.3352e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3262e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 1.3262e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3172e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 1s 905ms/step - loss: 1.3172e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3083e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 1s 887ms/step - loss: 1.3083e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2994e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 1.2994e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2907e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 1s 907ms/step - loss: 1.2907e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2821e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 1s 907ms/step - loss: 1.2821e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2735e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 1s 968ms/step - loss: 1.2735e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2650e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 1s 931ms/step - loss: 1.2650e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2566e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 1.2566e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2483e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 1s 978ms/step - loss: 1.2483e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2401e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2401e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2319e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 1.2319e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2239e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2239e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2159e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2159e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2080e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2080e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2002e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2002e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1924e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1924e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1847e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1847e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1771e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 1s 922ms/step - loss: 1.1771e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1695e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 1s 890ms/step - loss: 1.1695e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1620e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1620e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1546e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 1s 990ms/step - loss: 1.1546e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1472e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 1s 986ms/step - loss: 1.1472e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1400e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1400e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1328e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 1s 981ms/step - loss: 1.1328e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1257e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1257e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1186e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 1.1186e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1117e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 1s 999ms/step - loss: 1.1117e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1048e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1048e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0979e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0979e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0911e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 1s 981ms/step - loss: 1.0911e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0845e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 1s 857ms/step - loss: 1.0845e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0779e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 1.0779e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0712e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0712e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0647e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 1.0647e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0583e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 1.0583e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0519e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 1s 929ms/step - loss: 1.0519e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0455e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0455e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0392e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 1s 913ms/step - loss: 1.0392e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0329e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0329e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0267e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 1s 935ms/step - loss: 1.0267e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0206e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 1s 990ms/step - loss: 1.0206e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0145e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0145e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0085e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0085e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0025e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0025e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.9649e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 9.9649e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.9043e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 9.9043e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.8436e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.8436e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.7833e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 1s 982ms/step - loss: 9.7833e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.7231e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.7231e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.6629e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 1s 848ms/step - loss: 9.6629e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.6029e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.6029e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.5431e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.5431e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.4841e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 1s 979ms/step - loss: 9.4841e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.4259e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 1s 974ms/step - loss: 9.4259e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.3679e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.3679e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.3104e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 1s 980ms/step - loss: 9.3104e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.2534e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.2534e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.1968e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 1s 986ms/step - loss: 9.1968e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.1405e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 9.1405e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.0851e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 1s 992ms/step - loss: 9.0851e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.0304e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 1s 982ms/step - loss: 9.0304e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.9760e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 1s 918ms/step - loss: 8.9760e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.9215e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 1s 982ms/step - loss: 8.9215e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.8677e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 8.8677e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.8145e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 8.8145e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.7614e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.7614e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.7092e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 1s 971ms/step - loss: 8.7092e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.6573e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 1s 959ms/step - loss: 8.6573e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.6056e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.6056e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.5549e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 1s 871ms/step - loss: 8.5549e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.5046e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 1s 965ms/step - loss: 8.5046e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.4549e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.4549e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.4054e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.4054e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.3563e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.3563e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.3075e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.3075e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.2588e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 1s 906ms/step - loss: 8.2588e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.2102e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 1s 996ms/step - loss: 8.2102e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.1619e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.1619e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.1142e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 1s 873ms/step - loss: 8.1142e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.0674e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 8.0674e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.0206e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.0206e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.9742e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.9742e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.9283e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 1s 957ms/step - loss: 7.9283e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.8832e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 1s 932ms/step - loss: 7.8832e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.8393e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.8393e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.7964e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.7964e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.7541e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 7.7541e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.7122e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.7122e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.6706e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 1s 973ms/step - loss: 7.6706e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.6292e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.6292e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.5882e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 1s 981ms/step - loss: 7.5882e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.5476e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.5476e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.5074e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 1s 904ms/step - loss: 7.5074e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.4675e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 7.4675e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.4279e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:5m:3s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [0.6739282608032227, 0.615707516670227, 0.4444287419319153, 0.332377165555954, 0.2909916043281555, 0.23274588584899902, 0.18808715045452118, 0.15039882063865662, 0.12195207178592682, 0.1001921072602272, 0.08296866714954376, 0.06683093309402466, 0.0523676872253418, 0.04133526235818863, 0.03327580541372299, 0.02698678709566593, 0.021942783147096634, 0.01791556179523468, 0.014650117605924606, 0.012028709053993225, 0.009963707998394966, 0.008367521688342094, 0.007145065348595381, 0.006185103207826614, 0.005395552609115839, 0.004722668789327145, 0.004132281988859177, 0.0036213521379977465, 0.0031867451034486294, 0.002820739056915045, 0.002518225461244583, 0.0022714294027537107, 0.002067635301500559, 0.0018963934853672981, 0.0017501316033303738, 0.0016234235372394323, 0.0015119760064408183, 0.0014119214611127973, 0.0013213548809289932, 0.0012384983710944653, 0.0011628670617938042, 0.0010938034392893314, 0.0010306616313755512, 0.0009737784857861698, 0.0009216293692588806, 0.000874796649441123, 0.0008329319534823298, 0.0007957752095535398, 0.0007628587773069739, 0.0007335697300732136, 0.0007074910681694746, 0.0006842324510216713, 0.0006633221055381, 0.0006444102618843317, 0.0006269981386139989, 0.0006109813111834228, 0.0005959706613793969, 0.0005818168283440173, 0.0005684846546500921, 0.0005557583644986153, 0.0005436133360490203, 0.0005321538774296641, 0.0005213631666265428, 0.0005111526697874069, 0.000501418428029865, 0.0004921348299831152, 0.00048327870899811387, 0.0004748936044052243, 0.00046706435387022793, 0.00045953813241794705, 0.0004523458192124963, 0.00044539012014865875, 0.0004387527296785265, 0.00043234415352344513, 0.00042616765131242573, 0.0004202802083455026, 0.00041468319250270724, 0.00040925387293100357, 0.00040395642281509936, 0.000398813106585294, 0.0003938179579563439, 0.0003889434738084674, 0.0003842891601379961, 0.0003797311510425061, 0.00037525349762290716, 0.00037085983785800636, 0.000366617226973176, 0.00036243355134502053, 0.00035831128479912877, 0.0003542945487424731, 0.0003503841580823064, 0.0003465886111371219, 0.00034285621950402856, 0.00033913057995960116, 0.0003354540094733238, 0.000331841001752764, 0.00032827118411660194, 0.00032473698956891894, 0.0003212721785530448, 0.00031789412605576217, 0.0003145794034935534, 0.0003113215498160571, 0.00030810339376330376, 0.0003049416409339756, 0.0003018947027157992, 0.00029891758458688855, 0.000295967620331794, 0.00029304553754627705, 0.0002901607658714056, 0.00028732852661050856, 0.00028455216670408845, 0.0002817994973156601, 0.00027908419724553823, 0.00027639896143227816, 0.00027376387151889503, 0.0002711776760406792, 0.0002686638617888093, 0.0002661706239450723, 0.0002637184807099402, 0.0002612894168123603, 0.0002589059295132756, 0.00025655573699623346, 0.0002542261208873242, 0.00025192220346070826, 0.00024964328622445464, 0.0002474167267791927, 0.0002452138578519225, 0.00024304495309479535, 0.0002408925793133676, 0.00023877547937445343, 0.00023668355424888432, 0.00023461529053747654, 0.00023260120360646397, 0.00023061121464706957, 0.00022863419144414365, 0.00022667404846288264, 0.00022474532306659967, 0.0002228367084171623, 0.00022095625172369182, 0.00021909750648774207, 0.00021726262639276683, 0.00021544838091358542, 0.0002136508992407471, 0.000211895297979936, 0.00021016053506173193, 0.00020843552192673087, 0.00020672057871706784, 0.00020502798724919558, 0.00020334463624749333, 0.00020168122136965394, 0.00020003726240247488, 0.0001984174596145749, 0.0001968072319868952, 0.00019520349451340735, 0.00019363612227607518, 0.00019208498997613788, 0.0001905410026665777, 0.0001890107523649931, 0.00018750787421595305, 0.0001860263291746378, 0.00018456047109793872, 0.00018310609448235482, 0.00018166624067816883, 0.0001802448241505772, 0.0001788275985745713, 0.00017743281205184758, 0.00017605366883799434, 0.00017469734302721918, 0.00017335830489173532, 0.00017202481103595346, 0.0001707003975752741, 0.00016939887427724898, 0.0001681101566646248, 0.0001668384938966483, 0.0001655876694712788, 0.00016434694407507777, 0.00016312109073624015, 0.00016191942268051207, 0.00016071998106781393, 0.00015952283865772188, 0.00015834734949748963, 0.00015717749192845076, 0.0001560255332151428, 0.00015488589997403324, 0.00015376118244603276, 0.0001526462729088962, 0.0001515449839644134, 0.00015045270265545696, 0.00014937279047444463, 0.00014830871077720076, 0.00014724931679666042, 0.00014619533612858504, 0.00014516252849716693, 0.00014413656026590616, 0.00014312297571450472, 0.0001421207416569814, 0.0001411326666129753, 0.00014014390762895346, 0.00013916655734647065, 0.00013820032472722232, 0.00013724356540478766, 0.00013630354078486562, 0.00013536468031816185, 0.00013443565694615245, 0.00013351711095310748, 0.00013261669664643705, 0.0001317170972470194, 0.00013082596706226468, 0.00012993979908060282, 0.00012906809570267797, 0.0001282083976548165, 0.00012735230848193169, 0.00012650164717342705, 0.00012566446093842387, 0.0001248345652129501, 0.00012400832201819867, 0.00012319326924625784, 0.0001223865256179124, 0.00012158750905655324, 0.00012079628504579887, 0.00012001550931017846, 0.00011924045975320041, 0.00011847188579849899, 0.00011770874698413536, 0.00011694786371663213, 0.00011620295845204964, 0.00011546023597475141, 0.00011472360347397625, 0.00011399516370147467, 0.00011328142136335373, 0.00011256727884756401, 0.00011186198389623314, 0.00011116509267594665, 0.00011047537554986775, 0.00010978782665915787, 0.0001091097219614312, 0.0001084519608411938, 0.00010778619616758078, 0.00010712297807913274, 0.00010647442104527727, 0.00010582984396023676, 0.00010518675844650716, 0.00010455207666382194, 0.00010392061085440218, 0.00010329406359232962, 0.00010267434117849916, 0.00010206137085333467, 0.00010145170381292701, 0.00010084953100886196, 0.00010025297524407506, 9.964851778931916e-05, 9.904283797368407e-05, 9.843573934631422e-05, 9.783252608031034e-05, 9.723104449221864e-05, 9.662936645327136e-05, 9.602926729712635e-05, 9.543094347463921e-05, 9.484078327659518e-05, 9.425874304724857e-05, 9.367940947413445e-05, 9.310402674600482e-05, 9.253418102161959e-05, 9.196800965582952e-05, 9.140541078522801e-05, 9.085105557460338e-05, 9.030408546095714e-05, 8.975985838333145e-05, 8.921510016079992e-05, 8.867747965268791e-05, 8.8144835899584e-05, 8.76136400620453e-05, 8.709180838195607e-05, 8.657287253299728e-05, 8.605590846855193e-05, 8.554902160540223e-05, 8.504636934958398e-05, 8.454869384877384e-05, 8.405426342505962e-05, 8.356261241715401e-05, 8.307452662847936e-05, 8.25883325887844e-05, 8.210174564737827e-05, 8.161916775861755e-05, 8.114238153211772e-05, 8.06740063126199e-05, 8.02059075795114e-05, 7.974231266416609e-05, 7.928317063488066e-05, 7.883212674641982e-05, 7.839288446120918e-05, 7.796406862325966e-05, 7.754052057862282e-05, 7.71218110457994e-05, 7.670558989048004e-05, 7.629238825757056e-05, 7.588243897771463e-05, 7.547553104814142e-05, 7.507366535719484e-05, 7.467549585271627e-05]\n",
      "@_@\t\tmacro_f1_score: [0.26430976390838623, 0.13333334028720856, 0.2370370477437973, 0.47654321789741516, 0.6388888955116272, 0.6507936716079712, 0.8518518805503845, 0.8518518805503845, 0.8518518805503845, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tsoft_f1_loss: [0.6656573414802551, 0.7451985478401184, 0.6419987082481384, 0.5425425171852112, 0.47292014956474304, 0.4186283349990845, 0.37649306654930115, 0.3364664912223816, 0.30027833580970764, 0.2674351930618286, 0.2400457262992859, 0.2150079905986786, 0.19215482473373413, 0.17407472431659698, 0.16124585270881653, 0.15181271731853485, 0.1444353312253952, 0.13849098980426788, 0.1335533857345581, 0.1294834017753601, 0.12628142535686493, 0.1238153800368309, 0.12194053083658218, 0.1204746812582016, 0.11926845461130142, 0.11824504286050797, 0.1173471063375473, 0.11657532304525375, 0.11592277139425278, 0.1153775081038475, 0.11492998152971268, 0.11456619203090668, 0.11426538974046707, 0.11401084065437317, 0.11379110813140869, 0.11359845101833344, 0.1134265810251236, 0.11327110230922699, 0.1131296306848526, 0.11300002038478851, 0.11288174241781235, 0.11277402937412262, 0.11267602443695068, 0.11258824914693832, 0.11250800639390945, 0.11243637651205063, 0.11237268894910812, 0.11231645196676254, 0.1122669130563736, 0.11222304403781891, 0.112184077501297, 0.11214937269687653, 0.11211816221475601, 0.11208987236022949, 0.11206384748220444, 0.11203993856906891, 0.11201757937669754, 0.11199644207954407, 0.11197659373283386, 0.11195772141218185, 0.11193977296352386, 0.1119229644536972, 0.1119072437286377, 0.11189235746860504, 0.11187819391489029, 0.11186468601226807, 0.11185179650783539, 0.11183958500623703, 0.11182819306850433, 0.11181722581386566, 0.11180680245161057, 0.11179675161838531, 0.11178711801767349, 0.11177781224250793, 0.11176881194114685, 0.11176029592752457, 0.11175206303596497, 0.11174403131008148, 0.11173619329929352, 0.11172858625650406, 0.11172114312648773, 0.11171391606330872, 0.1117069348692894, 0.111700139939785, 0.11169343441724777, 0.11168685555458069, 0.11168045550584793, 0.11167417466640472, 0.11166802793741226, 0.11166201531887054, 0.11165615916252136, 0.11165051907300949, 0.11164490133523941, 0.11163923144340515, 0.11163359135389328, 0.11162802577018738, 0.11162251979112625, 0.11161711812019348, 0.11161179095506668, 0.11160659790039062, 0.11160153895616531, 0.11159658432006836, 0.11159165948629379, 0.11158683896064758, 0.11158222705125809, 0.11157769709825516, 0.1115732192993164, 0.11156879365444183, 0.11156442761421204, 0.11156010627746582, 0.11155586689710617, 0.1115516796708107, 0.11154752224683762, 0.11154346913099289, 0.11153940111398697, 0.1115354672074318, 0.11153161525726318, 0.11152780055999756, 0.1115240529179573, 0.11152035742998123, 0.11151671409606934, 0.11151310801506042, 0.11150956153869629, 0.11150602251291275, 0.1115025132894516, 0.11149912327528, 0.11149570345878601, 0.11149239540100098, 0.11148908734321594, 0.11148586869239807, 0.1114826574921608, 0.11147946864366531, 0.1114763543009758, 0.11147333681583405, 0.11147025972604752, 0.11146727204322815, 0.11146429181098938, 0.1114613488316536, 0.11145845800638199, 0.11145558953285217, 0.11145276576280594, 0.1114499419927597, 0.11144722998142242, 0.11144450306892395, 0.11144182085990906, 0.11143917590379715, 0.11143651604652405, 0.11143387854099274, 0.1114313080906868, 0.11142872273921967, 0.11142616719007492, 0.11142367869615555, 0.11142119020223618, 0.11141873896121979, 0.11141631007194519, 0.11141390353441238, 0.11141154170036316, 0.11140915006399155, 0.1114068403840065, 0.11140450835227966, 0.1114022433757782, 0.11139999330043793, 0.11139777302742004, 0.11139553040266037, 0.11139333248138428, 0.11139117181301117, 0.11138904094696045, 0.11138693243265152, 0.11138486862182617, 0.11138280481100082, 0.11138071119785309, 0.11137870699167252, 0.11137672513723373, 0.11137475073337555, 0.11137279123067856, 0.11137088388204575, 0.11136894673109055, 0.11136707663536072, 0.11136523634195328, 0.11136335879564285, 0.1113615334033966, 0.11135970056056976, 0.1113579273223877, 0.11135613918304443, 0.11135438084602356, 0.11135265231132507, 0.11135096102952957, 0.11134923994541168, 0.11134754121303558, 0.11134590208530426, 0.11134425550699234, 0.11134261637926102, 0.11134099960327148, 0.11133941262960434, 0.1113378182053566, 0.11133626103401184, 0.11133473366498947, 0.1113331988453865, 0.11133164912462234, 0.11133016645908356, 0.11132863909006119, 0.11132720112800598, 0.11132574081420898, 0.11132427304983139, 0.11132282763719559, 0.11132141947746277, 0.11132003366947174, 0.11131864041090012, 0.11131726205348969, 0.11131588369607925, 0.1113145649433136, 0.11131320893764496, 0.1113118827342987, 0.11131056398153305, 0.11130928993225098, 0.11130799353122711, 0.11130671948194504, 0.11130547523498535, 0.11130417883396149, 0.11130297183990479, 0.11130174249410629, 0.11130053550004959, 0.11129934340715408, 0.11129813641309738, 0.11129694432020187, 0.11129578202962875, 0.11129462718963623, 0.11129347234964371, 0.11129236221313477, 0.11129123717546463, 0.11129012703895569, 0.11128903925418854, 0.111287921667099, 0.11128687858581543, 0.11128577589988708, 0.11128473281860352, 0.11128371208906174, 0.11128266900777817, 0.1112816333770752, 0.11128061264753342, 0.11127960681915283, 0.11127860099077225, 0.11127763241529465, 0.11127662658691406, 0.11127568781375885, 0.11127468943595886, 0.11127375066280365, 0.11127281188964844, 0.11127185821533203, 0.11127093434333801, 0.11127001792192459, 0.11126908659934998, 0.11126814782619476, 0.11126723885536194, 0.11126633733510971, 0.1112653911113739, 0.11126449704170227, 0.11126358062028885, 0.11126269400119781, 0.11126181483268738, 0.11126094311475754, 0.1112600713968277, 0.11125922203063965, 0.11125838756561279, 0.11125751584768295, 0.1112566813826561, 0.11125586926937103, 0.11125503480434418, 0.11125423014163971, 0.11125338077545166, 0.11125257611274719, 0.11125174909830093, 0.11125095933675766, 0.11125017702579498, 0.11124936491250992, 0.11124857515096664, 0.11124780774116516, 0.11124702543020248, 0.11124624311923981, 0.11124551296234131, 0.11124473065137863, 0.11124399304389954, 0.11124324798583984, 0.11124249547719955, 0.11124176532030106, 0.11124103516340256, 0.11124028265476227, 0.11123958975076675, 0.11123887449502945, 0.11123817414045334, 0.11123747378587723, 0.11123678088188171, 0.11123611032962799, 0.11123544722795486, 0.11123479902744293, 0.111234150826931, 0.11123350262641907, 0.11123282462358475, 0.1112322136759758, 0.11123156547546387]\n",
      "@_@\t\tAUC: [0.39146825671195984, 0.7571427822113037, 0.8629629611968994, 0.8740741014480591, 0.8814814686775208, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tglobal_accuracy: [0.625, 0.7361111044883728, 0.7638888955116272, 0.8611111044883728, 0.9305555820465088, 0.9444444179534912, 0.9861111044883728, 0.9861111044883728, 0.9861111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tglobal_precision: [0.40625, 0.75, 1.0, 0.9230769276618958, 0.8999999761581421, 0.9473684430122375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tglobal_recall: [0.6190476417541504, 0.1428571492433548, 0.190476194024086, 0.5714285969734192, 0.8571428656578064, 0.8571428656578064, 0.9523809552192688, 0.9523809552192688, 0.9523809552192688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_loss: [0.615707516670227, 0.4444287419319153, 0.33237719535827637, 0.2909916043281555, 0.23274588584899902, 0.18808716535568237, 0.15039882063865662, 0.12195207178592682, 0.1001921147108078, 0.08296866714954376, 0.06683092564344406, 0.0523676872253418, 0.04133526235818863, 0.03327580541372299, 0.02698678709566593, 0.021942781284451485, 0.01791556179523468, 0.014650117605924606, 0.012028709053993225, 0.009963707998394966, 0.008367521688342094, 0.0071450648829340935, 0.006185103207826614, 0.005395552609115839, 0.004722668789327145, 0.004132281988859177, 0.00362135237082839, 0.0031867451034486294, 0.002820739056915045, 0.002518225461244583, 0.002271429169923067, 0.002067635301500559, 0.0018963934853672981, 0.0017501316033303738, 0.0016234235372394323, 0.0015119760064408183, 0.0014119213446974754, 0.0013213548809289932, 0.0012384983710944653, 0.0011628670617938042, 0.0010938034392893314, 0.0010306616313755512, 0.0009737784857861698, 0.0009216293692588806, 0.000874796649441123, 0.0008329319534823298, 0.0007957752677612007, 0.0007628587773069739, 0.0007335697300732136, 0.0007074910681694746, 0.0006842324510216713, 0.0006633220473304391, 0.0006444102618843317, 0.0006269981386139989, 0.0006109813693910837, 0.0005959706613793969, 0.0005818167701363564, 0.0005684846546500921, 0.0005557583644986153, 0.0005436133360490203, 0.000532153935637325, 0.0005213631666265428, 0.0005111526697874069, 0.000501418428029865, 0.0004921348299831152, 0.0004832786798942834, 0.00047489357530139387, 0.0004670643247663975, 0.00045953813241794705, 0.0004523458192124963, 0.00044539012014865875, 0.0004387527296785265, 0.00043234415352344513, 0.0004261676222085953, 0.0004202802083455026, 0.00041468319250270724, 0.00040925387293100357, 0.00040395642281509936, 0.000398813106585294, 0.0003938179579563439, 0.0003889434738084674, 0.0003842891601379961, 0.00037973112193867564, 0.00037525349762290716, 0.00037085983785800636, 0.000366617226973176, 0.00036243355134502053, 0.00035831128479912877, 0.0003542945487424731, 0.00035038412897847593, 0.00034658858203329146, 0.00034285621950402856, 0.00033913057995960116, 0.0003354540094733238, 0.000331841001752764, 0.00032827118411660194, 0.00032473698956891894, 0.0003212721785530448, 0.0003178940969519317, 0.0003145794034935534, 0.0003113215498160571, 0.0003081034228671342, 0.0003049416118301451, 0.0003018947027157992, 0.00029891758458688855, 0.000295967620331794, 0.00029304553754627705, 0.0002901607658714056, 0.00028732852661050856, 0.00028455216670408845, 0.0002817994973156601, 0.00027908419724553823, 0.00027639896143227816, 0.00027376387151889503, 0.0002711777051445097, 0.0002686638617888093, 0.0002661706239450723, 0.0002637184807099402, 0.0002612894168123603, 0.0002589059295132756, 0.00025655573699623346, 0.0002542261208873242, 0.0002519221743568778, 0.00024964328622445464, 0.0002474167267791927, 0.0002452138578519225, 0.00024304496764671057, 0.0002408925793133676, 0.00023877547937445343, 0.00023668355424888432, 0.00023461529053747654, 0.00023260120360646397, 0.0002306112291989848, 0.00022863419144414365, 0.00022667404846288264, 0.00022474532306659967, 0.0002228367084171623, 0.00022095625172369182, 0.00021909749193582684, 0.00021726262639276683, 0.00021544838091358542, 0.0002136508992407471, 0.000211895297979936, 0.00021016053506173193, 0.00020843549282290041, 0.00020672057871706784, 0.00020502797269728035, 0.00020334463624749333, 0.00020168122136965394, 0.00020003726240247488, 0.00019841744506265968, 0.00019680721743497998, 0.00019520349451340735, 0.0001936361368279904, 0.0001920850045280531, 0.00019054098811466247, 0.0001890107523649931, 0.00018750785966403782, 0.0001860263291746378, 0.00018456047109793872, 0.00018310609448235482, 0.00018166624067816883, 0.0001802448241505772, 0.00017882758402265608, 0.00017743281205184758, 0.00017605366883799434, 0.00017469734302721918, 0.00017335830489173532, 0.00017202481103595346, 0.00017070041212718934, 0.00016939887427724898, 0.0001681101566646248, 0.0001668384938966483, 0.0001655876694712788, 0.00016434694407507777, 0.00016312109073624015, 0.00016191942268051207, 0.00016071998106781393, 0.00015952283865772188, 0.00015834734949748963, 0.000157177506480366, 0.00015602554776705801, 0.000154885885422118, 0.00015376118244603276, 0.0001526462729088962, 0.0001515449839644134, 0.00015045270265545696, 0.00014937279047444463, 0.000148308725329116, 0.0001472493022447452, 0.00014619533612858504, 0.00014516252849716693, 0.00014413654571399093, 0.00014312297571450472, 0.00014212072710506618, 0.0001411326666129753, 0.00014014390762895346, 0.00013916655734647065, 0.00013820032472722232, 0.00013724356540478766, 0.00013630354078486562, 0.00013536469487007707, 0.00013443565694615245, 0.00013351711095310748, 0.00013261669664643705, 0.0001317170972470194, 0.0001308259816141799, 0.00012993979908060282, 0.00012906809570267797, 0.00012820838310290128, 0.00012735230848193169, 0.00012650164717342705, 0.00012566446093842387, 0.00012483457976486534, 0.0001240083365701139, 0.0001231932546943426, 0.0001223865256179124, 0.00012158750905655324, 0.00012079627776984125, 0.00012001550931017846, 0.0001192404524772428, 0.00011847188579849899, 0.00011770874698413536, 0.00011694786371663213, 0.00011620295845204964, 0.00011546023597475141, 0.00011472358892206103, 0.00011399516370147467, 0.00011328142136335373, 0.0001125672715716064, 0.00011186198389623314, 0.00011116509995190427, 0.00011047537554986775, 0.00010978782665915787, 0.0001091097219614312, 0.0001084519608411938, 0.00010778618889162317, 0.00010712297807913274, 0.00010647442832123488, 0.00010582984396023676, 0.00010518675844650716, 0.00010455208393977955, 0.00010392061085440218, 0.00010329406359232962, 0.00010267434117849916, 0.00010206137085333467, 0.00010145171836484224, 0.00010084952373290434, 0.00010025297524407506, 9.964851778931916e-05, 9.904283797368407e-05, 9.843573934631422e-05, 9.783252608031034e-05, 9.723104449221864e-05, 9.662936645327136e-05, 9.602926729712635e-05, 9.543094347463921e-05, 9.484078327659518e-05, 9.425874304724857e-05, 9.367940947413445e-05, 9.310402674600482e-05, 9.253417374566197e-05, 9.196800965582952e-05, 9.140541806118563e-05, 9.085105557460338e-05, 9.030409273691475e-05, 8.975985110737383e-05, 8.921510743675753e-05, 8.867747965268791e-05, 8.8144835899584e-05, 8.76136400620453e-05, 8.709180838195607e-05, 8.657287253299728e-05, 8.605590846855193e-05, 8.554902160540223e-05, 8.504636934958398e-05, 8.454869384877384e-05, 8.405426342505962e-05, 8.356261241715401e-05, 8.307453390443698e-05, 8.25883325887844e-05, 8.210174564737827e-05, 8.161916048265994e-05, 8.114238153211772e-05, 8.06740063126199e-05, 8.020590030355379e-05, 7.974230538820848e-05, 7.928317063488066e-05, 7.883212674641982e-05, 7.839288446120918e-05, 7.796406862325966e-05, 7.754052057862282e-05, 7.71218110457994e-05, 7.670558989048004e-05, 7.629238825757056e-05, 7.588243897771463e-05, 7.547553104814142e-05, 7.507366535719484e-05, 7.467549585271627e-05, 7.427918899338692e-05]\n",
      "@_@\t\tval_macro_f1_score: [0.13333334028720856, 0.2370370477437973, 0.47654321789741516, 0.6388888955116272, 0.6507936716079712, 0.8518518805503845, 0.8518518805503845, 0.8518518805503845, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tval_soft_f1_loss: [0.7451985478401184, 0.6419987082481384, 0.5425425171852112, 0.47292014956474304, 0.4186283349990845, 0.37649303674697876, 0.3364664912223816, 0.30027833580970764, 0.2674351930618286, 0.2400457262992859, 0.2150079905986786, 0.19215482473373413, 0.17407472431659698, 0.16124585270881653, 0.15181268751621246, 0.1444353461265564, 0.13849100470542908, 0.1335533857345581, 0.1294834017753601, 0.12628142535686493, 0.1238153800368309, 0.12194053083658218, 0.1204746812582016, 0.11926844716072083, 0.11824504286050797, 0.1173471063375473, 0.11657532304525375, 0.11592277139425278, 0.1153775081038475, 0.11492998152971268, 0.11456617712974548, 0.11426540464162827, 0.11401084065437317, 0.11379110813140869, 0.11359845101833344, 0.1134265810251236, 0.11327110230922699, 0.1131296306848526, 0.11300002038478851, 0.11288174241781235, 0.11277402937412262, 0.11267602443695068, 0.11258824914693832, 0.11250800639390945, 0.11243637651205063, 0.11237268894910812, 0.11231645196676254, 0.1122669130563736, 0.11222304403781891, 0.112184077501297, 0.11214938759803772, 0.11211816221475601, 0.11208987236022949, 0.11206385493278503, 0.11203993856906891, 0.11201757937669754, 0.11199642717838287, 0.11197659373283386, 0.11195772141218185, 0.11193977296352386, 0.1119229644536972, 0.1119072437286377, 0.11189235746860504, 0.11187819391489029, 0.11186468601226807, 0.11185179650783539, 0.11183958500623703, 0.11182819306850433, 0.11181722581386566, 0.11180680245161057, 0.11179675161838531, 0.11178711801767349, 0.11177781224250793, 0.11176881194114685, 0.11176028847694397, 0.11175206303596497, 0.11174403131008148, 0.11173619329929352, 0.11172860115766525, 0.11172114312648773, 0.11171391606330872, 0.1117069348692894, 0.111700139939785, 0.11169343441724777, 0.11168685555458069, 0.11168045550584793, 0.11167417466640472, 0.11166802793741226, 0.11166201531887054, 0.11165617406368256, 0.11165051907300949, 0.11164490133523941, 0.11163923144340515, 0.11163359135389328, 0.11162804067134857, 0.11162251979112625, 0.11161711812019348, 0.11161179095506668, 0.11160659790039062, 0.11160153895616531, 0.11159658432006836, 0.11159165948629379, 0.11158683896064758, 0.11158222705125809, 0.11157769709825516, 0.1115732192993164, 0.11156879365444183, 0.11156442761421204, 0.11156010627746582, 0.11155586689710617, 0.1115516796708107, 0.11154752224683762, 0.11154346913099289, 0.11153940111398697, 0.1115354672074318, 0.11153161525726318, 0.11152780055999756, 0.1115240529179573, 0.11152035742998123, 0.11151671409606934, 0.11151312291622162, 0.11150956153869629, 0.11150603741407394, 0.1115025132894516, 0.1114991083741188, 0.11149570345878601, 0.11149239540100098, 0.11148908734321594, 0.11148586869239807, 0.1114826500415802, 0.11147946864366531, 0.1114763543009758, 0.11147333681583405, 0.11147025972604752, 0.11146727204322815, 0.11146429181098938, 0.1114613488316536, 0.11145845800638199, 0.11145558953285217, 0.11145276576280594, 0.1114499419927597, 0.11144722998142242, 0.11144450306892395, 0.11144182085990906, 0.11143917590379715, 0.11143651604652405, 0.11143387854099274, 0.1114313080906868, 0.11142872273921967, 0.11142616719007492, 0.11142367869615555, 0.11142119020223618, 0.11141873896121979, 0.11141631007194519, 0.11141390353441238, 0.11141154170036316, 0.11140915006399155, 0.1114068403840065, 0.11140450835227966, 0.1114022433757782, 0.11139999330043793, 0.11139777302742004, 0.11139553040266037, 0.11139333248138428, 0.11139117181301117, 0.11138904094696045, 0.11138693243265152, 0.11138486862182617, 0.11138280481100082, 0.11138071119785309, 0.11137869954109192, 0.11137672513723373, 0.11137475073337555, 0.11137279123067856, 0.11137086898088455, 0.11136894673109055, 0.11136706918478012, 0.11136523634195328, 0.11136335879564285, 0.1113615334033966, 0.11135970056056976, 0.1113579273223877, 0.11135613918304443, 0.11135438084602356, 0.11135265231132507, 0.11135094612836838, 0.11134923994541168, 0.11134755611419678, 0.11134590208530426, 0.11134425550699234, 0.11134260147809982, 0.11134099960327148, 0.11133941262960434, 0.1113378182053566, 0.11133626103401184, 0.11133473366498947, 0.1113332137465477, 0.11133166402578354, 0.11133016645908356, 0.11132863909006119, 0.11132720112800598, 0.11132574081420898, 0.11132427304983139, 0.11132282763719559, 0.11132141947746277, 0.11132003366947174, 0.11131864041090012, 0.11131726205348969, 0.11131588369607925, 0.1113145649433136, 0.11131320893764496, 0.1113118827342987, 0.11131057143211365, 0.11130928993225098, 0.11130799353122711, 0.11130671948194504, 0.11130547523498535, 0.11130417883396149, 0.11130297183990479, 0.11130174249410629, 0.11130053550004959, 0.11129934340715408, 0.11129813641309738, 0.11129694432020187, 0.11129578202962875, 0.11129462718963623, 0.11129347234964371, 0.11129236221313477, 0.11129123717546463, 0.11129012703895569, 0.11128903925418854, 0.1112879142165184, 0.11128686368465424, 0.11128577589988708, 0.11128474771976471, 0.11128371208906174, 0.11128266900777817, 0.1112816333770752, 0.11128061264753342, 0.11127960681915283, 0.11127861589193344, 0.11127763241529465, 0.11127662658691406, 0.11127568781375885, 0.11127468943595886, 0.11127375066280365, 0.11127281188964844, 0.11127185821533203, 0.11127093434333801, 0.11127001792192459, 0.11126908659934998, 0.11126814782619476, 0.11126723885536194, 0.11126632243394852, 0.1112653911113739, 0.11126449704170227, 0.11126358062028885, 0.11126269400119781, 0.11126180738210678, 0.11126094311475754, 0.1112600713968277, 0.11125920712947845, 0.11125838756561279, 0.11125750094652176, 0.1112566813826561, 0.11125586926937103, 0.11125503480434418, 0.11125421524047852, 0.11125338077545166, 0.11125257611274719, 0.11125174909830093, 0.11125095933675766, 0.11125017702579498, 0.11124936491250992, 0.11124857515096664, 0.11124780774116516, 0.11124702543020248, 0.111246258020401, 0.11124551296234131, 0.11124473065137863, 0.11124399304389954, 0.11124324798583984, 0.11124249547719955, 0.11124176532030106, 0.11124103516340256, 0.11124028265476227, 0.11123958975076675, 0.11123887449502945, 0.11123817414045334, 0.11123747378587723, 0.11123678088188171, 0.11123612523078918, 0.11123543232679367, 0.11123479902744293, 0.111234150826931, 0.11123350262641907, 0.11123282462358475, 0.1112322136759758, 0.11123156547546387, 0.11123093962669373]\n",
      "@_@\t\tval_AUC: [0.7571427822113037, 0.8629629611968994, 0.8740741014480591, 0.8814814686775208, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tval_global_accuracy: [0.7361111044883728, 0.7638888955116272, 0.8611111044883728, 0.9305555820465088, 0.9444444179534912, 0.9861111044883728, 0.9861111044883728, 0.9861111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_global_precision: [0.75, 1.0, 0.9230769276618958, 0.8999999761581421, 0.9473684430122375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_global_recall: [0.1428571492433548, 0.190476194024086, 0.5714285969734192, 0.8571428656578064, 0.8571428656578064, 0.9523809552192688, 0.9523809552192688, 0.9523809552192688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1850/0_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.0001\n",
      "@_@\tValidation Macro F1-score: 0.8889\n",
      "W0402 18:56:40.279477 125766439323456 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1850/0_unfrozen_block/model/saved_model/assets\n",
      "I0402 18:56:45.056556 125766439323456 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1850/0_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1850/0_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Evaluate ------------------\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1850/predict_sample.png\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "@_@\tResult of testing on 1 batches:\n",
      "@_@\t\tbinary_crossentropy: 7.416828884743154e-05\n",
      "@_@\t\tAUC: 0.8888888955116272\n",
      "@_@\t\tmacro_f1_score: 0.8888888955116272\n",
      "@_@\t\tsoft_f1_loss: 0.11123093962669373\n",
      "@_@\t\tglobal_accuracy: 1.0\n",
      "@_@\t\tglobal_precision: 1.0\n",
      "@_@\t\tglobal_recall: 1.0\n",
      "@_@\t\n",
      "@_@\tDONE!\n"
     ]
    }
   ],
   "source": [
    "!python ../neural_nets/resnet50_test.py \\\n",
    "  --ouput_name=resnet50-transfer-miniBatchTest \\\n",
    "  --image_size=224 --epochs=300 --batch_size=8 --train_size=8 \\\n",
    "  --mode=train_then_eval --min_unfreeze_blocks=0 --max_unfreeze_blocks=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cwd to '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation'\n",
      "['/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode/neural_nets', '/home/payam/miniconda3/envs/tf2-gpu/lib/python39.zip', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/lib-dynload', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages', '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode']\n",
      "/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation\n",
      "2024-04-02 19:26:31.429949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 19:26:31.962565: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-02 19:26:31.962673: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-02 19:26:31.962682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-02 19:26:33.001866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.039157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.039358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.039753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 19:26:33.040901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.041093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.041250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.478622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.478839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.478995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-02 19:26:33.479124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "@_@\t\n",
      "@_@\t------------------ Configuration ------------------\n",
      "@_@\tStart: 2024-04-02_1926\n",
      "@_@\t\n",
      "@_@\tMode: train_then_eval\n",
      "@_@\tUnfreeze blocks: start 0, end 0\n",
      "@_@\tContinue from checkpoint: False\n",
      "@_@\t\n",
      "@_@\tDataset: MIMIC-CXR\n",
      "@_@\tTraining Dataset Size: 8\n",
      "@_@\tBatch size: 8\n",
      "@_@\tImage size: (448, 448)\n",
      "@_@\tMax Epochs per training round: 300\n",
      "@_@\tDefault Learning Rate: 0.1\n",
      "@_@\t\n",
      "@_@\tGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "@_@\tCPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "@_@\t\n",
      "@_@\t------------------ Data ------------------\n",
      "@_@\tnum_classes: 9\n",
      "@_@\tclass_names: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Pleural Effusion'\n",
      "@_@\t 'Pneumonia' 'Pneumothorax' 'Fracture' 'Support Devices']\n",
      "@_@\tall_data_size: 377110\n",
      "@_@\tall_data_filtered_size: 112134\n",
      "@_@\tsplit_size: {'train': 8, 'validate': 889, 'test': 512}\n",
      "@_@\tmax value before preprocess tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "@_@\tmin value before preprocess tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "@_@\tShape of image batch: [2402, 2697, 3]\n",
      "@_@\tShape of labels batch: [9]\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0402 19:27:20.213532 125225695786816 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "@_@\tmax value after preprocess tf.Tensor(255, shape=(), dtype=uint8)\n",
      "@_@\tmin value after preprocess tf.Tensor(0, shape=(), dtype=uint8)\n",
      "@_@\tShape of image batch: [8, 448, 448, 3]\n",
      "@_@\tShape of labels batch: [8, 9]\n",
      "Model: \"my_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 448, 448, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 454, 454, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 224, 224, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 224, 224, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 224, 224, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 226, 226, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 112, 112, 64  0           ['pool1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 112, 112, 64  4160        ['pool1_pool[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block1_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 112, 112, 25  16640       ['pool1_pool[0][0]']             \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 112, 112, 25  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                6)                                'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 112, 112, 25  0           ['conv2_block1_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 112, 112, 64  16448       ['conv2_block1_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block2_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 112, 112, 25  0           ['conv2_block1_out[0][0]',       \n",
      "                                6)                                'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 112, 112, 25  0           ['conv2_block2_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 112, 112, 64  16448       ['conv2_block2_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 112, 112, 64  36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 112, 112, 64  256        ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 112, 112, 64  0          ['conv2_block3_2_bn[0][0]']      \n",
      " n)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 112, 112, 25  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 112, 112, 25  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                       6)                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 112, 112, 25  0           ['conv2_block2_out[0][0]',       \n",
      "                                6)                                'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 112, 112, 25  0           ['conv2_block3_add[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 56, 56, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 56, 56, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 56, 56, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 56, 56, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 56, 56, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 56, 56, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 56, 56, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 56, 56, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 56, 56, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 56, 56, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 56, 56, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 56, 56, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 28, 28, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 28, 28, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 28, 28, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 28, 28, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 28, 28, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 28, 28, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 28, 28, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 28, 28, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 28, 28, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 28, 28, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 28, 28, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 28, 28, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 28, 28, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 28, 28, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 14, 14, 512)  524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 14, 14, 2048  2099200     ['conv4_block6_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 14, 14, 2048  0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 14, 14, 512)  1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 14, 14, 2048  0           ['conv5_block1_out[0][0]',       \n",
      "                                )                                 'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 14, 14, 512)  1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 14, 14, 512)  2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 14, 14, 512)  0          ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 14, 14, 2048  1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 14, 14, 2048  8192       ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 14, 14, 2048  0           ['conv5_block2_out[0][0]',       \n",
      "                                )                                 'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 14, 14, 2048  0           ['conv5_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " my_avg_pool (GlobalMaxPooling2  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " my_fc_1 (Dense)                (None, 256)          524544      ['my_avg_pool[0][0]']            \n",
      "                                                                                                  \n",
      " my_fc_2 (Dense)                (None, 128)          32896       ['my_fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " my_output (Dense)              (None, 9)            1161        ['my_fc_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,146,313\n",
      "Trainable params: 558,601\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "@_@\tUnfreezing 0 blocks...\n",
      "@_@\tTotal trainable weights: 6\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 0.001\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0402 19:27:22.931565 125225695786816 deprecation.py:554] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2024-04-02 19:27:25.911157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2024-04-02 19:27:26.571608: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x71e29c015460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-02 19:27:26.571631: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-04-02 19:27:26.577877: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-02 19:27:26.695135: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.6487 - macro_f1_score: 0.2537 - soft_f1_loss: 0.7131 - AUC: 0.5567 - global_accuracy: 0.6111 - global_precision: 0.3333 - global_recall: 0.3333 - val_loss: 4.9273 - val_macro_f1_score: 0.3605 - val_soft_f1_loss: 0.6529 - val_AUC: 0.5755 - val_global_accuracy: 0.5694 - val_global_precision: 0.3750 - val_global_recall: 0.7143\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9273 - macro_f1_score: 0.3605 - soft_f1_loss: 0.6529 - AUC: 0.5755 - global_accuracy: 0.5694 - global_precision: 0.3750 - global_recall: 0.7143 - val_loss: 1.5485 - val_macro_f1_score: 0.1364 - val_soft_f1_loss: 0.8105 - val_AUC: 0.5681 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.2857\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5485 - macro_f1_score: 0.1364 - soft_f1_loss: 0.8105 - AUC: 0.5681 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.2857 - val_loss: 2.6246 - val_macro_f1_score: 0.2551 - val_soft_f1_loss: 0.7413 - val_AUC: 0.5764 - val_global_accuracy: 0.7500 - val_global_precision: 0.6000 - val_global_recall: 0.4286\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6246 - macro_f1_score: 0.2551 - soft_f1_loss: 0.7413 - AUC: 0.5764 - global_accuracy: 0.7500 - global_precision: 0.6000 - global_recall: 0.4286 - val_loss: 2.1126 - val_macro_f1_score: 0.2759 - val_soft_f1_loss: 0.6876 - val_AUC: 0.6875 - val_global_accuracy: 0.7083 - val_global_precision: 0.5000 - val_global_recall: 0.4762\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1126 - macro_f1_score: 0.2759 - soft_f1_loss: 0.6876 - AUC: 0.6875 - global_accuracy: 0.7083 - global_precision: 0.5000 - global_recall: 0.4762 - val_loss: 1.6620 - val_macro_f1_score: 0.4283 - val_soft_f1_loss: 0.5871 - val_AUC: 0.7176 - val_global_accuracy: 0.7778 - val_global_precision: 0.6087 - val_global_recall: 0.6667\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6620 - macro_f1_score: 0.4283 - soft_f1_loss: 0.5871 - AUC: 0.7176 - global_accuracy: 0.7778 - global_precision: 0.6087 - global_recall: 0.6667 - val_loss: 1.2371 - val_macro_f1_score: 0.4259 - val_soft_f1_loss: 0.5996 - val_AUC: 0.7222 - val_global_accuracy: 0.8056 - val_global_precision: 0.6842 - val_global_recall: 0.6190\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2371 - macro_f1_score: 0.4259 - soft_f1_loss: 0.5996 - AUC: 0.7222 - global_accuracy: 0.8056 - global_precision: 0.6842 - global_recall: 0.6190 - val_loss: 1.2262 - val_macro_f1_score: 0.4828 - val_soft_f1_loss: 0.5741 - val_AUC: 0.7528 - val_global_accuracy: 0.8194 - val_global_precision: 0.7222 - val_global_recall: 0.6190\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2262 - macro_f1_score: 0.4828 - soft_f1_loss: 0.5741 - AUC: 0.7528 - global_accuracy: 0.8194 - global_precision: 0.7222 - global_recall: 0.6190 - val_loss: 0.8263 - val_macro_f1_score: 0.5693 - val_soft_f1_loss: 0.4508 - val_AUC: 0.7778 - val_global_accuracy: 0.8611 - val_global_precision: 0.7895 - val_global_recall: 0.7143\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8263 - macro_f1_score: 0.5693 - soft_f1_loss: 0.4508 - AUC: 0.7778 - global_accuracy: 0.8611 - global_precision: 0.7895 - global_recall: 0.7143 - val_loss: 0.5804 - val_macro_f1_score: 0.6000 - val_soft_f1_loss: 0.4260 - val_AUC: 0.7778 - val_global_accuracy: 0.9167 - val_global_precision: 1.0000 - val_global_recall: 0.7143\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5804 - macro_f1_score: 0.6000 - soft_f1_loss: 0.4260 - AUC: 0.7778 - global_accuracy: 0.9167 - global_precision: 1.0000 - global_recall: 0.7143 - val_loss: 0.6181 - val_macro_f1_score: 0.4526 - val_soft_f1_loss: 0.5064 - val_AUC: 0.7917 - val_global_accuracy: 0.8194 - val_global_precision: 0.7000 - val_global_recall: 0.6667\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6181 - macro_f1_score: 0.4526 - soft_f1_loss: 0.5064 - AUC: 0.7917 - global_accuracy: 0.8194 - global_precision: 0.7000 - global_recall: 0.6667 - val_loss: 0.1313 - val_macro_f1_score: 0.8586 - val_soft_f1_loss: 0.2123 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 0.8750 - val_global_recall: 1.0000\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1313 - macro_f1_score: 0.8586 - soft_f1_loss: 0.2123 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 0.8750 - global_recall: 1.0000 - val_loss: 0.0762 - val_macro_f1_score: 0.8444 - val_soft_f1_loss: 0.2040 - val_AUC: 0.8889 - val_global_accuracy: 0.9722 - val_global_precision: 0.9130 - val_global_recall: 1.0000\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0762 - macro_f1_score: 0.8444 - soft_f1_loss: 0.2040 - AUC: 0.8889 - global_accuracy: 0.9722 - global_precision: 0.9130 - global_recall: 1.0000 - val_loss: 0.3816 - val_macro_f1_score: 0.7286 - val_soft_f1_loss: 0.2893 - val_AUC: 0.8889 - val_global_accuracy: 0.8611 - val_global_precision: 0.6774 - val_global_recall: 1.0000\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3816 - macro_f1_score: 0.7286 - soft_f1_loss: 0.2893 - AUC: 0.8889 - global_accuracy: 0.8611 - global_precision: 0.6774 - global_recall: 1.0000 - val_loss: 0.0792 - val_macro_f1_score: 0.8296 - val_soft_f1_loss: 0.2058 - val_AUC: 0.8889 - val_global_accuracy: 0.9722 - val_global_precision: 0.9130 - val_global_recall: 1.0000\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0792 - macro_f1_score: 0.8296 - soft_f1_loss: 0.2058 - AUC: 0.8889 - global_accuracy: 0.9722 - global_precision: 0.9130 - global_recall: 1.0000 - val_loss: 0.0250 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1458 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0250 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1458 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.1145 - val_macro_f1_score: 0.6963 - val_soft_f1_loss: 0.2476 - val_AUC: 0.8889 - val_global_accuracy: 0.9167 - val_global_precision: 0.9412 - val_global_recall: 0.7619\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1145 - macro_f1_score: 0.6963 - soft_f1_loss: 0.2476 - AUC: 0.8889 - global_accuracy: 0.9167 - global_precision: 0.9412 - global_recall: 0.7619 - val_loss: 0.0901 - val_macro_f1_score: 0.7778 - val_soft_f1_loss: 0.2259 - val_AUC: 0.8889 - val_global_accuracy: 0.9583 - val_global_precision: 1.0000 - val_global_recall: 0.8571\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0901 - macro_f1_score: 0.7778 - soft_f1_loss: 0.2259 - AUC: 0.8889 - global_accuracy: 0.9583 - global_precision: 1.0000 - global_recall: 0.8571 - val_loss: 0.0333 - val_macro_f1_score: 0.8519 - val_soft_f1_loss: 0.1566 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 1.0000 - val_global_recall: 0.9524\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0333 - macro_f1_score: 0.8519 - soft_f1_loss: 0.1566 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 1.0000 - global_recall: 0.9524 - val_loss: 0.0206 - val_macro_f1_score: 0.8519 - val_soft_f1_loss: 0.1420 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 1.0000 - val_global_recall: 0.9524\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0206 - macro_f1_score: 0.8519 - soft_f1_loss: 0.1420 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 1.0000 - global_recall: 0.9524 - val_loss: 0.0102 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1250 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1250 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0199 - val_macro_f1_score: 0.8730 - val_soft_f1_loss: 0.1283 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 0.9545 - val_global_recall: 1.0000\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0199 - macro_f1_score: 0.8730 - soft_f1_loss: 0.1283 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 0.9545 - global_recall: 1.0000 - val_loss: 0.0300 - val_macro_f1_score: 0.8730 - val_soft_f1_loss: 0.1316 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 0.9545 - val_global_recall: 1.0000\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0300 - macro_f1_score: 0.8730 - soft_f1_loss: 0.1316 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 0.9545 - global_recall: 1.0000 - val_loss: 0.0210 - val_macro_f1_score: 0.8730 - val_soft_f1_loss: 0.1304 - val_AUC: 0.8889 - val_global_accuracy: 0.9861 - val_global_precision: 0.9545 - val_global_recall: 1.0000\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0210 - macro_f1_score: 0.8730 - soft_f1_loss: 0.1304 - AUC: 0.8889 - global_accuracy: 0.9861 - global_precision: 0.9545 - global_recall: 1.0000 - val_loss: 0.0079 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1249 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0079 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1249 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0061 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1272 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0061 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1272 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0073 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1320 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1320 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0073 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1303 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1303 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0054 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1231 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0054 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1231 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0037 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1176 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0037 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1176 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0031 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1153 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0031 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1153 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0038 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1153 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0038 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1153 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0045 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1157 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0045 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1157 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0040 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1153 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0040 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1153 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0028 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1143 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0028 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1143 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0019 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1134 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0019 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1134 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0014 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1129 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0014 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1129 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 0.0010 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1125 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0010 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1125 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.4524e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1122 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.4524e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1122 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.1474e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1120 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.1474e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1120 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.2625e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1119 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.2625e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1119 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.6263e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1118 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6263e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1118 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.1161e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1161e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.6523e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1117 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6523e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1117 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2207e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2207e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8045e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1116 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8045e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1116 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4104e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4104e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0497e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1115 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0497e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1115 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7337e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7337e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4632e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4632e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.2361e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1114 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2361e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1114 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.0485e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0485e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.8956e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8956e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.7730e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7730e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.6740e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6740e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5939e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5939e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.5291e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5291e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4758e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4758e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.4307e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4307e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3916e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3916e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3566e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3566e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.3246e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3246e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2945e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2945e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2672e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2672e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2408e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1113 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2408e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1113 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.2150e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2150e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1897e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1897e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1650e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1650e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1410e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1410e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.1178e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1178e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0954e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0954e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0740e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0740e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0536e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0536e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0342e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0342e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 1.0156e-04 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0156e-04 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.9797e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.9797e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.8115e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.8115e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.6528e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.6528e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.5036e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.5036e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.3595e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.3595e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.2235e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.2235e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 9.0971e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.0971e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.9746e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.9746e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.8555e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.8555e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.7395e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.7395e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.6263e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.6263e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.5157e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.5157e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.4075e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.4075e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.3016e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.3016e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.1980e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.1980e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.0974e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.0974e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 8.0010e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 8.0010e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.9068e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.9068e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.8149e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.8149e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.7254e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.7254e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.6379e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.6379e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.5528e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.5528e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.4698e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.4698e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.3890e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.3890e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.3103e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.3103e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.2337e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.2337e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.1590e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.1590e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.0863e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.0863e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 7.0153e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.0153e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.9463e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.9463e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.8791e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.8791e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.8134e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.8134e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.7492e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.7492e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.6864e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.6864e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.6251e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.6251e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.5650e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.5650e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.5062e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.5062e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.4485e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.4485e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.3920e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.3920e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.3366e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.3366e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.2822e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.2822e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.2290e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.2290e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.1767e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.1767e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.1254e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.1254e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.0750e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0750e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 6.0255e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0255e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.9768e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.9768e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.9289e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.9289e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.8818e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.8818e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.8355e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.8355e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.7900e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7900e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.7452e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7452e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.7012e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7012e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.6581e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6581e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.6158e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6158e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.5739e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5739e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.5323e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5323e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.4919e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.4919e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.4525e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.4525e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.4132e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.4132e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.3741e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3741e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.3360e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3360e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.2985e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2985e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.2615e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2615e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.2250e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2250e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.1893e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1893e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.1540e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1540e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.1192e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1192e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.0848e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0848e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.0510e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0510e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 5.0178e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0178e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.9848e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9848e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.9522e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9522e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.9202e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9202e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.8886e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8886e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.8574e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8574e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.8265e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8265e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.7960e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7960e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.7660e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7660e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.7362e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7362e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.7068e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7068e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.6780e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6780e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.6493e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6493e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.6209e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6209e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.5930e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5930e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.5654e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5654e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.5381e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5381e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.5111e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5111e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.4844e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4844e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.4579e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4579e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.4319e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4319e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.4068e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4068e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.3819e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3819e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.3574e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3574e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.3331e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3331e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.3091e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3091e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2854e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2854e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2618e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2618e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2386e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2386e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.2155e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2155e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.1927e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1927e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.1702e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1702e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.1479e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1479e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.1258e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1258e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.1040e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1040e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.0824e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0824e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.0611e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0611e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.0399e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0399e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 4.0189e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0189e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9981e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9981e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9777e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9777e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9573e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9573e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9370e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9370e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.9171e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9171e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8974e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8974e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8778e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8778e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8584e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8584e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8392e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8392e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8203e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8203e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.8014e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8014e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7827e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7827e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7643e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7643e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7460e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7460e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7279e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7279e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.7099e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7099e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6921e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6921e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6744e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6744e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6571e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6571e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6398e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6398e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6225e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6225e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.6055e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6055e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5887e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5887e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5720e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5720e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5555e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5555e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5392e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5392e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5229e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5229e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.5068e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5068e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4908e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4908e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4750e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4750e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4594e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4594e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4438e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4438e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4283e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4283e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.4131e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4131e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3979e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3979e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3829e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3829e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3680e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3680e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3533e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3533e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3386e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3386e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3241e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3241e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.3097e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3097e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2954e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2954e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2812e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2812e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2672e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1112 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2672e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1112 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2533e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2533e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2395e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2395e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2257e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2257e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.2121e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2121e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1986e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1986e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1853e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1853e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1720e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1720e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1589e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1589e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1458e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1458e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1328e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1328e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1200e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1200e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.1073e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1073e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0946e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0946e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0820e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0820e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0696e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0696e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0572e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0572e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0449e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0449e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0328e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0328e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0208e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0208e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 3.0088e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0088e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9970e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9970e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9852e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9852e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9735e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9735e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9619e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9619e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9503e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9503e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9389e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9389e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9276e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9276e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9163e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9163e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.9051e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9051e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8941e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8941e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8830e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8830e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8721e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8721e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8613e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8613e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8505e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8505e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8398e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8398e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8292e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8292e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8187e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8187e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.8082e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8082e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7978e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7978e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7874e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7874e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7772e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7772e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7670e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7670e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7569e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7569e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7469e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7469e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7370e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7370e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7270e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7270e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7172e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7172e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.7075e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7075e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6978e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6978e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6882e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6882e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6786e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6786e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6691e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6691e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6596e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6596e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6502e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6502e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6409e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6409e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6317e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6317e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6225e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6225e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6134e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6134e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.6044e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6044e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5954e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5954e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5864e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5864e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5775e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5775e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5687e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5687e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5600e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5600e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5513e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5513e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5426e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5426e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5340e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5340e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5255e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5255e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5170e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5170e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5086e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5086e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.5002e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5002e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4919e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4919e-05 - macro_f1_score: 0.8889 - soft_f1_loss: 0.1111 - AUC: 0.8889 - global_accuracy: 1.0000 - global_precision: 1.0000 - global_recall: 1.0000 - val_loss: 2.4837e-05 - val_macro_f1_score: 0.8889 - val_soft_f1_loss: 0.1111 - val_AUC: 0.8889 - val_global_accuracy: 1.0000 - val_global_precision: 1.0000 - val_global_recall: 1.0000\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:5m:48s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [2.648730754852295, 4.927337646484375, 1.5485233068466187, 2.6246085166931152, 2.1126136779785156, 1.6619961261749268, 1.237125277519226, 1.226235270500183, 0.8262660503387451, 0.5803829431533813, 0.6180514097213745, 0.1313191056251526, 0.07618240267038345, 0.3815990686416626, 0.0792226493358612, 0.02496464177966118, 0.11452175676822662, 0.09013983607292175, 0.03326629847288132, 0.020556041970849037, 0.010247365571558475, 0.01986481249332428, 0.029965732246637344, 0.020963303744792938, 0.00794526468962431, 0.006104134954512119, 0.007288041524589062, 0.007280238904058933, 0.005388573743402958, 0.0036568548530340195, 0.0031288056634366512, 0.0037944165524095297, 0.004503883421421051, 0.0040035611018538475, 0.0028296385426074266, 0.0019108715932816267, 0.0013662537094205618, 0.0010455825831741095, 0.0008452415349893272, 0.0007147400756366551, 0.0006262502865865827, 0.0005626300117000937, 0.0005116054089739919, 0.0004652316856663674, 0.00042206826037727296, 0.0003804489388130605, 0.0003410422068554908, 0.0003049734514206648, 0.0002733748988248408, 0.0002463167766109109, 0.00022361119044944644, 0.00020484543347265571, 0.000189557031262666, 0.00017729547107592225, 0.00016739746206440032, 0.0001593912165844813, 0.0001529090804979205, 0.00014757944154553115, 0.00014307103992905468, 0.00013915718591306359, 0.0001356619322905317, 0.00013245621812529862, 0.00012944724585395306, 0.00012672120647039264, 0.00012408438487909734, 0.00012150072143413126, 0.00011897183867404237, 0.00011650349188130349, 0.00011410476145101711, 0.00011178130807820708, 0.0001095430925488472, 0.00010739719436969608, 0.00010535956243984401, 0.00010341545566916466, 0.00010156258940696716, 9.979748574551195e-05, 9.811524796532467e-05, 9.652777225710452e-05, 9.503596811555326e-05, 9.359548857901245e-05, 9.223518281942233e-05, 9.097106521949172e-05, 8.974601951194927e-05, 8.855493069859222e-05, 8.739496115595102e-05, 8.626332419225946e-05, 8.515684748999774e-05, 8.40751308714971e-05, 8.301623893203214e-05, 8.197968418244272e-05, 8.097430691123009e-05, 8.00097914179787e-05, 7.906807877589017e-05, 7.814948912709951e-05, 7.725350587861612e-05, 7.637929229531437e-05, 7.552803435828537e-05, 7.469808042515069e-05, 7.389021629933268e-05, 7.310316868824884e-05, 7.233666838146746e-05, 7.158979133237153e-05, 7.086251571308821e-05, 7.015336450422183e-05, 6.946329085621983e-05, 6.87906431267038e-05, 6.813399522798136e-05, 6.74919574521482e-05, 6.686439155600965e-05, 6.625080277444795e-05, 6.565013609360904e-05, 6.506170029751956e-05, 6.448512431234121e-05, 6.391988426912576e-05, 6.33658273727633e-05, 6.28220004728064e-05, 6.22896768618375e-05, 6.17674522800371e-05, 6.125417712610215e-05, 6.075008423067629e-05, 6.025478069204837e-05, 5.9767749917227775e-05, 5.928908285568468e-05, 5.881818287889473e-05, 5.835531192133203e-05, 5.789987699245103e-05, 5.7452209148323163e-05, 5.701215923181735e-05, 5.6581266107968986e-05, 5.615816917270422e-05, 5.573855742113665e-05, 5.532253271667287e-05, 5.491922638611868e-05, 5.452532786875963e-05, 5.413183316704817e-05, 5.3740783187095076e-05, 5.336043250281364e-05, 5.298518226481974e-05, 5.2614697779063135e-05, 5.224986671237275e-05, 5.189271905692294e-05, 5.1539998821681365e-05, 5.11917460244149e-05, 5.0848437240347266e-05, 5.050981417298317e-05, 5.017804869567044e-05, 4.9848207709146664e-05, 4.952220479026437e-05, 4.920219726045616e-05, 4.888610419584438e-05, 4.857389649259858e-05, 4.8265079385600984e-05, 4.795997665496543e-05, 4.765998892253265e-05, 4.7362442273879424e-05, 4.706784966401756e-05, 4.6779859985690564e-05, 4.649295442504808e-05, 4.6209308493416756e-05, 4.5930195483379066e-05, 4.5654349378310144e-05, 4.538146095001139e-05, 4.511134102358483e-05, 4.4843913201475516e-05, 4.457942850422114e-05, 4.4319236621959135e-05, 4.406810330692679e-05, 4.381909820949659e-05, 4.357357829576358e-05, 4.3331048800610006e-05, 4.3091051338706166e-05, 4.2853680497501045e-05, 4.261848152964376e-05, 4.2385567212477326e-05, 4.2154952097916976e-05, 4.192673077341169e-05, 4.170215834164992e-05, 4.147928848396987e-05, 4.125794657738879e-05, 4.1040264477487653e-05, 4.082435043528676e-05, 4.0610651922179386e-05, 4.039872146677226e-05, 4.018873369204812e-05, 3.9980597648536786e-05, 3.9776903577148914e-05, 3.95730858144816e-05, 3.937003202736378e-05, 3.9170903619378805e-05, 3.8973761547822505e-05, 3.877801646012813e-05, 3.858404670609161e-05, 3.8391932321246713e-05, 3.8202630094019696e-05, 3.8013997254893184e-05, 3.782739076996222e-05, 3.764314897125587e-05, 3.7460384191945195e-05, 3.727906005224213e-05, 3.709904558490962e-05, 3.692086465889588e-05, 3.674397521535866e-05, 3.6570945667335764e-05, 3.639784699771553e-05, 3.622496296884492e-05, 3.605545134632848e-05, 3.588728577597067e-05, 3.572028799680993e-05, 3.555455623427406e-05, 3.5391894925851375e-05, 3.522901170072146e-05, 3.5067525459453464e-05, 3.490845119813457e-05, 3.475042831269093e-05, 3.4593656891956925e-05, 3.4437871363479644e-05, 3.428337367950007e-05, 3.413101512705907e-05, 3.3979427826125175e-05, 3.382904105819762e-05, 3.368034231243655e-05, 3.353278952999972e-05, 3.3386200811946765e-05, 3.324062345200218e-05, 3.309698513476178e-05, 3.2953950722003356e-05, 3.281230965512805e-05, 3.2672334782546386e-05, 3.253300747019239e-05, 3.239471698179841e-05, 3.225737964385189e-05, 3.212109731975943e-05, 3.198628473910503e-05, 3.185265086358413e-05, 3.172010474372655e-05, 3.158860272378661e-05, 3.145801747450605e-05, 3.132829806418158e-05, 3.120044493698515e-05, 3.1072660931386054e-05, 3.0945819162297994e-05, 3.08204980683513e-05, 3.069603553740308e-05, 3.057244248338975e-05, 3.0449351470451802e-05, 3.0328370485221967e-05, 3.0207709642127156e-05, 3.008785643032752e-05, 2.996980219904799e-05, 2.9852135412511416e-05, 2.97351652989164e-05, 2.9618950065923855e-05, 2.9503409678000025e-05, 2.938870784419123e-05, 2.9275959605001844e-05, 2.9163074941607192e-05, 2.905139081121888e-05, 2.8940894480911084e-05, 2.883048728108406e-05, 2.872120967367664e-05, 2.8612679670914076e-05, 2.8504879082902335e-05, 2.8398007998475805e-05, 2.829158074746374e-05, 2.818664324877318e-05, 2.8081689379177988e-05, 2.797753768390976e-05, 2.787443736451678e-05, 2.7772173780249432e-05, 2.7670352210407145e-05, 2.7568949008127674e-05, 2.7469402994029224e-05, 2.7369636882212944e-05, 2.7270074497209862e-05, 2.7172018235432915e-05, 2.7074805984739214e-05, 2.697805393836461e-05, 2.6881762096309103e-05, 2.6785895897774026e-05, 2.669053901627194e-05, 2.6595811505103484e-05, 2.6502437322051264e-05, 2.6409266865812242e-05, 2.631687675602734e-05, 2.6224945031572133e-05, 2.613423566799611e-05, 2.6043808247777633e-05, 2.5953542717616074e-05, 2.5864312192425132e-05, 2.5775458198040724e-05, 2.5687080778880045e-05, 2.5599856599001214e-05, 2.551277975726407e-05, 2.5425953936064616e-05, 2.5340119464090094e-05, 2.5254772481275722e-05, 2.5170069420710206e-05, 2.5085744709940627e-05, 2.50023695116397e-05, 2.4919252609834075e-05]\n",
      "@_@\t\tmacro_f1_score: [0.2537277638912201, 0.36049383878707886, 0.13636364042758942, 0.25505051016807556, 0.2759259343147278, 0.4282507598400116, 0.42592594027519226, 0.4828282594680786, 0.5693121552467346, 0.6000000238418579, 0.45264551043510437, 0.8585858941078186, 0.8444444537162781, 0.7285714745521545, 0.82962965965271, 0.8888888955116272, 0.6962962746620178, 0.7777777910232544, 0.8518518805503845, 0.8518518805503845, 0.8888888955116272, 0.8730158805847168, 0.8730158805847168, 0.8730158805847168, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tsoft_f1_loss: [0.7130717635154724, 0.6528613567352295, 0.8104935884475708, 0.7413264513015747, 0.6875858902931213, 0.5871490836143494, 0.5995500087738037, 0.5740637183189392, 0.4507583975791931, 0.4259982109069824, 0.5064297318458557, 0.21225036680698395, 0.20403596758842468, 0.28925079107284546, 0.20582309365272522, 0.14583376049995422, 0.2475787252187729, 0.22587667405605316, 0.15661782026290894, 0.1419912725687027, 0.12495991587638855, 0.12830524146556854, 0.1315830945968628, 0.13035161793231964, 0.12490391731262207, 0.12724855542182922, 0.13199803233146667, 0.13034707307815552, 0.12309660017490387, 0.1175546944141388, 0.11534847319126129, 0.11530445516109467, 0.11571526527404785, 0.11531265825033188, 0.11431712657213211, 0.11344349384307861, 0.1128656193614006, 0.1124868094921112, 0.11222603917121887, 0.1120390072464943, 0.1119055449962616, 0.11180775612592697, 0.11173110455274582, 0.11166778206825256, 0.11161118000745773, 0.11155880987644196, 0.11151055246591568, 0.11146720498800278, 0.11142972856760025, 0.11139789968729019, 0.111371248960495, 0.1113491952419281, 0.11133120954036713, 0.11131671816110611, 0.11130491644144058, 0.11129530519247055, 0.11128740757703781, 0.11128083616495132, 0.11127528548240662, 0.11127037554979324, 0.11126601696014404, 0.11126204580068588, 0.11125835031270981, 0.11125501990318298, 0.1112518310546875, 0.11124876141548157, 0.1112457662820816, 0.11124287545681, 0.11124009639024734, 0.11123742163181305, 0.11123482137918472, 0.11123238503932953, 0.1112300455570221, 0.11122780293226242, 0.1112256869673729, 0.11122369766235352, 0.11122175306081772, 0.11121993511915207, 0.11121825128793716, 0.11121662706136703, 0.11121506243944168, 0.11121363192796707, 0.11121225357055664, 0.1112108901143074, 0.11120960116386414, 0.11120833456516266, 0.11120711266994476, 0.11120589822530746, 0.11120472848415375, 0.11120358109474182, 0.11120244860649109, 0.11120139062404633, 0.11120034754276276, 0.11119934916496277, 0.11119834333658218, 0.11119738966226578, 0.11119646579027176, 0.11119554936885834, 0.11119464784860611, 0.11119379103183746, 0.1111929714679718, 0.11119214445352554, 0.11119135469198227, 0.11119059473276138, 0.1111898273229599, 0.1111890971660614, 0.1111883819103241, 0.11118768155574799, 0.11118701100349426, 0.11118634790182114, 0.11118568480014801, 0.11118504405021667, 0.11118441075086594, 0.11118379235267639, 0.11118319630622864, 0.11118263751268387, 0.11118205636739731, 0.11118148267269135, 0.11118091642856598, 0.11118035763502121, 0.11117984354496002, 0.11117929965257645, 0.11117878556251526, 0.11117829382419586, 0.11117778718471527, 0.11117728799581528, 0.11117683351039886, 0.11117635667324066, 0.11117587983608246, 0.11117543280124664, 0.11117495596408844, 0.11117450147867203, 0.1111740693449974, 0.11117364466190338, 0.11117321997880936, 0.11117279529571533, 0.1111724004149437, 0.11117201298475266, 0.11117160320281982, 0.1111711785197258, 0.11117079854011536, 0.11117039620876312, 0.11117005348205566, 0.11116968095302582, 0.11116930097341537, 0.11116896569728851, 0.11116860806941986, 0.11116825044155121, 0.11116790771484375, 0.11116756498813629, 0.11116722226142883, 0.11116688698530197, 0.1111665591597557, 0.11116623878479004, 0.11116591095924377, 0.1111656054854393, 0.11116530001163483, 0.11116499453783035, 0.11116467416286469, 0.1111643835902214, 0.11116407811641693, 0.11116377264261246, 0.11116348206996918, 0.1111631914973259, 0.11116290092468262, 0.11116262525320053, 0.11116237193346024, 0.11116209626197815, 0.11116182804107666, 0.11116155982017517, 0.11116131395101547, 0.11116104573011398, 0.11116078495979309, 0.11116054654121399, 0.1111602932214737, 0.11116006970405579, 0.1111598014831543, 0.1111595630645752, 0.1111593097448349, 0.11115908622741699, 0.11115886270999908, 0.11115863919258118, 0.11115840822458267, 0.11115818470716476, 0.11115794628858566, 0.11115776002407074, 0.11115750670433044, 0.11115729808807373, 0.11115707457065582, 0.11115685850381851, 0.1111566498875618, 0.11115643382072449, 0.11115622520446777, 0.11115601658821106, 0.11115580052137375, 0.11115561425685883, 0.1111554205417633, 0.11115521937608719, 0.11115503311157227, 0.11115486174821854, 0.11115463823080063, 0.1111544668674469, 0.11115426570177078, 0.11115409433841705, 0.11115389317274094, 0.11115370690822601, 0.11115352064371109, 0.11115333437919617, 0.11115316301584244, 0.11115296930074692, 0.11115280538797379, 0.11115261912345886, 0.11115246266126633, 0.11115230619907379, 0.11115210503339767, 0.11115196347236633, 0.111151784658432, 0.11115161329507828, 0.11115148663520813, 0.1111513003706932, 0.11115111410617828, 0.11115095019340515, 0.11115079373121262, 0.11115063726902008, 0.11115047335624695, 0.1111503317952156, 0.11115018278360367, 0.11115001142024994, 0.1111498773097992, 0.11114970594644547, 0.11114957928657532, 0.11114941537380219, 0.11114928126335144, 0.1111491397023201, 0.11114899069070816, 0.11114886403083801, 0.11114872992038727, 0.11114856600761414, 0.1111484095454216, 0.11114829033613205, 0.11114814877510071, 0.11114801466464996, 0.11114788055419922, 0.11114772409200668, 0.11114758998155594, 0.1111474558711052, 0.11114733666181564, 0.1111472025513649, 0.11114706099033356, 0.1111469566822052, 0.11114682257175446, 0.11114668846130371, 0.11114658415317535, 0.11114643514156342, 0.11114631593227386, 0.11114619672298431, 0.11114606261253357, 0.11114595830440521, 0.11114581674337387, 0.11114568263292313, 0.11114557832479477, 0.11114546656608582, 0.11114536225795746, 0.11114522069692612, 0.11114510148763657, 0.11114498972892761, 0.11114488542079926, 0.1111447811126709, 0.11114464700222015, 0.1111445277929306, 0.11114443838596344, 0.11114433407783508, 0.11114421486854553, 0.11114411801099777, 0.11114401370286942, 0.11114390939474106, 0.1111437976360321, 0.11114369332790375, 0.11114358901977539, 0.11114345490932465, 0.11114335060119629, 0.11114326119422913, 0.11114316433668137, 0.11114306002855301, 0.11114292591810226, 0.11114285886287689, 0.11114273965358734, 0.11114263534545898, 0.11114254593849182, 0.11114244908094406, 0.1111423596739769, 0.11114229261875153, 0.11114215850830078, 0.11114207655191422, 0.11114197224378586, 0.1111418679356575, 0.11114181578159332, 0.11114171147346497, 0.11114159971475601, 0.11114149540662766]\n",
      "@_@\t\tAUC: [0.556679904460907, 0.5754629969596863, 0.5680555105209351, 0.5763888955116272, 0.6875, 0.7175925374031067, 0.7222222089767456, 0.7527778148651123, 0.7777777910232544, 0.7777777910232544, 0.7916666865348816, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tglobal_accuracy: [0.6111111044883728, 0.5694444179534912, 0.7083333134651184, 0.75, 0.7083333134651184, 0.7777777910232544, 0.8055555820465088, 0.8194444179534912, 0.8611111044883728, 0.9166666865348816, 0.8194444179534912, 0.9583333134651184, 0.9722222089767456, 0.8611111044883728, 0.9722222089767456, 1.0, 0.9166666865348816, 0.9583333134651184, 0.9861111044883728, 0.9861111044883728, 1.0, 0.9861111044883728, 0.9861111044883728, 0.9861111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tglobal_precision: [0.3333333432674408, 0.375, 0.5, 0.6000000238418579, 0.5, 0.6086956262588501, 0.6842105388641357, 0.7222222089767456, 0.7894737124443054, 1.0, 0.699999988079071, 0.875, 0.9130434989929199, 0.6774193644523621, 0.9130434989929199, 1.0, 0.9411764740943909, 1.0, 1.0, 1.0, 1.0, 0.9545454382896423, 0.9545454382896423, 0.9545454382896423, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tglobal_recall: [0.3333333432674408, 0.7142857313156128, 0.2857142984867096, 0.4285714328289032, 0.4761904776096344, 0.6666666865348816, 0.6190476417541504, 0.6190476417541504, 0.7142857313156128, 0.7142857313156128, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.761904776096344, 0.8571428656578064, 0.9523809552192688, 0.9523809552192688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_loss: [4.927338123321533, 1.5485234260559082, 2.6246085166931152, 2.1126136779785156, 1.6619961261749268, 1.237125277519226, 1.226235270500183, 0.8262660503387451, 0.5803829431533813, 0.618051290512085, 0.13131912052631378, 0.07618240267038345, 0.3815990686416626, 0.0792226493358612, 0.02496464177966118, 0.11452175676822662, 0.09013983607292175, 0.03326629847288132, 0.020556043833494186, 0.010247365571558475, 0.01986481249332428, 0.029965732246637344, 0.020963303744792938, 0.007945263758301735, 0.006104134954512119, 0.007288041990250349, 0.007280238904058933, 0.005388573743402958, 0.0036568548530340195, 0.0031288056634366512, 0.0037944165524095297, 0.004503883421421051, 0.0040035611018538475, 0.002829638309776783, 0.0019108715932816267, 0.0013662537094205618, 0.0010455825831741095, 0.0008452415931969881, 0.0007147400756366551, 0.0006262502865865827, 0.0005626300117000937, 0.0005116054089739919, 0.00046523171477019787, 0.0004220682312734425, 0.0003804489388130605, 0.00034104223595932126, 0.00030497348052449524, 0.0002733748988248408, 0.0002463167766109109, 0.00022361119044944644, 0.00020484543347265571, 0.000189557031262666, 0.00017729547107592225, 0.00016739746206440032, 0.00015939120203256607, 0.0001529090804979205, 0.00014757944154553115, 0.0001430710544809699, 0.00013915717136114836, 0.0001356619322905317, 0.0001324562035733834, 0.00012944724585395306, 0.00012672120647039264, 0.00012408438487909734, 0.00012150073598604649, 0.00011897183139808476, 0.00011650348460534588, 0.0001141047541750595, 0.00011178130080224946, 0.00010954309982480481, 0.0001073972016456537, 0.00010535956243984401, 0.00010341544839320704, 0.00010156258940696716, 9.979748574551195e-05, 9.811524796532467e-05, 9.652777225710452e-05, 9.503597539151087e-05, 9.359548857901245e-05, 9.223518281942233e-05, 9.097106521949172e-05, 8.974601951194927e-05, 8.855493069859222e-05, 8.739496115595102e-05, 8.626332419225946e-05, 8.515685476595536e-05, 8.40751308714971e-05, 8.301623165607452e-05, 8.197967690648511e-05, 8.097430691123009e-05, 8.000979869393632e-05, 7.906807877589017e-05, 7.814948912709951e-05, 7.725351315457374e-05, 7.637929229531437e-05, 7.552803435828537e-05, 7.46980877011083e-05, 7.389022357529029e-05, 7.310316868824884e-05, 7.233666838146746e-05, 7.158979133237153e-05, 7.086251571308821e-05, 7.015336450422183e-05, 6.946329085621983e-05, 6.87906431267038e-05, 6.813398795202374e-05, 6.74919574521482e-05, 6.686439155600965e-05, 6.625080277444795e-05, 6.565013609360904e-05, 6.506170029751956e-05, 6.448512431234121e-05, 6.391988426912576e-05, 6.336583464872092e-05, 6.28220004728064e-05, 6.22896768618375e-05, 6.17674522800371e-05, 6.125417712610215e-05, 6.075009150663391e-05, 6.025478069204837e-05, 5.976774627924897e-05, 5.928908285568468e-05, 5.881818651687354e-05, 5.835531192133203e-05, 5.789987699245103e-05, 5.7452209148323163e-05, 5.701216286979616e-05, 5.6581266107968986e-05, 5.615816917270422e-05, 5.573855742113665e-05, 5.5322536354651675e-05, 5.491922274813987e-05, 5.452532786875963e-05, 5.4131829529069364e-05, 5.374079046305269e-05, 5.336043250281364e-05, 5.2985178626840934e-05, 5.2614697779063135e-05, 5.224987035035156e-05, 5.189271905692294e-05, 5.1539998821681365e-05, 5.1191749662393704e-05, 5.0848437240347266e-05, 5.050981417298317e-05, 5.0178045057691634e-05, 4.9848207709146664e-05, 4.952220479026437e-05, 4.920219362247735e-05, 4.888610419584438e-05, 4.8573900130577385e-05, 4.8265079385600984e-05, 4.795997665496543e-05, 4.765998892253265e-05, 4.7362442273879424e-05, 4.706784966401756e-05, 4.6779859985690564e-05, 4.6492950787069276e-05, 4.6209308493416756e-05, 4.5930195483379066e-05, 4.5654349378310144e-05, 4.5381464587990195e-05, 4.511133738560602e-05, 4.4843913201475516e-05, 4.457943214219995e-05, 4.4319236621959135e-05, 4.406810330692679e-05, 4.381909820949659e-05, 4.357357829576358e-05, 4.3331048800610006e-05, 4.3091051338706166e-05, 4.2853680497501045e-05, 4.261848152964376e-05, 4.2385567212477326e-05, 4.2154952097916976e-05, 4.19267344113905e-05, 4.170215834164992e-05, 4.147928848396987e-05, 4.125794657738879e-05, 4.104026811546646e-05, 4.082435043528676e-05, 4.0610651922179386e-05, 4.039872146677226e-05, 4.018873369204812e-05, 3.9980597648536786e-05, 3.977690721512772e-05, 3.9573082176502794e-05, 3.937003202736378e-05, 3.917090725735761e-05, 3.8973761547822505e-05, 3.877801646012813e-05, 3.858404670609161e-05, 3.8391932321246713e-05, 3.8202630094019696e-05, 3.801400089287199e-05, 3.782739440794103e-05, 3.764314897125587e-05, 3.746038055396639e-05, 3.727906005224213e-05, 3.709904558490962e-05, 3.692086465889588e-05, 3.674397885333747e-05, 3.6570945667335764e-05, 3.639784699771553e-05, 3.622496296884492e-05, 3.6055447708349675e-05, 3.5887289413949475e-05, 3.572028435883112e-05, 3.555455623427406e-05, 3.5391894925851375e-05, 3.522901170072146e-05, 3.5067525459453464e-05, 3.490845119813457e-05, 3.475042831269093e-05, 3.4593656891956925e-05, 3.4437871363479644e-05, 3.428337367950007e-05, 3.413101512705907e-05, 3.3979427826125175e-05, 3.382904105819762e-05, 3.368034231243655e-05, 3.353278952999972e-05, 3.338619717396796e-05, 3.324062345200218e-05, 3.309698513476178e-05, 3.2953950722003356e-05, 3.281230965512805e-05, 3.267233114456758e-05, 3.253300747019239e-05, 3.23947133438196e-05, 3.225737964385189e-05, 3.212109731975943e-05, 3.198628473910503e-05, 3.185265086358413e-05, 3.172010474372655e-05, 3.15885990858078e-05, 3.145801747450605e-05, 3.132829806418158e-05, 3.120044493698515e-05, 3.1072660931386054e-05, 3.09458228002768e-05, 3.08204980683513e-05, 3.069603553740308e-05, 3.057244248338975e-05, 3.0449351470451802e-05, 3.0328370485221967e-05, 3.0207709642127156e-05, 3.008785643032752e-05, 2.996980219904799e-05, 2.9852133593522012e-05, 2.97351652989164e-05, 2.9618950065923855e-05, 2.9503407859010622e-05, 2.9388709663180634e-05, 2.9275961423991248e-05, 2.9163074941607192e-05, 2.905139081121888e-05, 2.8940894480911084e-05, 2.8830490919062868e-05, 2.872120967367664e-05, 2.8612679670914076e-05, 2.850487726391293e-05, 2.8398004360496998e-05, 2.8291578928474337e-05, 2.8186641429783776e-05, 2.808169119816739e-05, 2.7977535864920355e-05, 2.7874435545527376e-05, 2.7772173780249432e-05, 2.7670352210407145e-05, 2.7568949008127674e-05, 2.7469402994029224e-05, 2.7369636882212944e-05, 2.7270074497209862e-05, 2.7172018235432915e-05, 2.7074805984739214e-05, 2.697805393836461e-05, 2.6881762096309103e-05, 2.6785895897774026e-05, 2.6690540835261345e-05, 2.659580968611408e-05, 2.6502437322051264e-05, 2.6409266865812242e-05, 2.631687675602734e-05, 2.6224945031572133e-05, 2.613423566799611e-05, 2.604380642878823e-05, 2.5953542717616074e-05, 2.5864312192425132e-05, 2.5775458198040724e-05, 2.5687080778880045e-05, 2.5599856599001214e-05, 2.5512777938274667e-05, 2.5425953936064616e-05, 2.5340119464090094e-05, 2.5254772481275722e-05, 2.5170069420710206e-05, 2.5085744709940627e-05, 2.50023695116397e-05, 2.4919252609834075e-05, 2.483664866304025e-05]\n",
      "@_@\t\tval_macro_f1_score: [0.36049383878707886, 0.13636364042758942, 0.25505051016807556, 0.2759259343147278, 0.4282507598400116, 0.42592594027519226, 0.4828282594680786, 0.5693121552467346, 0.6000000238418579, 0.45264551043510437, 0.8585858941078186, 0.8444444537162781, 0.7285714745521545, 0.82962965965271, 0.8888888955116272, 0.6962962746620178, 0.7777777910232544, 0.8518518805503845, 0.8518518805503845, 0.8888888955116272, 0.8730158805847168, 0.8730158805847168, 0.8730158805847168, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tval_soft_f1_loss: [0.6528613567352295, 0.8104935884475708, 0.7413264513015747, 0.6875858902931213, 0.5871490836143494, 0.5995500087738037, 0.5740637183189392, 0.4507583975791931, 0.4259982109069824, 0.5064297318458557, 0.21225036680698395, 0.20403596758842468, 0.28925079107284546, 0.20582309365272522, 0.14583376049995422, 0.2475787252187729, 0.22587667405605316, 0.15661782026290894, 0.1419912725687027, 0.12495990842580795, 0.12830522656440735, 0.1315830945968628, 0.13035161793231964, 0.12490391731262207, 0.12724855542182922, 0.13199803233146667, 0.13034707307815552, 0.12309661507606506, 0.1175546944141388, 0.11534847319126129, 0.11530445516109467, 0.11571526527404785, 0.11531265825033188, 0.11431712657213211, 0.11344349384307861, 0.1128656342625618, 0.1124868094921112, 0.11222603917121887, 0.1120390072464943, 0.1119055449962616, 0.11180774122476578, 0.11173110455274582, 0.11166778206825256, 0.11161118000745773, 0.11155880987644196, 0.11151055246591568, 0.11146720498800278, 0.11142972856760025, 0.11139789968729019, 0.111371248960495, 0.1113491952419281, 0.11133120954036713, 0.11131671816110611, 0.11130491644144058, 0.11129530519247055, 0.11128740757703781, 0.11128083616495132, 0.11127528548240662, 0.11127037554979324, 0.11126603186130524, 0.11126204580068588, 0.11125833541154861, 0.11125503480434418, 0.1112518310546875, 0.11124876141548157, 0.1112457662820816, 0.11124289035797119, 0.11124009639024734, 0.11123742163181305, 0.11123481392860413, 0.11123238503932953, 0.1112300306558609, 0.11122780293226242, 0.1112256869673729, 0.11122369766235352, 0.11122176051139832, 0.11121993511915207, 0.11121825128793716, 0.11121662706136703, 0.11121506243944168, 0.11121363192796707, 0.11121225357055664, 0.1112108901143074, 0.11120960116386414, 0.11120833456516266, 0.11120711266994476, 0.11120589822530746, 0.11120472848415375, 0.11120358109474182, 0.11120244860649109, 0.11120139062404633, 0.11120034754276276, 0.11119934916496277, 0.11119834333658218, 0.11119738966226578, 0.11119646579027176, 0.11119554936885834, 0.11119464784860611, 0.11119379103183746, 0.1111929714679718, 0.11119214445352554, 0.11119135469198227, 0.11119059473276138, 0.1111898273229599, 0.1111890971660614, 0.1111883819103241, 0.11118768155574799, 0.11118701100349426, 0.11118634790182114, 0.1111856997013092, 0.11118504405021667, 0.11118441075086594, 0.11118379235267639, 0.11118319630622864, 0.11118263751268387, 0.11118205636739731, 0.11118148267269135, 0.11118091642856598, 0.11118035763502121, 0.11117984354496002, 0.11117929965257645, 0.11117878556251526, 0.11117829382419586, 0.11117778718471527, 0.11117728799581528, 0.11117683351039886, 0.11117635667324066, 0.11117587983608246, 0.11117543280124664, 0.11117495596408844, 0.11117450147867203, 0.1111740693449974, 0.11117364466190338, 0.11117321997880936, 0.11117279529571533, 0.1111724004149437, 0.11117201298475266, 0.11117160320281982, 0.1111711785197258, 0.11117079854011536, 0.11117039620876312, 0.11117005348205566, 0.11116968095302582, 0.11116930097341537, 0.11116896569728851, 0.11116862297058105, 0.11116825044155121, 0.11116792261600494, 0.11116756498813629, 0.11116722226142883, 0.11116688698530197, 0.1111665591597557, 0.11116623878479004, 0.11116591095924377, 0.1111656054854393, 0.11116530001163483, 0.11116499453783035, 0.11116467416286469, 0.1111643835902214, 0.11116407811641693, 0.11116378754377365, 0.11116348206996918, 0.1111631914973259, 0.11116290092468262, 0.11116262525320053, 0.11116237193346024, 0.11116209626197815, 0.11116182804107666, 0.11116155982017517, 0.11116131395101547, 0.11116104573011398, 0.11116078495979309, 0.11116054654121399, 0.1111602932214737, 0.11116006970405579, 0.1111598014831543, 0.1111595630645752, 0.1111593097448349, 0.11115908622741699, 0.11115886270999908, 0.11115863919258118, 0.11115840077400208, 0.11115818470716476, 0.11115794628858566, 0.11115776002407074, 0.11115750670433044, 0.11115729808807373, 0.11115707457065582, 0.11115685850381851, 0.1111566498875618, 0.11115643382072449, 0.11115622520446777, 0.11115601658821106, 0.11115581542253494, 0.11115561425685883, 0.1111554205417633, 0.11115521937608719, 0.11115503311157227, 0.11115486174821854, 0.11115463823080063, 0.1111544668674469, 0.11115426570177078, 0.11115409433841705, 0.11115389317274094, 0.11115370690822601, 0.11115352064371109, 0.11115333437919617, 0.11115316301584244, 0.11115296930074692, 0.11115280538797379, 0.11115261912345886, 0.11115246266126633, 0.11115230619907379, 0.11115210503339767, 0.11115196347236633, 0.111151784658432, 0.11115161329507828, 0.11115148663520813, 0.1111513003706932, 0.11115111410617828, 0.11115095019340515, 0.11115079373121262, 0.11115063726902008, 0.11115047335624695, 0.1111503317952156, 0.11115018278360367, 0.11115002632141113, 0.1111498773097992, 0.11114970594644547, 0.11114957928657532, 0.11114941537380219, 0.11114928126335144, 0.1111491397023201, 0.11114899069070816, 0.11114886403083801, 0.11114872992038727, 0.11114856600761414, 0.1111484095454216, 0.11114829033613205, 0.11114814877510071, 0.11114801466464996, 0.11114788055419922, 0.11114772409200668, 0.11114758998155594, 0.1111474558711052, 0.11114733666181564, 0.1111472025513649, 0.11114706099033356, 0.1111469566822052, 0.11114682257175446, 0.11114668846130371, 0.11114656925201416, 0.11114643514156342, 0.11114631593227386, 0.11114619672298431, 0.11114606261253357, 0.11114595830440521, 0.11114581674337387, 0.11114568263292313, 0.11114557832479477, 0.11114546656608582, 0.11114534735679626, 0.11114522069692612, 0.11114510148763657, 0.11114498972892761, 0.11114488542079926, 0.1111447811126709, 0.11114464700222015, 0.1111445277929306, 0.11114443838596344, 0.11114433407783508, 0.11114421486854553, 0.11114411801099777, 0.11114399880170822, 0.11114390939474106, 0.1111437976360321, 0.11114369332790375, 0.11114358901977539, 0.11114345490932465, 0.11114336550235748, 0.11114326119422913, 0.11114316433668137, 0.11114306002855301, 0.11114292591810226, 0.11114285886287689, 0.11114273965358734, 0.11114263534545898, 0.11114254593849182, 0.11114244908094406, 0.1111423596739769, 0.11114229261875153, 0.11114215850830078, 0.11114209145307541, 0.11114197224378586, 0.1111418679356575, 0.11114181578159332, 0.11114171147346497, 0.11114159971475601, 0.11114149540662766, 0.11114141345024109]\n",
      "@_@\t\tval_AUC: [0.5754629969596863, 0.5680555105209351, 0.5763888955116272, 0.6875, 0.7175925374031067, 0.7222222089767456, 0.7527778148651123, 0.7777777910232544, 0.7777777910232544, 0.7916666865348816, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272]\n",
      "@_@\t\tval_global_accuracy: [0.5694444179534912, 0.7083333134651184, 0.75, 0.7083333134651184, 0.7777777910232544, 0.8055555820465088, 0.8194444179534912, 0.8611111044883728, 0.9166666865348816, 0.8194444179534912, 0.9583333134651184, 0.9722222089767456, 0.8611111044883728, 0.9722222089767456, 1.0, 0.9166666865348816, 0.9583333134651184, 0.9861111044883728, 0.9861111044883728, 1.0, 0.9861111044883728, 0.9861111044883728, 0.9861111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_global_precision: [0.375, 0.5, 0.6000000238418579, 0.5, 0.6086956262588501, 0.6842105388641357, 0.7222222089767456, 0.7894737124443054, 1.0, 0.699999988079071, 0.875, 0.9130434989929199, 0.6774193644523621, 0.9130434989929199, 1.0, 0.9411764740943909, 1.0, 1.0, 1.0, 1.0, 0.9545454382896423, 0.9545454382896423, 0.9545454382896423, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\t\tval_global_recall: [0.7142857313156128, 0.2857142984867096, 0.4285714328289032, 0.4761904776096344, 0.6666666865348816, 0.6190476417541504, 0.6190476417541504, 0.7142857313156128, 0.7142857313156128, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.761904776096344, 0.8571428656578064, 0.9523809552192688, 0.9523809552192688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1926/0_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.0000\n",
      "@_@\tValidation Macro F1-score: 0.8889\n",
      "W0402 19:33:18.193797 125225695786816 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1926/0_unfrozen_block/model/saved_model/assets\n",
      "I0402 19:33:22.984871 125225695786816 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1926/0_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1926/0_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Evaluate ------------------\n",
      "1/1 [==============================] - 1s 669ms/step\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-transfer-miniBatchTest_2024-04-02_1926/predict_sample.png\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "@_@\tResult of testing on 1 batches:\n",
      "@_@\t\tbinary_crossentropy: 2.476617919455748e-05\n",
      "@_@\t\tAUC: 0.8888888955116272\n",
      "@_@\t\tmacro_f1_score: 0.8888888955116272\n",
      "@_@\t\tsoft_f1_loss: 0.11114141345024109\n",
      "@_@\t\tglobal_accuracy: 1.0\n",
      "@_@\t\tglobal_precision: 1.0\n",
      "@_@\t\tglobal_recall: 1.0\n",
      "@_@\t\n",
      "@_@\tDONE!\n"
     ]
    }
   ],
   "source": [
    "!python ../neural_nets/resnet50_test.py \\\n",
    "  --ouput_name=resnet50-transfer-miniBatchTest \\\n",
    "  --image_size=224 --epochs=100 --batch_size=8 --train_size=8 \\\n",
    "  --mode=train_then_eval --min_unfreeze_blocks=0 --max_unfreeze_blocks=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cwd to '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation'\n",
      "['/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode/neural_nets', '/home/payam/miniconda3/envs/tf2-gpu/lib/python39.zip', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/lib-dynload', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages', '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode']\n",
      "/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation\n",
      "2024-04-05 12:34:16.865924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 12:34:17.384693: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-05 12:34:17.384799: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-05 12:34:17.384807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-05 12:34:18.448808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.483484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.483703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.484075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 12:34:18.485112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.485297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.485468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.921383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.921583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.921718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 12:34:18.921823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "@_@\t\n",
      "@_@\t------------------ Configuration ------------------\n",
      "@_@\tStart: 2024-04-05_1234\n",
      "@_@\t\n",
      "@_@\tMode: train_then_eval\n",
      "@_@\tUnfreeze blocks: start 0, end 3\n",
      "@_@\tContinue from checkpoint: False\n",
      "@_@\t\n",
      "@_@\tDataset: MIMIC-CXR\n",
      "@_@\tTraining Dataset Size: 256\n",
      "@_@\tBatch size: 64\n",
      "@_@\tImage size: (224, 224)\n",
      "@_@\tMax Epochs per training round: 2\n",
      "@_@\tDefault Learning Rate: 0.0001\n",
      "@_@\t\n",
      "@_@\tGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "@_@\tCPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "@_@\t\n",
      "@_@\t------------------ Data ------------------\n",
      "@_@\tnum_classes: 9\n",
      "@_@\tclass_names: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Pleural Effusion'\n",
      "@_@\t 'Pneumonia' 'Pneumothorax' 'Fracture' 'Support Devices']\n",
      "@_@\tall_data_size: 377110\n",
      "@_@\tall_data_filtered_size: 112134\n",
      "@_@\tsplit_size: {'train': 256, 'validate': 889, 'test': 512}\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0405 12:35:05.678973 125908164081472 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0405 12:35:05.785560 125908164081472 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0405 12:35:06.563144 125908164081472 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0405 12:35:06.566850 125908164081472 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "@_@\tShape of image batch: [64, 224, 224, 3]\n",
      "@_@\tShape of labels batch: [64, 9]\n",
      "Model: \"my_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " my_avg_pool (GlobalAveragePool  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " my_fc_1 (Dense)                (None, 256)          524544      ['my_avg_pool[0][0]']            \n",
      "                                                                                                  \n",
      " my_fc_2 (Dense)                (None, 128)          32896       ['my_fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " my_output (Dense)              (None, 9)            1161        ['my_fc_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,146,313\n",
      "Trainable params: 558,601\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "2024-04-05 12:35:19.296413: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-04-05 12:35:19.296432: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2024-04-05 12:35:19.296464: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1664] Profiler found 1 GPUs\n",
      "2024-04-05 12:35:19.402005: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-04-05 12:35:19.403247: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "@_@\tUnfreezing 0 blocks...\n",
      "@_@\tTotal trainable weights: 6\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 0.0001\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0405 12:35:20.405199 125908164081472 deprecation.py:554] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2024-04-05 12:35:32.097206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2024-04-05 12:35:33.089312: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7281781853b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-05 12:35:33.089357: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-04-05 12:35:33.194970: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-05 12:35:33.317797: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6910 - macro_f1_score: 0.2652 - soft_f1_loss: 0.7041 - AUC: 0.4699 - global_accuracy: 0.5642 - global_precision: 0.3130 - global_recall: 0.6426\n",
      "Epoch 1: val_loss improved from inf to 0.54668, saving model to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/model/checkpoints/epoch-01_valloss-0.5467.ckpt\n",
      "4/4 [==============================] - 61s 16s/step - loss: 0.6910 - macro_f1_score: 0.2652 - soft_f1_loss: 0.7041 - AUC: 0.4699 - global_accuracy: 0.5642 - global_precision: 0.3130 - global_recall: 0.6426 - val_loss: 0.5467 - val_macro_f1_score: 0.2005 - val_soft_f1_loss: 0.7393 - val_AUC: 0.4757 - val_global_accuracy: 0.7137 - val_global_precision: 0.3958 - val_global_recall: 0.4639\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5471 - macro_f1_score: 0.1932 - soft_f1_loss: 0.7350 - AUC: 0.4924 - global_accuracy: 0.7201 - global_precision: 0.4074 - global_recall: 0.3651\n",
      "Epoch 2: val_loss improved from 0.54668 to 0.48034, saving model to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/model/checkpoints/epoch-02_valloss-0.4803.ckpt\n",
      "4/4 [==============================] - 54s 15s/step - loss: 0.5471 - macro_f1_score: 0.1932 - soft_f1_loss: 0.7350 - AUC: 0.4924 - global_accuracy: 0.7201 - global_precision: 0.4074 - global_recall: 0.3651 - val_loss: 0.4803 - val_macro_f1_score: 0.0948 - val_soft_f1_loss: 0.7706 - val_AUC: 0.4837 - val_global_accuracy: 0.7506 - val_global_precision: 0.3852 - val_global_recall: 0.1452\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:1m:55s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [0.6910080313682556, 0.5470949411392212]\n",
      "@_@\t\tmacro_f1_score: [0.2652295231819153, 0.19324560463428497]\n",
      "@_@\t\tsoft_f1_loss: [0.7040703892707825, 0.7349882125854492]\n",
      "@_@\t\tAUC: [0.469889760017395, 0.49238407611846924]\n",
      "@_@\t\tglobal_accuracy: [0.5642361044883728, 0.7200520634651184]\n",
      "@_@\t\tglobal_precision: [0.3129581809043884, 0.407448947429657]\n",
      "@_@\t\tglobal_recall: [0.6425737142562866, 0.3650975227355957]\n",
      "@_@\t\tval_loss: [0.5466781854629517, 0.48033568263053894]\n",
      "@_@\t\tval_macro_f1_score: [0.20051789283752441, 0.09475958347320557]\n",
      "@_@\t\tval_soft_f1_loss: [0.7393109202384949, 0.7705996632575989]\n",
      "@_@\t\tval_AUC: [0.47572293877601624, 0.48367390036582947]\n",
      "@_@\t\tval_global_accuracy: [0.7137048840522766, 0.7506070137023926]\n",
      "@_@\t\tval_global_precision: [0.39577430486679077, 0.38519158959388733]\n",
      "@_@\t\tval_global_recall: [0.46388697624206543, 0.14516839385032654]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.4803\n",
      "@_@\tValidation Macro F1-score: 0.0948\n",
      "W0405 12:37:23.495937 125908164081472 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/model/saved_model/assets\n",
      "I0405 12:37:28.198837 125908164081472 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "2024-04-05 12:37:29.009611: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-04-05 12:37:29.009630: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2024-04-05 12:37:29.167862: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-04-05 12:37:29.173996: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "@_@\tUnfreezing 1 blocks...\n",
      "@_@\tTotal trainable weights: 46\n",
      "@_@\t\tconv5_block1_1_conv/kernel:0\n",
      "@_@\t\tconv5_block1_1_conv/bias:0\n",
      "@_@\t\tconv5_block1_1_bn/gamma:0\n",
      "@_@\t\tconv5_block1_1_bn/beta:0\n",
      "@_@\t\tconv5_block1_2_conv/kernel:0\n",
      "@_@\t\tconv5_block1_2_conv/bias:0\n",
      "@_@\t\tconv5_block1_2_bn/gamma:0\n",
      "@_@\t\tconv5_block1_2_bn/beta:0\n",
      "@_@\t\tconv5_block1_0_conv/kernel:0\n",
      "@_@\t\tconv5_block1_0_conv/bias:0\n",
      "@_@\t\tconv5_block1_3_conv/kernel:0\n",
      "@_@\t\tconv5_block1_3_conv/bias:0\n",
      "@_@\t\tconv5_block1_0_bn/gamma:0\n",
      "@_@\t\tconv5_block1_0_bn/beta:0\n",
      "@_@\t\tconv5_block1_3_bn/gamma:0\n",
      "@_@\t\tconv5_block1_3_bn/beta:0\n",
      "@_@\t\tconv5_block2_1_conv/kernel:0\n",
      "@_@\t\tconv5_block2_1_conv/bias:0\n",
      "@_@\t\tconv5_block2_1_bn/gamma:0\n",
      "@_@\t\tconv5_block2_1_bn/beta:0\n",
      "@_@\t\tconv5_block2_2_conv/kernel:0\n",
      "@_@\t\tconv5_block2_2_conv/bias:0\n",
      "@_@\t\tconv5_block2_2_bn/gamma:0\n",
      "@_@\t\tconv5_block2_2_bn/beta:0\n",
      "@_@\t\tconv5_block2_3_conv/kernel:0\n",
      "@_@\t\tconv5_block2_3_conv/bias:0\n",
      "@_@\t\tconv5_block2_3_bn/gamma:0\n",
      "@_@\t\tconv5_block2_3_bn/beta:0\n",
      "@_@\t\tconv5_block3_1_conv/kernel:0\n",
      "@_@\t\tconv5_block3_1_conv/bias:0\n",
      "@_@\t\tconv5_block3_1_bn/gamma:0\n",
      "@_@\t\tconv5_block3_1_bn/beta:0\n",
      "@_@\t\tconv5_block3_2_conv/kernel:0\n",
      "@_@\t\tconv5_block3_2_conv/bias:0\n",
      "@_@\t\tconv5_block3_2_bn/gamma:0\n",
      "@_@\t\tconv5_block3_2_bn/beta:0\n",
      "@_@\t\tconv5_block3_3_conv/kernel:0\n",
      "@_@\t\tconv5_block3_3_conv/bias:0\n",
      "@_@\t\tconv5_block3_3_bn/gamma:0\n",
      "@_@\t\tconv5_block3_3_bn/beta:0\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 1e-05\n",
      "@_@\tLoading weights from /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/0_unfrozen_block/model/checkpoints/epoch-02_valloss-0.4803.ckpt\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5400 - macro_f1_score: 0.1766 - soft_f1_loss: 0.7296 - AUC: 0.4996 - global_accuracy: 0.7374 - global_precision: 0.4300 - global_recall: 0.2932\n",
      "Epoch 1: val_loss improved from inf to 0.47197, saving model to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/1_unfrozen_block/model/checkpoints/epoch-01_valloss-0.4720.ckpt\n",
      "4/4 [==============================] - 63s 15s/step - loss: 0.5400 - macro_f1_score: 0.1766 - soft_f1_loss: 0.7296 - AUC: 0.4996 - global_accuracy: 0.7374 - global_precision: 0.4300 - global_recall: 0.2932 - val_loss: 0.4720 - val_macro_f1_score: 0.0746 - val_soft_f1_loss: 0.7944 - val_AUC: 0.4926 - val_global_accuracy: 0.7694 - val_global_precision: 0.4939 - val_global_recall: 0.1295\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4903 - macro_f1_score: 0.1494 - soft_f1_loss: 0.7411 - AUC: 0.5391 - global_accuracy: 0.7639 - global_precision: 0.5250 - global_recall: 0.2293\n",
      "Epoch 2: val_loss did not improve from 0.47197\n",
      "4/4 [==============================] - 51s 14s/step - loss: 0.4903 - macro_f1_score: 0.1494 - soft_f1_loss: 0.7411 - AUC: 0.5391 - global_accuracy: 0.7639 - global_precision: 0.5250 - global_recall: 0.2293 - val_loss: 0.4852 - val_macro_f1_score: 0.0589 - val_soft_f1_loss: 0.8122 - val_AUC: 0.5173 - val_global_accuracy: 0.7771 - val_global_precision: 0.5804 - val_global_recall: 0.1121\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:1m:53s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [0.5399935245513916, 0.49030590057373047]\n",
      "@_@\t\tmacro_f1_score: [0.1766071319580078, 0.14942273497581482]\n",
      "@_@\t\tsoft_f1_loss: [0.7295668125152588, 0.7410868406295776]\n",
      "@_@\t\tAUC: [0.4996473491191864, 0.539111852645874]\n",
      "@_@\t\tglobal_accuracy: [0.737413227558136, 0.7638888955116272]\n",
      "@_@\t\tglobal_precision: [0.42996829748153687, 0.5249965190887451]\n",
      "@_@\t\tglobal_recall: [0.29319214820861816, 0.22933194041252136]\n",
      "@_@\t\tval_loss: [0.471966952085495, 0.4852074086666107]\n",
      "@_@\t\tval_macro_f1_score: [0.07458788901567459, 0.0588805116713047]\n",
      "@_@\t\tval_soft_f1_loss: [0.7944476008415222, 0.8122177720069885]\n",
      "@_@\t\tval_AUC: [0.4925791919231415, 0.5172993540763855]\n",
      "@_@\t\tval_global_accuracy: [0.7694236040115356, 0.7771011590957642]\n",
      "@_@\t\tval_global_precision: [0.4939018785953522, 0.5803647637367249]\n",
      "@_@\t\tval_global_recall: [0.12951572239398956, 0.11214440315961838]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/1_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.4852\n",
      "@_@\tValidation Macro F1-score: 0.0589\n",
      "W0405 12:39:32.521670 125908164081472 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/1_unfrozen_block/model/saved_model/assets\n",
      "I0405 12:39:39.753253 125908164081472 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/1_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/1_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "2024-04-05 12:39:40.604703: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-04-05 12:39:40.604739: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2024-04-05 12:39:40.765694: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-04-05 12:39:40.779696: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "@_@\tUnfreezing 2 blocks...\n",
      "@_@\tTotal trainable weights: 122\n",
      "@_@\t\tconv4_block1_1_conv/kernel:0\n",
      "@_@\t\tconv4_block1_1_conv/bias:0\n",
      "@_@\t\tconv4_block1_1_bn/gamma:0\n",
      "@_@\t\tconv4_block1_1_bn/beta:0\n",
      "@_@\t\tconv4_block1_2_conv/kernel:0\n",
      "@_@\t\tconv4_block1_2_conv/bias:0\n",
      "@_@\t\tconv4_block1_2_bn/gamma:0\n",
      "@_@\t\tconv4_block1_2_bn/beta:0\n",
      "@_@\t\tconv4_block1_0_conv/kernel:0\n",
      "@_@\t\tconv4_block1_0_conv/bias:0\n",
      "@_@\t\tconv4_block1_3_conv/kernel:0\n",
      "@_@\t\tconv4_block1_3_conv/bias:0\n",
      "@_@\t\tconv4_block1_0_bn/gamma:0\n",
      "@_@\t\tconv4_block1_0_bn/beta:0\n",
      "@_@\t\tconv4_block1_3_bn/gamma:0\n",
      "@_@\t\tconv4_block1_3_bn/beta:0\n",
      "@_@\t\tconv4_block2_1_conv/kernel:0\n",
      "@_@\t\tconv4_block2_1_conv/bias:0\n",
      "@_@\t\tconv4_block2_1_bn/gamma:0\n",
      "@_@\t\tconv4_block2_1_bn/beta:0\n",
      "@_@\t\tconv4_block2_2_conv/kernel:0\n",
      "@_@\t\tconv4_block2_2_conv/bias:0\n",
      "@_@\t\tconv4_block2_2_bn/gamma:0\n",
      "@_@\t\tconv4_block2_2_bn/beta:0\n",
      "@_@\t\tconv4_block2_3_conv/kernel:0\n",
      "@_@\t\tconv4_block2_3_conv/bias:0\n",
      "@_@\t\tconv4_block2_3_bn/gamma:0\n",
      "@_@\t\tconv4_block2_3_bn/beta:0\n",
      "@_@\t\tconv4_block3_1_conv/kernel:0\n",
      "@_@\t\tconv4_block3_1_conv/bias:0\n",
      "@_@\t\tconv4_block3_1_bn/gamma:0\n",
      "@_@\t\tconv4_block3_1_bn/beta:0\n",
      "@_@\t\tconv4_block3_2_conv/kernel:0\n",
      "@_@\t\tconv4_block3_2_conv/bias:0\n",
      "@_@\t\tconv4_block3_2_bn/gamma:0\n",
      "@_@\t\tconv4_block3_2_bn/beta:0\n",
      "@_@\t\tconv4_block3_3_conv/kernel:0\n",
      "@_@\t\tconv4_block3_3_conv/bias:0\n",
      "@_@\t\tconv4_block3_3_bn/gamma:0\n",
      "@_@\t\tconv4_block3_3_bn/beta:0\n",
      "@_@\t\tconv4_block4_1_conv/kernel:0\n",
      "@_@\t\tconv4_block4_1_conv/bias:0\n",
      "@_@\t\tconv4_block4_1_bn/gamma:0\n",
      "@_@\t\tconv4_block4_1_bn/beta:0\n",
      "@_@\t\tconv4_block4_2_conv/kernel:0\n",
      "@_@\t\tconv4_block4_2_conv/bias:0\n",
      "@_@\t\tconv4_block4_2_bn/gamma:0\n",
      "@_@\t\tconv4_block4_2_bn/beta:0\n",
      "@_@\t\tconv4_block4_3_conv/kernel:0\n",
      "@_@\t\tconv4_block4_3_conv/bias:0\n",
      "@_@\t\tconv4_block4_3_bn/gamma:0\n",
      "@_@\t\tconv4_block4_3_bn/beta:0\n",
      "@_@\t\tconv4_block5_1_conv/kernel:0\n",
      "@_@\t\tconv4_block5_1_conv/bias:0\n",
      "@_@\t\tconv4_block5_1_bn/gamma:0\n",
      "@_@\t\tconv4_block5_1_bn/beta:0\n",
      "@_@\t\tconv4_block5_2_conv/kernel:0\n",
      "@_@\t\tconv4_block5_2_conv/bias:0\n",
      "@_@\t\tconv4_block5_2_bn/gamma:0\n",
      "@_@\t\tconv4_block5_2_bn/beta:0\n",
      "@_@\t\tconv4_block5_3_conv/kernel:0\n",
      "@_@\t\tconv4_block5_3_conv/bias:0\n",
      "@_@\t\tconv4_block5_3_bn/gamma:0\n",
      "@_@\t\tconv4_block5_3_bn/beta:0\n",
      "@_@\t\tconv4_block6_1_conv/kernel:0\n",
      "@_@\t\tconv4_block6_1_conv/bias:0\n",
      "@_@\t\tconv4_block6_1_bn/gamma:0\n",
      "@_@\t\tconv4_block6_1_bn/beta:0\n",
      "@_@\t\tconv4_block6_2_conv/kernel:0\n",
      "@_@\t\tconv4_block6_2_conv/bias:0\n",
      "@_@\t\tconv4_block6_2_bn/gamma:0\n",
      "@_@\t\tconv4_block6_2_bn/beta:0\n",
      "@_@\t\tconv4_block6_3_conv/kernel:0\n",
      "@_@\t\tconv4_block6_3_conv/bias:0\n",
      "@_@\t\tconv4_block6_3_bn/gamma:0\n",
      "@_@\t\tconv4_block6_3_bn/beta:0\n",
      "@_@\t\tconv5_block1_1_conv/kernel:0\n",
      "@_@\t\tconv5_block1_1_conv/bias:0\n",
      "@_@\t\tconv5_block1_1_bn/gamma:0\n",
      "@_@\t\tconv5_block1_1_bn/beta:0\n",
      "@_@\t\tconv5_block1_2_conv/kernel:0\n",
      "@_@\t\tconv5_block1_2_conv/bias:0\n",
      "@_@\t\tconv5_block1_2_bn/gamma:0\n",
      "@_@\t\tconv5_block1_2_bn/beta:0\n",
      "@_@\t\tconv5_block1_0_conv/kernel:0\n",
      "@_@\t\tconv5_block1_0_conv/bias:0\n",
      "@_@\t\tconv5_block1_3_conv/kernel:0\n",
      "@_@\t\tconv5_block1_3_conv/bias:0\n",
      "@_@\t\tconv5_block1_0_bn/gamma:0\n",
      "@_@\t\tconv5_block1_0_bn/beta:0\n",
      "@_@\t\tconv5_block1_3_bn/gamma:0\n",
      "@_@\t\tconv5_block1_3_bn/beta:0\n",
      "@_@\t\tconv5_block2_1_conv/kernel:0\n",
      "@_@\t\tconv5_block2_1_conv/bias:0\n",
      "@_@\t\tconv5_block2_1_bn/gamma:0\n",
      "@_@\t\tconv5_block2_1_bn/beta:0\n",
      "@_@\t\tconv5_block2_2_conv/kernel:0\n",
      "@_@\t\tconv5_block2_2_conv/bias:0\n",
      "@_@\t\tconv5_block2_2_bn/gamma:0\n",
      "@_@\t\tconv5_block2_2_bn/beta:0\n",
      "@_@\t\tconv5_block2_3_conv/kernel:0\n",
      "@_@\t\tconv5_block2_3_conv/bias:0\n",
      "@_@\t\tconv5_block2_3_bn/gamma:0\n",
      "@_@\t\tconv5_block2_3_bn/beta:0\n",
      "@_@\t\tconv5_block3_1_conv/kernel:0\n",
      "@_@\t\tconv5_block3_1_conv/bias:0\n",
      "@_@\t\tconv5_block3_1_bn/gamma:0\n",
      "@_@\t\tconv5_block3_1_bn/beta:0\n",
      "@_@\t\tconv5_block3_2_conv/kernel:0\n",
      "@_@\t\tconv5_block3_2_conv/bias:0\n",
      "@_@\t\tconv5_block3_2_bn/gamma:0\n",
      "@_@\t\tconv5_block3_2_bn/beta:0\n",
      "@_@\t\tconv5_block3_3_conv/kernel:0\n",
      "@_@\t\tconv5_block3_3_conv/bias:0\n",
      "@_@\t\tconv5_block3_3_bn/gamma:0\n",
      "@_@\t\tconv5_block3_3_bn/beta:0\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 1e-05\n",
      "@_@\tLoading weights from /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/1_unfrozen_block/model/checkpoints/epoch-01_valloss-0.4720.ckpt\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4986 - macro_f1_score: 0.1399 - soft_f1_loss: 0.7428 - AUC: 0.5217 - global_accuracy: 0.7491 - global_precision: 0.4512 - global_recall: 0.2204\n",
      "Epoch 1: val_loss improved from inf to 0.47838, saving model to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/2_unfrozen_block/model/checkpoints/epoch-01_valloss-0.4784.ckpt\n",
      "4/4 [==============================] - 70s 15s/step - loss: 0.4986 - macro_f1_score: 0.1399 - soft_f1_loss: 0.7428 - AUC: 0.5217 - global_accuracy: 0.7491 - global_precision: 0.4512 - global_recall: 0.2204 - val_loss: 0.4784 - val_macro_f1_score: 0.0598 - val_soft_f1_loss: 0.8090 - val_AUC: 0.4996 - val_global_accuracy: 0.7772 - val_global_precision: 0.5787 - val_global_recall: 0.1196\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4729 - macro_f1_score: 0.1378 - soft_f1_loss: 0.7443 - AUC: 0.5698 - global_accuracy: 0.7734 - global_precision: 0.5702 - global_recall: 0.2161\n",
      "Epoch 2: val_loss did not improve from 0.47838\n",
      "4/4 [==============================] - 51s 14s/step - loss: 0.4729 - macro_f1_score: 0.1378 - soft_f1_loss: 0.7443 - AUC: 0.5698 - global_accuracy: 0.7734 - global_precision: 0.5702 - global_recall: 0.2161 - val_loss: 0.4995 - val_macro_f1_score: 0.0751 - val_soft_f1_loss: 0.8180 - val_AUC: 0.5093 - val_global_accuracy: 0.7819 - val_global_precision: 0.5695 - val_global_recall: 0.2182\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:2m:1s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [0.4985872805118561, 0.47288209199905396]\n",
      "@_@\t\tmacro_f1_score: [0.13988035917282104, 0.13780274987220764]\n",
      "@_@\t\tsoft_f1_loss: [0.7427597045898438, 0.7443152666091919]\n",
      "@_@\t\tAUC: [0.5217398405075073, 0.5698439478874207]\n",
      "@_@\t\tglobal_accuracy: [0.749131977558136, 0.7734375]\n",
      "@_@\t\tglobal_precision: [0.45121484994888306, 0.5701882243156433]\n",
      "@_@\t\tglobal_recall: [0.22044320404529572, 0.2160789966583252]\n",
      "@_@\t\tval_loss: [0.4783842861652374, 0.4994698464870453]\n",
      "@_@\t\tval_macro_f1_score: [0.059844426810741425, 0.07514321058988571]\n",
      "@_@\t\tval_soft_f1_loss: [0.8090344667434692, 0.818006694316864]\n",
      "@_@\t\tval_AUC: [0.4996327757835388, 0.5093095898628235]\n",
      "@_@\t\tval_global_accuracy: [0.7772360444068909, 0.7818722724914551]\n",
      "@_@\t\tval_global_precision: [0.5787258148193359, 0.5695410966873169]\n",
      "@_@\t\tval_global_recall: [0.1196269765496254, 0.2181728631258011]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/2_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.4995\n",
      "@_@\tValidation Macro F1-score: 0.0751\n",
      "W0405 12:41:52.836507 125908164081472 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/2_unfrozen_block/model/saved_model/assets\n",
      "I0405 12:42:01.096015 125908164081472 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/2_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/2_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "2024-04-05 12:42:02.022640: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-04-05 12:42:02.022711: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2024-04-05 12:42:02.196118: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-04-05 12:42:02.226778: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "@_@\tUnfreezing 3 blocks...\n",
      "@_@\tTotal trainable weights: 174\n",
      "@_@\t\tconv3_block1_1_conv/kernel:0\n",
      "@_@\t\tconv3_block1_1_conv/bias:0\n",
      "@_@\t\tconv3_block1_1_bn/gamma:0\n",
      "@_@\t\tconv3_block1_1_bn/beta:0\n",
      "@_@\t\tconv3_block1_2_conv/kernel:0\n",
      "@_@\t\tconv3_block1_2_conv/bias:0\n",
      "@_@\t\tconv3_block1_2_bn/gamma:0\n",
      "@_@\t\tconv3_block1_2_bn/beta:0\n",
      "@_@\t\tconv3_block1_0_conv/kernel:0\n",
      "@_@\t\tconv3_block1_0_conv/bias:0\n",
      "@_@\t\tconv3_block1_3_conv/kernel:0\n",
      "@_@\t\tconv3_block1_3_conv/bias:0\n",
      "@_@\t\tconv3_block1_0_bn/gamma:0\n",
      "@_@\t\tconv3_block1_0_bn/beta:0\n",
      "@_@\t\tconv3_block1_3_bn/gamma:0\n",
      "@_@\t\tconv3_block1_3_bn/beta:0\n",
      "@_@\t\tconv3_block2_1_conv/kernel:0\n",
      "@_@\t\tconv3_block2_1_conv/bias:0\n",
      "@_@\t\tconv3_block2_1_bn/gamma:0\n",
      "@_@\t\tconv3_block2_1_bn/beta:0\n",
      "@_@\t\tconv3_block2_2_conv/kernel:0\n",
      "@_@\t\tconv3_block2_2_conv/bias:0\n",
      "@_@\t\tconv3_block2_2_bn/gamma:0\n",
      "@_@\t\tconv3_block2_2_bn/beta:0\n",
      "@_@\t\tconv3_block2_3_conv/kernel:0\n",
      "@_@\t\tconv3_block2_3_conv/bias:0\n",
      "@_@\t\tconv3_block2_3_bn/gamma:0\n",
      "@_@\t\tconv3_block2_3_bn/beta:0\n",
      "@_@\t\tconv3_block3_1_conv/kernel:0\n",
      "@_@\t\tconv3_block3_1_conv/bias:0\n",
      "@_@\t\tconv3_block3_1_bn/gamma:0\n",
      "@_@\t\tconv3_block3_1_bn/beta:0\n",
      "@_@\t\tconv3_block3_2_conv/kernel:0\n",
      "@_@\t\tconv3_block3_2_conv/bias:0\n",
      "@_@\t\tconv3_block3_2_bn/gamma:0\n",
      "@_@\t\tconv3_block3_2_bn/beta:0\n",
      "@_@\t\tconv3_block3_3_conv/kernel:0\n",
      "@_@\t\tconv3_block3_3_conv/bias:0\n",
      "@_@\t\tconv3_block3_3_bn/gamma:0\n",
      "@_@\t\tconv3_block3_3_bn/beta:0\n",
      "@_@\t\tconv3_block4_1_conv/kernel:0\n",
      "@_@\t\tconv3_block4_1_conv/bias:0\n",
      "@_@\t\tconv3_block4_1_bn/gamma:0\n",
      "@_@\t\tconv3_block4_1_bn/beta:0\n",
      "@_@\t\tconv3_block4_2_conv/kernel:0\n",
      "@_@\t\tconv3_block4_2_conv/bias:0\n",
      "@_@\t\tconv3_block4_2_bn/gamma:0\n",
      "@_@\t\tconv3_block4_2_bn/beta:0\n",
      "@_@\t\tconv3_block4_3_conv/kernel:0\n",
      "@_@\t\tconv3_block4_3_conv/bias:0\n",
      "@_@\t\tconv3_block4_3_bn/gamma:0\n",
      "@_@\t\tconv3_block4_3_bn/beta:0\n",
      "@_@\t\tconv4_block1_1_conv/kernel:0\n",
      "@_@\t\tconv4_block1_1_conv/bias:0\n",
      "@_@\t\tconv4_block1_1_bn/gamma:0\n",
      "@_@\t\tconv4_block1_1_bn/beta:0\n",
      "@_@\t\tconv4_block1_2_conv/kernel:0\n",
      "@_@\t\tconv4_block1_2_conv/bias:0\n",
      "@_@\t\tconv4_block1_2_bn/gamma:0\n",
      "@_@\t\tconv4_block1_2_bn/beta:0\n",
      "@_@\t\tconv4_block1_0_conv/kernel:0\n",
      "@_@\t\tconv4_block1_0_conv/bias:0\n",
      "@_@\t\tconv4_block1_3_conv/kernel:0\n",
      "@_@\t\tconv4_block1_3_conv/bias:0\n",
      "@_@\t\tconv4_block1_0_bn/gamma:0\n",
      "@_@\t\tconv4_block1_0_bn/beta:0\n",
      "@_@\t\tconv4_block1_3_bn/gamma:0\n",
      "@_@\t\tconv4_block1_3_bn/beta:0\n",
      "@_@\t\tconv4_block2_1_conv/kernel:0\n",
      "@_@\t\tconv4_block2_1_conv/bias:0\n",
      "@_@\t\tconv4_block2_1_bn/gamma:0\n",
      "@_@\t\tconv4_block2_1_bn/beta:0\n",
      "@_@\t\tconv4_block2_2_conv/kernel:0\n",
      "@_@\t\tconv4_block2_2_conv/bias:0\n",
      "@_@\t\tconv4_block2_2_bn/gamma:0\n",
      "@_@\t\tconv4_block2_2_bn/beta:0\n",
      "@_@\t\tconv4_block2_3_conv/kernel:0\n",
      "@_@\t\tconv4_block2_3_conv/bias:0\n",
      "@_@\t\tconv4_block2_3_bn/gamma:0\n",
      "@_@\t\tconv4_block2_3_bn/beta:0\n",
      "@_@\t\tconv4_block3_1_conv/kernel:0\n",
      "@_@\t\tconv4_block3_1_conv/bias:0\n",
      "@_@\t\tconv4_block3_1_bn/gamma:0\n",
      "@_@\t\tconv4_block3_1_bn/beta:0\n",
      "@_@\t\tconv4_block3_2_conv/kernel:0\n",
      "@_@\t\tconv4_block3_2_conv/bias:0\n",
      "@_@\t\tconv4_block3_2_bn/gamma:0\n",
      "@_@\t\tconv4_block3_2_bn/beta:0\n",
      "@_@\t\tconv4_block3_3_conv/kernel:0\n",
      "@_@\t\tconv4_block3_3_conv/bias:0\n",
      "@_@\t\tconv4_block3_3_bn/gamma:0\n",
      "@_@\t\tconv4_block3_3_bn/beta:0\n",
      "@_@\t\tconv4_block4_1_conv/kernel:0\n",
      "@_@\t\tconv4_block4_1_conv/bias:0\n",
      "@_@\t\tconv4_block4_1_bn/gamma:0\n",
      "@_@\t\tconv4_block4_1_bn/beta:0\n",
      "@_@\t\tconv4_block4_2_conv/kernel:0\n",
      "@_@\t\tconv4_block4_2_conv/bias:0\n",
      "@_@\t\tconv4_block4_2_bn/gamma:0\n",
      "@_@\t\tconv4_block4_2_bn/beta:0\n",
      "@_@\t\tconv4_block4_3_conv/kernel:0\n",
      "@_@\t\tconv4_block4_3_conv/bias:0\n",
      "@_@\t\tconv4_block4_3_bn/gamma:0\n",
      "@_@\t\tconv4_block4_3_bn/beta:0\n",
      "@_@\t\tconv4_block5_1_conv/kernel:0\n",
      "@_@\t\tconv4_block5_1_conv/bias:0\n",
      "@_@\t\tconv4_block5_1_bn/gamma:0\n",
      "@_@\t\tconv4_block5_1_bn/beta:0\n",
      "@_@\t\tconv4_block5_2_conv/kernel:0\n",
      "@_@\t\tconv4_block5_2_conv/bias:0\n",
      "@_@\t\tconv4_block5_2_bn/gamma:0\n",
      "@_@\t\tconv4_block5_2_bn/beta:0\n",
      "@_@\t\tconv4_block5_3_conv/kernel:0\n",
      "@_@\t\tconv4_block5_3_conv/bias:0\n",
      "@_@\t\tconv4_block5_3_bn/gamma:0\n",
      "@_@\t\tconv4_block5_3_bn/beta:0\n",
      "@_@\t\tconv4_block6_1_conv/kernel:0\n",
      "@_@\t\tconv4_block6_1_conv/bias:0\n",
      "@_@\t\tconv4_block6_1_bn/gamma:0\n",
      "@_@\t\tconv4_block6_1_bn/beta:0\n",
      "@_@\t\tconv4_block6_2_conv/kernel:0\n",
      "@_@\t\tconv4_block6_2_conv/bias:0\n",
      "@_@\t\tconv4_block6_2_bn/gamma:0\n",
      "@_@\t\tconv4_block6_2_bn/beta:0\n",
      "@_@\t\tconv4_block6_3_conv/kernel:0\n",
      "@_@\t\tconv4_block6_3_conv/bias:0\n",
      "@_@\t\tconv4_block6_3_bn/gamma:0\n",
      "@_@\t\tconv4_block6_3_bn/beta:0\n",
      "@_@\t\tconv5_block1_1_conv/kernel:0\n",
      "@_@\t\tconv5_block1_1_conv/bias:0\n",
      "@_@\t\tconv5_block1_1_bn/gamma:0\n",
      "@_@\t\tconv5_block1_1_bn/beta:0\n",
      "@_@\t\tconv5_block1_2_conv/kernel:0\n",
      "@_@\t\tconv5_block1_2_conv/bias:0\n",
      "@_@\t\tconv5_block1_2_bn/gamma:0\n",
      "@_@\t\tconv5_block1_2_bn/beta:0\n",
      "@_@\t\tconv5_block1_0_conv/kernel:0\n",
      "@_@\t\tconv5_block1_0_conv/bias:0\n",
      "@_@\t\tconv5_block1_3_conv/kernel:0\n",
      "@_@\t\tconv5_block1_3_conv/bias:0\n",
      "@_@\t\tconv5_block1_0_bn/gamma:0\n",
      "@_@\t\tconv5_block1_0_bn/beta:0\n",
      "@_@\t\tconv5_block1_3_bn/gamma:0\n",
      "@_@\t\tconv5_block1_3_bn/beta:0\n",
      "@_@\t\tconv5_block2_1_conv/kernel:0\n",
      "@_@\t\tconv5_block2_1_conv/bias:0\n",
      "@_@\t\tconv5_block2_1_bn/gamma:0\n",
      "@_@\t\tconv5_block2_1_bn/beta:0\n",
      "@_@\t\tconv5_block2_2_conv/kernel:0\n",
      "@_@\t\tconv5_block2_2_conv/bias:0\n",
      "@_@\t\tconv5_block2_2_bn/gamma:0\n",
      "@_@\t\tconv5_block2_2_bn/beta:0\n",
      "@_@\t\tconv5_block2_3_conv/kernel:0\n",
      "@_@\t\tconv5_block2_3_conv/bias:0\n",
      "@_@\t\tconv5_block2_3_bn/gamma:0\n",
      "@_@\t\tconv5_block2_3_bn/beta:0\n",
      "@_@\t\tconv5_block3_1_conv/kernel:0\n",
      "@_@\t\tconv5_block3_1_conv/bias:0\n",
      "@_@\t\tconv5_block3_1_bn/gamma:0\n",
      "@_@\t\tconv5_block3_1_bn/beta:0\n",
      "@_@\t\tconv5_block3_2_conv/kernel:0\n",
      "@_@\t\tconv5_block3_2_conv/bias:0\n",
      "@_@\t\tconv5_block3_2_bn/gamma:0\n",
      "@_@\t\tconv5_block3_2_bn/beta:0\n",
      "@_@\t\tconv5_block3_3_conv/kernel:0\n",
      "@_@\t\tconv5_block3_3_conv/bias:0\n",
      "@_@\t\tconv5_block3_3_bn/gamma:0\n",
      "@_@\t\tconv5_block3_3_bn/beta:0\n",
      "@_@\t\tmy_fc_1/kernel:0\n",
      "@_@\t\tmy_fc_1/bias:0\n",
      "@_@\t\tmy_fc_2/kernel:0\n",
      "@_@\t\tmy_fc_2/bias:0\n",
      "@_@\t\tmy_output/kernel:0\n",
      "@_@\t\tmy_output/bias:0\n",
      "@_@\tLearning rate = 1e-06\n",
      "@_@\tLoading weights from /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/2_unfrozen_block/model/checkpoints/epoch-01_valloss-0.4784.ckpt\n",
      "@_@\t\n",
      "@_@\t------------------ Training ------------------\n",
      "Epoch 1/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4761 - macro_f1_score: 0.1303 - soft_f1_loss: 0.7399 - AUC: 0.5629 - global_accuracy: 0.7561 - global_precision: 0.4762 - global_recall: 0.1966\n",
      "Epoch 1: val_loss improved from inf to 0.46585, saving model to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/3_unfrozen_block/model/checkpoints/epoch-01_valloss-0.4659.ckpt\n",
      "4/4 [==============================] - 77s 15s/step - loss: 0.4761 - macro_f1_score: 0.1303 - soft_f1_loss: 0.7399 - AUC: 0.5629 - global_accuracy: 0.7561 - global_precision: 0.4762 - global_recall: 0.1966 - val_loss: 0.4659 - val_macro_f1_score: 0.0718 - val_soft_f1_loss: 0.7993 - val_AUC: 0.5185 - val_global_accuracy: 0.7837 - val_global_precision: 0.6250 - val_global_recall: 0.1490\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4580 - macro_f1_score: 0.1429 - soft_f1_loss: 0.7327 - AUC: 0.6341 - global_accuracy: 0.7726 - global_precision: 0.5637 - global_recall: 0.2253\n",
      "Epoch 2: val_loss improved from 0.46585 to 0.45479, saving model to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/3_unfrozen_block/model/checkpoints/epoch-02_valloss-0.4548.ckpt\n",
      "4/4 [==============================] - 56s 16s/step - loss: 0.4580 - macro_f1_score: 0.1429 - soft_f1_loss: 0.7327 - AUC: 0.6341 - global_accuracy: 0.7726 - global_precision: 0.5637 - global_recall: 0.2253 - val_loss: 0.4548 - val_macro_f1_score: 0.0866 - val_soft_f1_loss: 0.7854 - val_AUC: 0.5245 - val_global_accuracy: 0.7859 - val_global_precision: 0.6097 - val_global_recall: 0.1908\n",
      "@_@\t\n",
      "@_@\t------------------ Training Round Summary ------------------\n",
      "@_@\tTraining took 0h:2m:13s\n",
      "@_@\thistory of metrics:\n",
      "@_@\t\tloss: [0.4760611951351166, 0.4580470621585846]\n",
      "@_@\t\tmacro_f1_score: [0.13025790452957153, 0.14291930198669434]\n",
      "@_@\t\tsoft_f1_loss: [0.7399325370788574, 0.7327294945716858]\n",
      "@_@\t\tAUC: [0.5628597736358643, 0.6340597867965698]\n",
      "@_@\t\tglobal_accuracy: [0.756076455116272, 0.7725694179534912]\n",
      "@_@\t\tglobal_precision: [0.476174533367157, 0.5637472867965698]\n",
      "@_@\t\tglobal_recall: [0.1966480016708374, 0.22533085942268372]\n",
      "@_@\t\tval_loss: [0.46585047245025635, 0.4547872245311737]\n",
      "@_@\t\tval_macro_f1_score: [0.0718165785074234, 0.08659394830465317]\n",
      "@_@\t\tval_soft_f1_loss: [0.7992780804634094, 0.7854452133178711]\n",
      "@_@\t\tval_AUC: [0.518482506275177, 0.5244743824005127]\n",
      "@_@\t\tval_global_accuracy: [0.7836518287658691, 0.785914421081543]\n",
      "@_@\t\tval_global_precision: [0.6250355839729309, 0.6097274422645569]\n",
      "@_@\t\tval_global_recall: [0.148965984582901, 0.19083252549171448]\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/3_unfrozen_block/figs/learning_curve.png\n",
      "@_@\tValidation loss: 0.4548\n",
      "@_@\tValidation Macro F1-score: 0.0866\n",
      "W0405 12:44:25.899240 125908164081472 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/3_unfrozen_block/model/saved_model/assets\n",
      "I0405 12:44:34.995004 125908164081472 builder_impl.py:797] Assets written to: /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/3_unfrozen_block/model/saved_model/assets\n",
      "@_@\tSaved model to: \"/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/3_unfrozen_block/model/saved_model\"\n",
      "@_@\t\n",
      "@_@\t------------------ Evaluate ------------------\n",
      "2024-04-05 12:44:46.073192: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 222 of 640\n",
      "2024-04-05 12:44:55.904058: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 452 of 640\n",
      "2024-04-05 12:44:58.318819: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-test_2024-04-05_1234/predict_sample.png\n",
      "2024-04-05 12:45:14.912331: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 219 of 640\n",
      "2024-04-05 12:45:24.913818: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 456 of 640\n",
      "2024-04-05 12:45:27.152149: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2/2 [==============================] - 0s 49ms/step\n",
      "2/2 [==============================] - 0s 49ms/step\n",
      "2/2 [==============================] - 0s 46ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x727e8f7aed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "W0405 12:45:27.884741 125908164081472 polymorphic_function.py:154] 5 out of the last 5 calls to <function pfor.<locals>.f at 0x727e8f7aed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x727e8f7aed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "W0405 12:45:27.915427 125908164081472 polymorphic_function.py:154] 6 out of the last 6 calls to <function pfor.<locals>.f at 0x727e8f7aed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 47ms/step\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "@_@\tResult of testing on 8 batches:\n",
      "@_@\t\tbinary_crossentropy: 0.5127490758895874\n",
      "@_@\t\tAUC: 0.5213600993156433\n",
      "@_@\t\tmacro_f1_score: 0.07853952050209045\n",
      "@_@\t\tsoft_f1_loss: 0.7799149751663208\n",
      "@_@\t\tglobal_accuracy: 0.7562934160232544\n",
      "@_@\t\tglobal_precision: 0.5307589769363403\n",
      "@_@\t\tglobal_recall: 0.14645126461982727\n",
      "@_@\t\n",
      "@_@\tDONE!\n"
     ]
    }
   ],
   "source": [
    "!python ../neural_nets/resnet50.py \\\n",
    "  --ouput_name=resnet50-test \\\n",
    "  --learning_rate=1e-4 --image_size=224 --epochs=2 --batch_size=64 --train_size=256 \\\n",
    "  --mode=train_then_eval --min_unfreeze_blocks=0 --max_unfreeze_blocks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cwd to '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation'\n",
      "['/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode/neural_nets', '/home/payam/miniconda3/envs/tf2-gpu/lib/python39.zip', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/lib-dynload', '/home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages', '/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/mycode']\n",
      "/mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation\n",
      "2024-04-05 10:41:01.477019: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 10:41:02.003091: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-05 10:41:02.003199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/home/payam/miniconda3/envs/tf2-gpu/lib/\n",
      "2024-04-05 10:41:02.003207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-05 10:41:03.061825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.096422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.096710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.097178: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 10:41:03.098057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.098235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.098393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.546683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.546900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.547061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-05 10:41:03.547191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "@_@\t\n",
      "@_@\t------------------ Configuration ------------------\n",
      "@_@\tStart: 2024-04-05_1041\n",
      "@_@\t\n",
      "@_@\tMode: eval\n",
      "@_@\tUnfreeze blocks: start 0, end 0\n",
      "@_@\tContinue from checkpoint: ./out_archive/resnet50/resnet50-full_2024-04-04_1055/5_unfrozen_block/model/checkpoints\n",
      "@_@\t\n",
      "@_@\tDataset: MIMIC-CXR\n",
      "@_@\tTraining Dataset Size: 0\n",
      "@_@\tBatch size: 32\n",
      "@_@\tImage size: (224, 224)\n",
      "@_@\tMax Epochs per training round: 10\n",
      "@_@\tDefault Learning Rate: 0.1\n",
      "@_@\t\n",
      "@_@\tGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "@_@\tCPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "@_@\t\n",
      "@_@\t------------------ Data ------------------\n",
      "@_@\tnum_classes: 9\n",
      "@_@\tclass_names: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Pleural Effusion'\n",
      "@_@\t 'Pneumonia' 'Pneumothorax' 'Fracture' 'Support Devices']\n",
      "@_@\tall_data_size: 377110\n",
      "@_@\tall_data_filtered_size: 112134\n",
      "@_@\tsplit_size: {'train': 0, 'validate': 889, 'test': 512}\n",
      "WARNING:tensorflow:From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0405 10:41:50.074412 125346261677888 deprecation.py:350] From /home/payam/miniconda3/envs/tf2-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Model: \"my_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " my_avg_pool (GlobalAveragePool  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " my_fc_1 (Dense)                (None, 256)          524544      ['my_avg_pool[0][0]']            \n",
      "                                                                                                  \n",
      " my_fc_2 (Dense)                (None, 128)          32896       ['my_fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " my_output (Dense)              (None, 9)            1161        ['my_fc_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,146,313\n",
      "Trainable params: 558,601\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "@_@\t\n",
      "@_@\t------------------ Setup Round ------------------\n",
      "2024-04-05 10:41:51.623523: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-04-05 10:41:51.623594: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2024-04-05 10:41:51.623715: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1664] Profiler found 1 GPUs\n",
      "2024-04-05 10:41:51.766777: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-04-05 10:41:51.768119: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "@_@\tLoading weights from /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/./out_archive/resnet50/resnet50-full_2024-04-04_1055/5_unfrozen_block/model/checkpoints/epoch-02_valloss-0.3891.ckpt\n",
      "@_@\t\n",
      "@_@\t------------------ Evaluate ------------------\n",
      "2024-04-05 10:42:03.398516: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 221 of 320\n",
      "2024-04-05 10:42:07.453360: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2024-04-05 10:42:11.296463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "@_@\tSaving to /mnt/samba/research/shield/projects/payamfz/medical-ssl-segmentation/out/resnet50-eval-best_2024-04-05_1041/predict_sample.png\n",
      "2024-04-05 10:42:24.972469: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 230 of 320\n",
      "2024-04-05 10:42:28.582369: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x71fe81f7ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "W0405 10:42:34.436088 125346261677888 polymorphic_function.py:154] 5 out of the last 5 calls to <function pfor.<locals>.f at 0x71fe81f7ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x71fe81f7ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "W0405 10:42:34.513201 125346261677888 polymorphic_function.py:154] 6 out of the last 6 calls to <function pfor.<locals>.f at 0x71fe81f7ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "@_@\tResult of testing on 16 batches:\n",
      "@_@\t\tbinary_crossentropy: 0.4466290771961212\n",
      "@_@\t\tAUC: 0.7059942483901978\n",
      "@_@\t\tmacro_f1_score: 0.27543139457702637\n",
      "@_@\t\tsoft_f1_loss: 0.6958718299865723\n",
      "@_@\t\tglobal_accuracy: 0.798828125\n",
      "@_@\t\tglobal_precision: 0.6408952474594116\n",
      "@_@\t\tglobal_recall: 0.4338400363922119\n",
      "@_@\t\n",
      "@_@\tDONE!\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "W0405 10:42:39.113863 125346261677888 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "W0405 10:42:39.113984 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "W0405 10:42:39.114026 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "W0405 10:42:39.114060 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "W0405 10:42:39.114092 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "W0405 10:42:39.114122 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "W0405 10:42:39.114152 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "W0405 10:42:39.114181 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "W0405 10:42:39.114211 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "W0405 10:42:39.114241 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "W0405 10:42:39.114270 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "W0405 10:42:39.114299 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "W0405 10:42:39.114329 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "W0405 10:42:39.114358 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "W0405 10:42:39.114387 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "W0405 10:42:39.114417 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "W0405 10:42:39.114446 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "W0405 10:42:39.114476 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "W0405 10:42:39.114505 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "W0405 10:42:39.114534 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "W0405 10:42:39.114563 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "W0405 10:42:39.114592 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "W0405 10:42:39.114622 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "W0405 10:42:39.114651 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "W0405 10:42:39.114680 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "W0405 10:42:39.114710 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "W0405 10:42:39.114739 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "W0405 10:42:39.114768 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "W0405 10:42:39.114797 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "W0405 10:42:39.114826 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "W0405 10:42:39.114855 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "W0405 10:42:39.114885 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "W0405 10:42:39.114914 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
      "W0405 10:42:39.114943 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
      "W0405 10:42:39.114972 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
      "W0405 10:42:39.115002 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
      "W0405 10:42:39.115031 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
      "W0405 10:42:39.115061 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
      "W0405 10:42:39.115090 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n",
      "W0405 10:42:39.115120 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n",
      "W0405 10:42:39.115149 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n",
      "W0405 10:42:39.115179 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n",
      "W0405 10:42:39.115208 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n",
      "W0405 10:42:39.115237 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n",
      "W0405 10:42:39.115267 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.45\n",
      "W0405 10:42:39.115296 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.45\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.46\n",
      "W0405 10:42:39.115325 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.46\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.47\n",
      "W0405 10:42:39.115355 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.47\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.48\n",
      "W0405 10:42:39.115385 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.48\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.49\n",
      "W0405 10:42:39.115439 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.49\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.50\n",
      "W0405 10:42:39.115495 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.50\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.51\n",
      "W0405 10:42:39.115525 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.51\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.52\n",
      "W0405 10:42:39.115554 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.52\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.53\n",
      "W0405 10:42:39.115584 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.53\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.54\n",
      "W0405 10:42:39.115613 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.54\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.55\n",
      "W0405 10:42:39.115642 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.55\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.56\n",
      "W0405 10:42:39.115672 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.56\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.57\n",
      "W0405 10:42:39.115702 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.57\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.58\n",
      "W0405 10:42:39.115731 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.58\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.59\n",
      "W0405 10:42:39.115761 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.59\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.60\n",
      "W0405 10:42:39.115791 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.60\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.61\n",
      "W0405 10:42:39.115821 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.61\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.62\n",
      "W0405 10:42:39.115850 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.62\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.63\n",
      "W0405 10:42:39.115880 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.63\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.64\n",
      "W0405 10:42:39.115909 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.64\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.65\n",
      "W0405 10:42:39.115939 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.65\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.66\n",
      "W0405 10:42:39.115968 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.66\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.67\n",
      "W0405 10:42:39.115998 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.67\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.68\n",
      "W0405 10:42:39.116027 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.68\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.69\n",
      "W0405 10:42:39.116057 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.69\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.70\n",
      "W0405 10:42:39.116086 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.70\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.71\n",
      "W0405 10:42:39.116116 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.71\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.72\n",
      "W0405 10:42:39.116145 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.72\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.73\n",
      "W0405 10:42:39.116175 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.73\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.74\n",
      "W0405 10:42:39.116205 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.74\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.75\n",
      "W0405 10:42:39.116234 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.75\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.76\n",
      "W0405 10:42:39.116264 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.76\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.77\n",
      "W0405 10:42:39.116294 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.77\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.78\n",
      "W0405 10:42:39.116323 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.78\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.79\n",
      "W0405 10:42:39.116353 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.79\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.80\n",
      "W0405 10:42:39.116382 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.80\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.81\n",
      "W0405 10:42:39.116412 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.81\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.82\n",
      "W0405 10:42:39.116441 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.82\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.83\n",
      "W0405 10:42:39.116470 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.83\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.84\n",
      "W0405 10:42:39.116499 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.84\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.85\n",
      "W0405 10:42:39.116529 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.85\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.86\n",
      "W0405 10:42:39.116558 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.86\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.87\n",
      "W0405 10:42:39.116587 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.87\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.88\n",
      "W0405 10:42:39.116617 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.88\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.89\n",
      "W0405 10:42:39.116647 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.89\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.90\n",
      "W0405 10:42:39.116677 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.90\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.91\n",
      "W0405 10:42:39.116706 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.91\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.92\n",
      "W0405 10:42:39.116735 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.92\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.93\n",
      "W0405 10:42:39.116765 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.93\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.94\n",
      "W0405 10:42:39.116794 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.94\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.95\n",
      "W0405 10:42:39.116824 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.95\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.96\n",
      "W0405 10:42:39.116853 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.96\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.97\n",
      "W0405 10:42:39.116883 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.97\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.98\n",
      "W0405 10:42:39.116912 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.98\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.99\n",
      "W0405 10:42:39.116942 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.99\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.100\n",
      "W0405 10:42:39.116971 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.100\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.101\n",
      "W0405 10:42:39.117001 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.101\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.102\n",
      "W0405 10:42:39.117031 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.102\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.103\n",
      "W0405 10:42:39.117060 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.103\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.104\n",
      "W0405 10:42:39.117090 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.104\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.105\n",
      "W0405 10:42:39.117120 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.105\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.106\n",
      "W0405 10:42:39.117149 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.106\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.107\n",
      "W0405 10:42:39.117179 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.107\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.108\n",
      "W0405 10:42:39.117208 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.108\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.109\n",
      "W0405 10:42:39.117237 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.109\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.110\n",
      "W0405 10:42:39.117266 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.110\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.111\n",
      "W0405 10:42:39.117296 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.111\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.112\n",
      "W0405 10:42:39.117325 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.112\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.113\n",
      "W0405 10:42:39.117355 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.113\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.114\n",
      "W0405 10:42:39.117384 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.114\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.115\n",
      "W0405 10:42:39.117413 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.115\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.116\n",
      "W0405 10:42:39.117443 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.116\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.117\n",
      "W0405 10:42:39.117472 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.117\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.118\n",
      "W0405 10:42:39.117502 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.118\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.119\n",
      "W0405 10:42:39.117531 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.119\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.120\n",
      "W0405 10:42:39.117560 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.120\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.121\n",
      "W0405 10:42:39.117589 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.121\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.122\n",
      "W0405 10:42:39.117619 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.122\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.123\n",
      "W0405 10:42:39.117648 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.123\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.124\n",
      "W0405 10:42:39.117677 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.124\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.125\n",
      "W0405 10:42:39.117706 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.125\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.126\n",
      "W0405 10:42:39.117736 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.126\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.127\n",
      "W0405 10:42:39.117765 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.127\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.128\n",
      "W0405 10:42:39.117794 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.128\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.129\n",
      "W0405 10:42:39.117823 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.129\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.130\n",
      "W0405 10:42:39.117853 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.130\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.131\n",
      "W0405 10:42:39.117882 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.131\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.132\n",
      "W0405 10:42:39.117911 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.132\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.133\n",
      "W0405 10:42:39.117941 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.133\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.134\n",
      "W0405 10:42:39.117971 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.134\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.135\n",
      "W0405 10:42:39.118000 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.135\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.136\n",
      "W0405 10:42:39.118029 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.136\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.137\n",
      "W0405 10:42:39.118059 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.137\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.138\n",
      "W0405 10:42:39.118088 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.138\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.139\n",
      "W0405 10:42:39.118118 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.139\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.140\n",
      "W0405 10:42:39.118147 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.140\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.141\n",
      "W0405 10:42:39.118176 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.141\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.142\n",
      "W0405 10:42:39.118206 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.142\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.143\n",
      "W0405 10:42:39.118235 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.143\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.144\n",
      "W0405 10:42:39.118265 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.144\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.145\n",
      "W0405 10:42:39.118294 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.145\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.146\n",
      "W0405 10:42:39.118323 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.146\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.147\n",
      "W0405 10:42:39.118352 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.147\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.148\n",
      "W0405 10:42:39.118381 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.148\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.149\n",
      "W0405 10:42:39.118411 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.149\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.150\n",
      "W0405 10:42:39.118439 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.150\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.151\n",
      "W0405 10:42:39.118469 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.151\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.152\n",
      "W0405 10:42:39.118498 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.152\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.153\n",
      "W0405 10:42:39.118527 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.153\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.154\n",
      "W0405 10:42:39.118557 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.154\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.155\n",
      "W0405 10:42:39.118586 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.155\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.156\n",
      "W0405 10:42:39.118615 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.156\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.157\n",
      "W0405 10:42:39.118644 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.157\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.158\n",
      "W0405 10:42:39.118674 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.158\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.159\n",
      "W0405 10:42:39.118703 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.159\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.160\n",
      "W0405 10:42:39.118732 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.160\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.161\n",
      "W0405 10:42:39.118762 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.161\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.162\n",
      "W0405 10:42:39.118791 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.162\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.163\n",
      "W0405 10:42:39.118820 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.163\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.164\n",
      "W0405 10:42:39.118850 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.164\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.165\n",
      "W0405 10:42:39.118880 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.165\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.166\n",
      "W0405 10:42:39.118909 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.166\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.167\n",
      "W0405 10:42:39.118938 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.167\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.168\n",
      "W0405 10:42:39.118968 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.168\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.169\n",
      "W0405 10:42:39.118997 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.169\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.170\n",
      "W0405 10:42:39.119027 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.170\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.171\n",
      "W0405 10:42:39.119056 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.171\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.172\n",
      "W0405 10:42:39.119085 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.172\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.173\n",
      "W0405 10:42:39.119115 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.173\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.174\n",
      "W0405 10:42:39.119144 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.174\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.175\n",
      "W0405 10:42:39.119174 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.175\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.176\n",
      "W0405 10:42:39.119203 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.176\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.177\n",
      "W0405 10:42:39.119232 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.177\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.178\n",
      "W0405 10:42:39.119262 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.178\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.179\n",
      "W0405 10:42:39.119292 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.179\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.180\n",
      "W0405 10:42:39.119321 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.180\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.181\n",
      "W0405 10:42:39.119351 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.181\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.182\n",
      "W0405 10:42:39.119380 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.182\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.183\n",
      "W0405 10:42:39.119443 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.183\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.184\n",
      "W0405 10:42:39.119500 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.184\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.185\n",
      "W0405 10:42:39.119530 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.185\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.186\n",
      "W0405 10:42:39.119560 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.186\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.187\n",
      "W0405 10:42:39.119589 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.187\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.188\n",
      "W0405 10:42:39.119618 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.188\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.189\n",
      "W0405 10:42:39.119647 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.189\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.190\n",
      "W0405 10:42:39.119677 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.190\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.191\n",
      "W0405 10:42:39.119707 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.191\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.192\n",
      "W0405 10:42:39.119736 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.192\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.193\n",
      "W0405 10:42:39.119765 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.193\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.194\n",
      "W0405 10:42:39.119795 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.194\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.195\n",
      "W0405 10:42:39.119825 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.195\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.196\n",
      "W0405 10:42:39.119854 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.196\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.197\n",
      "W0405 10:42:39.119899 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.197\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.198\n",
      "W0405 10:42:39.119942 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.198\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.199\n",
      "W0405 10:42:39.119988 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.199\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.200\n",
      "W0405 10:42:39.120031 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.200\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.201\n",
      "W0405 10:42:39.120061 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.201\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.202\n",
      "W0405 10:42:39.120090 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.202\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.203\n",
      "W0405 10:42:39.120120 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.203\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.204\n",
      "W0405 10:42:39.120149 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.204\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.205\n",
      "W0405 10:42:39.120178 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.205\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.206\n",
      "W0405 10:42:39.120208 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.206\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.207\n",
      "W0405 10:42:39.120237 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.207\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.208\n",
      "W0405 10:42:39.120266 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.208\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.209\n",
      "W0405 10:42:39.120296 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.209\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.210\n",
      "W0405 10:42:39.120326 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.210\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.211\n",
      "W0405 10:42:39.120355 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.211\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.212\n",
      "W0405 10:42:39.120384 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.212\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.213\n",
      "W0405 10:42:39.120414 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.213\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.214\n",
      "W0405 10:42:39.120443 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.214\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.215\n",
      "W0405 10:42:39.120472 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.215\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.216\n",
      "W0405 10:42:39.120501 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.216\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.217\n",
      "W0405 10:42:39.120530 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.217\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.218\n",
      "W0405 10:42:39.120560 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.218\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.219\n",
      "W0405 10:42:39.120589 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.219\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.220\n",
      "W0405 10:42:39.120618 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.220\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.221\n",
      "W0405 10:42:39.120647 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.221\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.222\n",
      "W0405 10:42:39.120677 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.222\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.223\n",
      "W0405 10:42:39.120707 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.223\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.224\n",
      "W0405 10:42:39.120735 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.224\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.225\n",
      "W0405 10:42:39.120765 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.225\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.226\n",
      "W0405 10:42:39.120794 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.226\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.227\n",
      "W0405 10:42:39.120824 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.227\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.228\n",
      "W0405 10:42:39.120853 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.228\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.229\n",
      "W0405 10:42:39.120882 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.229\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.230\n",
      "W0405 10:42:39.120912 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.230\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.231\n",
      "W0405 10:42:39.120941 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.231\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.232\n",
      "W0405 10:42:39.120971 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.232\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.233\n",
      "W0405 10:42:39.121000 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.233\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.234\n",
      "W0405 10:42:39.121029 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.234\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.235\n",
      "W0405 10:42:39.121059 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.235\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.236\n",
      "W0405 10:42:39.121088 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.236\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.237\n",
      "W0405 10:42:39.121118 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.237\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.238\n",
      "W0405 10:42:39.121147 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.238\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.239\n",
      "W0405 10:42:39.121177 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.239\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.240\n",
      "W0405 10:42:39.121207 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.240\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.241\n",
      "W0405 10:42:39.121236 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.241\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.242\n",
      "W0405 10:42:39.121265 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.242\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.243\n",
      "W0405 10:42:39.121295 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.243\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.244\n",
      "W0405 10:42:39.121324 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.244\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.245\n",
      "W0405 10:42:39.121353 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.245\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.246\n",
      "W0405 10:42:39.121382 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.246\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.247\n",
      "W0405 10:42:39.121412 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.247\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.248\n",
      "W0405 10:42:39.121442 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.248\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.249\n",
      "W0405 10:42:39.121472 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.249\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.250\n",
      "W0405 10:42:39.121502 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.250\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.251\n",
      "W0405 10:42:39.121531 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.251\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.252\n",
      "W0405 10:42:39.121561 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.252\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.253\n",
      "W0405 10:42:39.121591 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.253\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.254\n",
      "W0405 10:42:39.121620 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.254\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.255\n",
      "W0405 10:42:39.121650 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.255\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.256\n",
      "W0405 10:42:39.121679 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.256\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.257\n",
      "W0405 10:42:39.121709 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.257\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.258\n",
      "W0405 10:42:39.121738 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.258\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.259\n",
      "W0405 10:42:39.121768 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.259\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.260\n",
      "W0405 10:42:39.121797 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.260\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.261\n",
      "W0405 10:42:39.121827 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.261\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.262\n",
      "W0405 10:42:39.121856 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.262\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.263\n",
      "W0405 10:42:39.121886 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.263\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.264\n",
      "W0405 10:42:39.121916 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.264\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.265\n",
      "W0405 10:42:39.121946 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.265\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.266\n",
      "W0405 10:42:39.121975 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.266\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.267\n",
      "W0405 10:42:39.122004 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.267\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.268\n",
      "W0405 10:42:39.122034 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.268\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.269\n",
      "W0405 10:42:39.122063 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.269\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.270\n",
      "W0405 10:42:39.122092 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.270\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.271\n",
      "W0405 10:42:39.122122 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.271\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.272\n",
      "W0405 10:42:39.122151 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.272\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.273\n",
      "W0405 10:42:39.122181 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.273\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.274\n",
      "W0405 10:42:39.122210 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.274\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.275\n",
      "W0405 10:42:39.122240 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.275\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.276\n",
      "W0405 10:42:39.122269 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.276\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.277\n",
      "W0405 10:42:39.122298 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.277\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.278\n",
      "W0405 10:42:39.122327 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.278\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.279\n",
      "W0405 10:42:39.122357 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.279\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.280\n",
      "W0405 10:42:39.122386 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.280\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.281\n",
      "W0405 10:42:39.122415 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.281\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.282\n",
      "W0405 10:42:39.122445 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.282\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.283\n",
      "W0405 10:42:39.122473 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.283\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.284\n",
      "W0405 10:42:39.122502 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.284\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.285\n",
      "W0405 10:42:39.122531 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.285\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.286\n",
      "W0405 10:42:39.122560 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.286\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.287\n",
      "W0405 10:42:39.122605 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.287\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.288\n",
      "W0405 10:42:39.122635 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.288\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.289\n",
      "W0405 10:42:39.122678 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.289\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.290\n",
      "W0405 10:42:39.122708 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.290\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.291\n",
      "W0405 10:42:39.122737 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.291\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.292\n",
      "W0405 10:42:39.122766 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.292\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.293\n",
      "W0405 10:42:39.122795 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.293\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.294\n",
      "W0405 10:42:39.122824 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.294\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.295\n",
      "W0405 10:42:39.122854 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.295\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.296\n",
      "W0405 10:42:39.122883 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.296\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.297\n",
      "W0405 10:42:39.122912 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.297\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.298\n",
      "W0405 10:42:39.122941 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.298\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.299\n",
      "W0405 10:42:39.122970 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.299\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.300\n",
      "W0405 10:42:39.122999 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.300\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.301\n",
      "W0405 10:42:39.123028 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.301\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.302\n",
      "W0405 10:42:39.123057 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.302\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.303\n",
      "W0405 10:42:39.123086 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.303\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.304\n",
      "W0405 10:42:39.123116 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.304\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.305\n",
      "W0405 10:42:39.123145 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.305\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.306\n",
      "W0405 10:42:39.123174 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.306\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.307\n",
      "W0405 10:42:39.123203 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.307\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.308\n",
      "W0405 10:42:39.123233 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.308\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.309\n",
      "W0405 10:42:39.123262 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.309\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.310\n",
      "W0405 10:42:39.123291 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.310\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.311\n",
      "W0405 10:42:39.123321 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.311\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.312\n",
      "W0405 10:42:39.123350 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.312\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.313\n",
      "W0405 10:42:39.123379 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.313\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.314\n",
      "W0405 10:42:39.123453 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.314\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.315\n",
      "W0405 10:42:39.123511 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.315\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.316\n",
      "W0405 10:42:39.123540 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.316\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.317\n",
      "W0405 10:42:39.123570 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.317\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.318\n",
      "W0405 10:42:39.123599 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.318\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.319\n",
      "W0405 10:42:39.123628 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.319\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.320\n",
      "W0405 10:42:39.123658 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.320\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.321\n",
      "W0405 10:42:39.123687 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.321\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.322\n",
      "W0405 10:42:39.123717 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.322\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.323\n",
      "W0405 10:42:39.123746 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.323\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.324\n",
      "W0405 10:42:39.123775 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.324\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.325\n",
      "W0405 10:42:39.123805 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.325\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.326\n",
      "W0405 10:42:39.123834 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.326\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.327\n",
      "W0405 10:42:39.123863 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.327\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.328\n",
      "W0405 10:42:39.123893 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.328\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.329\n",
      "W0405 10:42:39.123923 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.329\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.330\n",
      "W0405 10:42:39.123952 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.330\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.331\n",
      "W0405 10:42:39.123981 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.331\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.332\n",
      "W0405 10:42:39.124011 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.332\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.333\n",
      "W0405 10:42:39.124040 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.333\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.334\n",
      "W0405 10:42:39.124070 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.334\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.335\n",
      "W0405 10:42:39.124100 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.335\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.336\n",
      "W0405 10:42:39.124130 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.336\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.337\n",
      "W0405 10:42:39.124159 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.337\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.338\n",
      "W0405 10:42:39.124188 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.338\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.339\n",
      "W0405 10:42:39.124218 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.339\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.340\n",
      "W0405 10:42:39.124246 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.340\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.341\n",
      "W0405 10:42:39.124276 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.341\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.342\n",
      "W0405 10:42:39.124305 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.342\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.343\n",
      "W0405 10:42:39.124334 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.343\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.344\n",
      "W0405 10:42:39.124363 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.344\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.345\n",
      "W0405 10:42:39.124392 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.345\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.346\n",
      "W0405 10:42:39.124421 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.346\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.347\n",
      "W0405 10:42:39.124451 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.347\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.348\n",
      "W0405 10:42:39.124480 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.348\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.349\n",
      "W0405 10:42:39.124509 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.349\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.350\n",
      "W0405 10:42:39.124538 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.350\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.351\n",
      "W0405 10:42:39.124567 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.351\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.352\n",
      "W0405 10:42:39.124597 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.352\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.353\n",
      "W0405 10:42:39.124626 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.353\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.354\n",
      "W0405 10:42:39.124656 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.354\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.355\n",
      "W0405 10:42:39.124685 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.355\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.356\n",
      "W0405 10:42:39.124714 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.356\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.357\n",
      "W0405 10:42:39.124744 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.357\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.358\n",
      "W0405 10:42:39.124773 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.358\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.359\n",
      "W0405 10:42:39.124802 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.359\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.360\n",
      "W0405 10:42:39.124831 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.360\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.361\n",
      "W0405 10:42:39.124861 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.361\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.362\n",
      "W0405 10:42:39.124891 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.362\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.363\n",
      "W0405 10:42:39.124920 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.363\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.364\n",
      "W0405 10:42:39.124949 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.364\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.365\n",
      "W0405 10:42:39.124978 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.365\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.366\n",
      "W0405 10:42:39.125007 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.366\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.367\n",
      "W0405 10:42:39.125037 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.367\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.368\n",
      "W0405 10:42:39.125066 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.368\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.369\n",
      "W0405 10:42:39.125095 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.369\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.370\n",
      "W0405 10:42:39.125125 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.370\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.371\n",
      "W0405 10:42:39.125154 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.371\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.372\n",
      "W0405 10:42:39.125200 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.372\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.373\n",
      "W0405 10:42:39.125230 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.373\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.374\n",
      "W0405 10:42:39.125272 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.374\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.375\n",
      "W0405 10:42:39.125302 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.375\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.376\n",
      "W0405 10:42:39.125331 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.376\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.377\n",
      "W0405 10:42:39.125361 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.377\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.378\n",
      "W0405 10:42:39.125390 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.378\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.379\n",
      "W0405 10:42:39.125420 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.379\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.380\n",
      "W0405 10:42:39.125449 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.380\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.381\n",
      "W0405 10:42:39.125478 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.381\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.382\n",
      "W0405 10:42:39.125508 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.382\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.383\n",
      "W0405 10:42:39.125537 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.383\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.384\n",
      "W0405 10:42:39.125566 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.384\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.385\n",
      "W0405 10:42:39.125595 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.385\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.386\n",
      "W0405 10:42:39.125625 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.386\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.387\n",
      "W0405 10:42:39.125654 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.387\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.388\n",
      "W0405 10:42:39.125684 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.388\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.389\n",
      "W0405 10:42:39.125713 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.389\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.390\n",
      "W0405 10:42:39.125743 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.390\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.391\n",
      "W0405 10:42:39.125772 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.391\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.392\n",
      "W0405 10:42:39.125801 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.392\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.393\n",
      "W0405 10:42:39.125831 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.393\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.394\n",
      "W0405 10:42:39.125861 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.394\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.395\n",
      "W0405 10:42:39.125890 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.395\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.396\n",
      "W0405 10:42:39.125920 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.396\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.397\n",
      "W0405 10:42:39.125950 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.397\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.398\n",
      "W0405 10:42:39.125980 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.398\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.399\n",
      "W0405 10:42:39.126009 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.399\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.400\n",
      "W0405 10:42:39.126055 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.400\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.401\n",
      "W0405 10:42:39.126111 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.401\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.402\n",
      "W0405 10:42:39.126169 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.402\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.403\n",
      "W0405 10:42:39.126225 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.403\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.404\n",
      "W0405 10:42:39.126268 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.404\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.405\n",
      "W0405 10:42:39.126313 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.405\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.406\n",
      "W0405 10:42:39.126344 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.406\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.407\n",
      "W0405 10:42:39.126374 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.407\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.408\n",
      "W0405 10:42:39.126404 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.408\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.409\n",
      "W0405 10:42:39.126435 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.409\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.410\n",
      "W0405 10:42:39.126465 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.410\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.411\n",
      "W0405 10:42:39.126495 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.411\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.412\n",
      "W0405 10:42:39.126525 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.412\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.413\n",
      "W0405 10:42:39.126556 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.413\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.414\n",
      "W0405 10:42:39.126586 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.414\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.415\n",
      "W0405 10:42:39.126616 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.415\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.416\n",
      "W0405 10:42:39.126647 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.416\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.417\n",
      "W0405 10:42:39.126677 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.417\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.418\n",
      "W0405 10:42:39.126708 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.418\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.419\n",
      "W0405 10:42:39.126738 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.419\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.420\n",
      "W0405 10:42:39.126768 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.420\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.421\n",
      "W0405 10:42:39.126798 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.421\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.422\n",
      "W0405 10:42:39.126829 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.422\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.423\n",
      "W0405 10:42:39.126858 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.423\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.424\n",
      "W0405 10:42:39.126889 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.424\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.425\n",
      "W0405 10:42:39.126919 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.425\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.426\n",
      "W0405 10:42:39.126949 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.426\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.427\n",
      "W0405 10:42:39.126980 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.427\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.428\n",
      "W0405 10:42:39.127010 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.428\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.429\n",
      "W0405 10:42:39.127040 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.429\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.430\n",
      "W0405 10:42:39.127070 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.430\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.431\n",
      "W0405 10:42:39.127100 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.431\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.432\n",
      "W0405 10:42:39.127146 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.432\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.433\n",
      "W0405 10:42:39.127178 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.433\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.434\n",
      "W0405 10:42:39.127223 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.434\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.435\n",
      "W0405 10:42:39.127254 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.435\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.436\n",
      "W0405 10:42:39.127284 125346261677888 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer._variables.436\n"
     ]
    }
   ],
   "source": [
    "!python ../neural_nets/resnet50.py \\\n",
    "  --ouput_name=resnet50-eval-best \\\n",
    "  --image_size=224 --batch_size=32 --train_size=0 \\\n",
    "  --mode=eval \\\n",
    "  --load_checkpoint=./out_archive/resnet50/resnet50-full_2024-04-04_1055/5_unfrozen_block/model/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
